<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on XY&#39;s Blog</title>
    <link>https://gdy0924.github.io/categories/paper/</link>
    <description>Recent content in Paper on XY&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gdy0924.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepNet</title>
      <link>https://gdy0924.github.io/posts/deepnet/</link>
      <pubDate>Sat, 25 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/deepnet/</guid>
      <description>DeepNet: Scaling Transformers to 1,000 Layers
Abstract  本文提出了一种简单而有效的稳定更深层transformer的方法：引入了一个新的归一化函数DEEPNORM，该方法结合了两个优点：Post-LN的性能好和Pre-LN的稳定训练。
Introduction  近几年提出了许多大模型，尽管他们的参数量很大，但是它们的深度受到transformer训练不稳定性的限制。
有研究发现：Pre-LN提高了模型的稳定性，但由于Pre-LN在底层的梯度往往大于在顶层的梯度，因此与Post-LN相比，性能下降。为了缓解上述问题，通常通过更好的初始化来改进深度他说former的优化。
我们的目标是提高transformer的训练稳定性，并扩大模型深度的数量级。为此，我们研究了不稳定的原因，发现模型的爆炸性更新是导致不稳定的原因。基于上述观察，我们在残差连接处引入了一个新的归一化函数DEEPNORM。
TL;DR for Practitioners 在归一化前，DEEPNORM向上扩展了残差连接；同时在初始化时，引入缩放因子gain。
Instability of Deep Transformer  更好的初始化方法可以稳定transformer的训练，因此，我们研究了无论是否进行适当初始化情况下的Post-LN的训练过程。 上图中，Post-LN-init是对第l层的权值进行减小，Post-LN为标准的结果；可以看出，无论模型深度如何，最后一层Post-LN-init的梯度范数仍然远远大于Post-LN，这表明，深层的爆炸梯度不应该是Post-LN不稳定的根本原因。
上图为可视化：
（a）是训练的早期阶段的模型更新范数： $$ || \Delta F ||=|| F(x,\theta _ {i}) -F(x,\theta _ {0})|| $$ 可以看出，Post-LN在训练一开始就有一个爆炸式的更新，然后几乎没有更新，这表明该模型陷入了一个虚假的局部最优状态。warm-up和更好的初始化都有助于缓解这个问题，使模型能够平稳地更新。当update爆发时，LN的输入就会变得很大。如上图中的（b）和（c）所示。
总的来说，不稳定性始于训练开始时的大模型更新：它使模型陷入一个糟糕的局部最优，这反过来增加了LN的输入大小。随着训练的继续，通过LN的梯度变得越来越小，从而导致严重的梯度消失。梯度的消失使其难以脱离局部最优，并进一步使优化不稳定。相反，Post-LN-init的更新相对较小，并且对LN的输入是稳定的，这减轻了梯度消失的痛苦，使优化更加稳定。
DEEPNET: Extremely Deep Transformers Architecture  DEEPNORM的公式表示为： $$ x_{l+1}=LN(\alpha x_{l}+G_{l}(x_{l},\theta _ {l})) $$ 其中，$\alpha$是一个常数，$G_{l}(x_{l},\theta _ {l})$是参数为$\theta _ {l}$的第$l$层transformer层的子层，即注意力层或FFN层。此外，还通过$\beta $来缩放残差分支内的权重$\theta _ {l}$。 上图为在训练的早期阶段，vanilla Post-LN和DEEPNET的模型更新，可以看出，DEEPNET的更新更加稳定。
实验 Neural Machine Translation BLEU scores on the WMT-17 En-De test set</description>
    </item>
    
    <item>
      <title>EdgeViT</title>
      <link>https://gdy0924.github.io/posts/edgevit/</link>
      <pubDate>Sat, 25 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/edgevit/</guid>
      <description>EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers
Abstract  ViTs已经成为计算机视觉中卷积神经网络的一种非常有竞争力的替代架构，但其二次复杂度，限制很大。最近有很多工作重新将卷积网络的设计引入ViT，如卷积、分层架构等。但它们仍然不足以满足移动设备有限的资源需求。在本文中，我们提出了EdgeViTs，一个新的轻量级ViTs，通过引入一个高效的local-global-local（LGL）bottleneck来实现的，实现了卷积和自注意力的最优集成。
Introduction  由于ViT的二次复杂度，目前一些工作通过三种方式来降低计算量：
（1）使用层次结构逐步降低空间分辨率，减少token序列长度；
（2）控制输入token序列长度和参数共享的局部自注意机制；
（3）通过缩放因子对key和value进行降采样。
我们考虑在真实平台上运行的ViT模型有以下几个特点：Inference efficiency需要高；Model size是可负担得起的，即参数数量；Implementational friendliness实现的友好性。
提出了EdgeViTs网络，使ViTs能够在移动设备上与最好的轻量级CNN竞争，如MobileNetv2和EfficientNets。通过引入一个轻量级的local-global-local (LGL) information exchange bottleneck来实现，共包含三个操作：
（1）利用有效的深度卷积，从相邻token中聚合局部信息；
（2）形成一个稀疏的均匀分布的delegate token集，通过自注意机制实现远程信息交换；
（3）通过转置卷积，将更新后的信息从delegate token扩散到局部的的非delegate token。
Related Work Efficient CNNs  深度可分离卷积，MobileNets，ShuffleNets
Vision transformers  ViT，对其改进之一是从卷积架构中引入层级结构，一些工作还改进位置embedding，所有这些方法主要都是为了提高模型的性能。
一种有效的减少计算量的方式是减少MHSA内部的空间维度，发现key和value可以被降采样，从而更好平衡效率与精度。我们的工作通过对query进行降采样，扩展了这一想法，进一步提高了效率；一些其他的工作动态减少token数量，即在正向传递中，被认为不包含目标任务的一些token被修剪或合并在一起，从而降低了之后的整体复杂性。
Vision transformers for mobile devices  MobileViTs
EdgeViTs Overview （a）4个阶段的EdgeViT架构，每个架构由堆叠的LGL block组成；
（b）Local-Global-Local (LGL) blocks：local aggregation模块+ sparse-self-attention模块+local propagation模块。
我们采用了最近ViT变体中使用的分层金字塔网络结构，金字塔变压器模型通常会逐步降低空间分辨率，同时增加通道数量。基于此，我们深入研究了transformer block，并提出了一个更高效的bottleneck：Local-Global-Local，LGL通过稀疏注意力模块进一步减少了自注意力的开销，实现了更好的精度-延迟平衡。
Local-Global-Local bottleneck  自注意力机制可以捕捉长期依赖，但是图像的空间冗余很高，因此关注所有空间中的token，即使是在下采样的特征图中，也是低效的。因此，需要同时保留建模全局和局部的底层信息流。
我们将自注意分解为连续的模块，在不同的模块内处理token：（1）Local aggregation只聚合相邻token的信息；（2）Global sparse attention建模一组delegate token之间的信息，其中每个delegate token都被视为一个局部窗口的代表；（3）Local propagation将代表token学习到的全局上下文信息传播到具有相同窗口的非代表token中。 （a）Local aggregation：对于每个token，利用depth-wise卷积和point-wise卷积在大小为$k×k$的局部窗口中聚合信息；</description>
    </item>
    
    <item>
      <title>HaloNet</title>
      <link>https://gdy0924.github.io/posts/halonet/</link>
      <pubDate>Fri, 24 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/halonet/</guid>
      <description>Scaling Local Self-Attention for Parameter Efficient Visual Backbones
Abstract  通过对注意力机制进行修改和扩展，提出了新的网络架构HaloNets，其结果优于更大的模型，并具有更好的推理性能。
Introduction  卷积体现了局部处理的原则，以学习图像中丰富的边缘和纹理等局部空间特征；Transformer表明，注意力机制是一种有效的、计算效率搞得机制，可以捕获全局的信息。
自注意力机制有一下几个特点：（1）基于内容的交互，而不是与内容无关的卷积交互；（2）感受野大小与参数无关，而卷积中的感受野与参数有关；（3）在较大图像中能够捕获长期依赖关系；（4）灵活地处理多种类型的数据。
Models and Methods  虽然我们的模型使用自注意力机制而不是卷积来捕捉像素之间的空间局部交互，但其采用了卷积神经网络的一些重要结构特征。
Self-attention can generate spatially varying convolutional filters  最近的研究表明，具有少量头数和几何偏置的自注意力机制可以模拟卷积，这表明自注意力和卷积之间存在更深层次的关系。
我们将自注意机制和卷积写成一般的公式，对于给定的输入$x\in \mathbb{R}^{H\times W\times C_{in}}$，在$(i,j)$位置的输出为： $$ y_{ij}=\sum_{a,b\in N(i,j)}f(i,j,a,b)x_{ab} $$ 其中，$f(i,j,a,b)$是一个函数，在以$(i,j)$为中心、$k × k$为大小的局部窗口处，返回一个权重矩阵$W\in \mathbb{R}^{C_{in}\times C_{out}}$。
对于卷积和transformer，其函数分别为： $$ f(i,j,a,b)^{conv}=W_{a-i,b-j} $$ $$ f(i,j,a,b)^{self-att}=softmax_{ab}((W_{Q}x_{ij})^{T}W_{K}x_{ab}+(W_{Q}x_{ij})^{T}r_{a-i,b-j})W_{V}=p_{a-i,b-j}^{i,j}W_{V} $$
Improving the speed-memory tradeoff by relaxing translational equivariance  全局自注意，即所有的位置都相互关注，对于大多数图像尺度来说太昂贵了。但是如果用卷积中的滑动窗口思想，在每个窗口里做 local self-attention，由于每个窗口的结果都要一直存在显存里没法释放，容易造成显存溢出。
因此提出了blocked local self-attention，即将输入的特征图分成了不重复的一些 block，然后在每个 block 里做 local self-attention。如果每次只考虑block内的信息，必然会导致信息的损失，因此在计算Local Self-Attention之前，先对每个block进行的haloing操作（在边界进行填充）。下图为将$[4,4,c]$的图像，分割成大小为2的block，并且halo=1：首先将图像分割成不重叠的$[2,2,c]$block，然后由block生成query，接着halo步骤在每个block周围提取$[4,4,c]$的图像，生成key和value。最终输出的维度与block维度（query）相同。</description>
    </item>
    
    <item>
      <title>CeiT</title>
      <link>https://gdy0924.github.io/posts/ceit/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/ceit/</guid>
      <description>Incorporating Convolution Designs into Visual Transformers
Abstract  提出了一个新的网络CeiT（Convolution-enhanced image Transformer）：结合了卷积网络提取low-level特征、局部性和transformer建立长期依赖关系的优势。对原始的transformer进行了三方面的修改：（1）设计了一个I2T（Image-to-Tokens）模块，从low-level特征中提取patch，而不是ViT中直接分割得到token；（2）每个block中的FFN层替换为LeFF（Locally-enhanced Feed-Forward）层，以加强空间维度上相邻token之间的相关性；（3）提出LCA（Layer-wise Class token Attention）层用于融合多阶段的表征进行最终的预测。
Introduction  ViT的论文中提到：transformer缺少CNN固有的归纳偏置，因此在数据量不足的情况下，模型不能很好的泛化。在DeiT中，CNN的teacher网络效果更好，这可能是由于“通过蒸馏transformer继承得到了归纳偏置”。
针对卷积，主要特征是平移不不变性和局部性。然而transformer架构没有重新利用图像中存在的偏置。首先，ViT对大小为16×16或32×32的原始输入图像中的patch进行得到token，很难提取出图像中的一些低级特征（如角落和边缘）；其次，自注意力模块集中于建立token之间的长期依赖关系，而忽略了空间维度中的局部性。
为了提取低级特征，设计了一个I2T（Image-to-Tokens）模块，从生成的low-level特征中提取patch，得到的patch尺寸更小；为了增强局部性，将FFN层替换为LeFF层（Locally-enhanced Feed-Forward）；为了更好的利用自注意力机制，在transformer顶部增加了LCA层（Layer-wise Class token Attention）。
Methodology Revisiting Vision Transformer Tokenization
ViT直接将图像分割为16×16或32×32大小的patch，这样会存在两个限制：（1）难以捕捉图像中的低级信息，如边和角；（2）大的卷积核通常过度参数化，导致难以优化。
Encoder blocks
由MSA和MLP组成： $$ x&#39;=LN(x+MSA(x)) $$ $$ y=LN(x&#39;+FFN(x&#39;)) $$
Image-to-Tokens with Low-level Features 为了解决Tokenization的问题，提出了Image-to-Tokens模块，从特征图中提取patch，而不是原始图像。该模块由一个卷积层和一个最大池化层组成，公式表示为： $$ x&#39;=I2T(x)=MaxPool(BN(Conv(x))) $$ 其中，$x&#39;\in \mathbb{R}^{\frac{H}{S}\times \frac{W}{S}\times D}$，$S$是相对于原始图像的缩小倍数，将得到的特征图分割为patch，为了保证token数量相同，我们将patch的分辨率缩小为$(\frac{P}{S},\frac{P}{S})$。$S$通常为4。
I2T模块充分利用了卷积在提取low-level特征方面的优势，通过缩小patch的大小来降低embedding的训练难度。
Locally-Enhanced Feed-Forward Network  在每个block中，MSA模块不变，以捕获全局的相似性，使用LeFF层来代替原来的前馈层。 LeFF层的流程如下：
（1）对于MSA的输出$x_{t}^{h}\in \mathbb{R}^{(N+1)\times C}$，先将其分为patch token$x_{p}^{h}\in \mathbb{R}^{N\times C}$和class token$x_{c}^{h}\in \mathbb{R}^{C}$，通过一个线性层将patch token的维度增加$x_{p}^{h}\in \mathbb{R}^{N\times (e\times C)}$，e为扩展比；</description>
    </item>
    
    <item>
      <title>CaiT</title>
      <link>https://gdy0924.github.io/posts/cait/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/cait/</guid>
      <description>Going deeper with Image Transformers
Abstract  目前对图像transformer的优化研究工作还很少，在本文中，我们搭建了更深层次的transformer用于图像分类，提出了两个架构变化，显著提高了transformer的深度和精度。这使得我们的模型性能不会随着更多的深度而早期饱和。
Introduction  首先，残差结构可以定义为： $$ x_{l+1}=g_{l}(x_{l})+R_{l}(x_{l}) $$ 其中，$g_{l}$通常是恒等映射，$R_{l}$是block的主要操作。残差结构突出了体系结构设计和提高精度之间的强大相互作用。在ResNet论文中提到说，残差网络并不能提供更好的表示能力，但效果更好的原因是网络更容易训练。
Transformer架构可以表示为： $$ x_{l}^{&#39;}=x_{l}+SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+FFN(\eta (x_{l}^{&#39;})) $$ 其中，$\eta$是指LN。上述结构遵循残差连接。
如何归一化、初始化权重或初始化残差块是值得思考的，我们共讨论了4个方法，分别是：Fixup、T-Fixup、ReZero和SkipInit。
在分析了不同初始化、优化和架构设计之间的相互作用之后，我们提出了一种方法，与目前的图像变压器方法相比，它可以有效地改进更深层次的架构训练。具体的，在每个残差块的输出上添加一个可学习的对角矩阵，初始化接近为0，但不是0。我们称之为LayerScale。
其次，我们将patch之间的注意力transformer层和将patch内容传递到单个向量中的层分离，以便单个向量进行分类。这种显式的分离避免了在处理class embedding时引导注意力过程的矛盾目标。我们称之为CaiT（Class-Attention in Image Transformers）。
Deeper image transformers with LayerScale  在ViT和DeiT的工作中，没有证据表明深度更有效：更深的ViT架构性能较低，而DeiT只考虑12层的变压器。
下图为我们比较的几个变体： （a）原始的结构：ViT和DeiT中使用的，即pre-norm结构，层归一化在残差分支的开始部分，但在原始的attention文章中（Attention is all you need）是post-norm，但在我们的实验中，post-norm并不能收敛。
（b）ReZero/Skipinit/Fixup：在残差块的输出部分引入了可学习的标量权重$\alpha _ {l}$，同时出去了pre-norm，可以表示为： $$ x_{l}^{&#39;}=x_{l}+\alpha _ {l} SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+\alpha _ {l}^{&#39;} FFN(\eta (x_{l}^{&#39;})) $$ ReZero是将该参数初始化为0；Fixup将其初始化为1，并进行了其他的修改；但在我们的实验中，这些都没有收敛。
（c）我们的经验观察是，消除pre-norm是使在Fixup中的训练不稳定的原因，因此我们重新引入，从之后的实验中可以看到，是有效的。
（d）Our proposal LayerScale：残差块的输出进行每个通道的相乘，而不是单个标量。即对Self-attention或者FFN的输出乘以一个对角矩阵，目的是给不同的channel乘以不同的$\lambda $值，同时保持Layer Normalization： $$ x_{l}^{&#39;}=x_{l}+diag(\lambda _ {l,1},&amp;hellip;,\lambda _ {l,d}) SA(\eta (x_{l})) $$ $$ x_{l+1}=x_{l}^{&#39;}+diag({\lambda }&#39;_ {l,1},&amp;hellip;,{\lambda }&#39;_ {l,d}) FFN(\eta (x_{l}^{&#39;})) $$ 其中，$\lambda _ {l,1}$和${\lambda }&#39;_ {l,1}$都是可学习的参数权重。</description>
    </item>
    
    <item>
      <title>LeViT</title>
      <link>https://gdy0924.github.io/posts/levit/</link>
      <pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/levit/</guid>
      <description>LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference
Abstract  设计了一系列图像分类架构，以权衡在高速状态下的精度和效率。利用注意力架构，并将卷积网络的原理应用于transformer，并且引入了注意力偏置，一种新的引入位置信息的方式。提出LeViT：一种用于快速推理的图像分类混合网络。
Introduction  在本文中，探索中小型架构中比ViT/DeiT更好的权衡，目前许多工作旨在减少分类器和特征提取器的内存占用，但其推理速度也很重要。探索一个基于视觉transformer的模型架构，具有更好的推理速度。重新引入卷积，以代替类似卷积的transformer组件，同时利用池化金字塔结构替换transformer的柱状结构。
Contributions：
（1）使用注意力机制作为降采样方式的多阶段transformer架构；
（2）计算更高效的patch，减少了第一层中的特征数量；
（3）一个可学习的注意力偏置代替ViT中的位置embedding；
（4）一个重新设计的Attention-MLP block，在给定的计算时间中提高模型学习能力。
Related work Transformers The vision transformer (ViT) Positional encoding  在我们的工作中，用隐式编码空间信息的位置偏置来代替显式的位置编码。
Attention along other mechanisms  在图像分类方面，最近与我们并行的工作是PVT，其设计很大程度上受到ResNet的启发，主要用于处理分割任务。同时还有T2T网络，与PVT类似。但是这些工作没有平衡精度与推理时间。
Motivation  将patch的线性投影看作卷积操作，在一个卷积网络ResNet-50上嫁接一个transformer网络DeiT-S。
我们裁剪了ResNet-50的上部阶段，并同样减少了DeiT层的数量，由于裁剪的ResNet产生的特征图比DeiT输入的14×14的特征图更大，因此在它们之间引入了池化层。
Results
嫁接的架构比单独使用DeiT和ResNet-50的精度更好，参数量最少并且效果最好的结构是使用ResNet-50的前两个阶段。 与DeiT模型相比，卷积层模型在早期阶段表现出更快的收敛速度；并且嫁接模型在早期的收敛类似于卷积网络，后期切换到transformer结构的收敛速度。
Discussion
变压器架构需要与卷积阶段更紧密地合并，而不是仅仅是嫁接。
Model Design principles of LeViT  LeViT建立在ViT和DeiT架构上，引入那些被证明对卷积网络有效的部件。第一步是得到一个兼容的表征，不考虑class token的话，ViT就是处理特征图的堆叠层，中间的token可以看作FCN架构中的特征图。因此，应用于特征图的操作（池化/卷积）同样可以应用于DeiT的中间表征。
在本文中，我们优化了体系结构，而不一定要最小化参数的数量。ResNet网络比VGG网络更有效率的原因之一是在其前两个阶段以相对较小的计算应用较强的分辨率降低，即当特征图到达ResNet的第三阶段时，它的分辨率已经缩小到足够小，从而降低了计算成本。
LeViT components Patch embedding  在LeViT中，使用4层3×3的卷积（步幅为2）来对特征图的分辨率进行降低，通道数分别为3、32、64、128、256。
No classification token  为了使用BCHW张量格式，删除了class token。与卷积网络类似，通过在最后一个特征图上进行平均池化，来实现最终的分类。为了实现蒸馏，分别训练不同的头，在测试时，使用两个头的输出平均值。
Normalization layers and activations  ViT中的FC层相当于1×1的卷积，同时在每个注意力层和MLP层之前使用层归一化。对于LeViT，每个卷积之后都进行批归一化。</description>
    </item>
    
    <item>
      <title>CoAtNet</title>
      <link>https://gdy0924.github.io/posts/coatnet/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/coatnet/</guid>
      <description>CoAtNet: Marrying Convolution and Attention for All Data Sizes
Abstract  在本文中，我们证明了，虽然transformer有更大的潜力，但是由于缺乏正确的归纳偏置，其泛化性可能要比卷积网络更差。为了结合两种网络的优势，我们提出了CoAtNet（Convolution and self-Attention），是一种混合模型：（1）深度逐通道卷积和自注意力机制通过简单的相对注意力来进行统一；（2）有规则的叠加卷积层和注意力层在提高模型的泛化性和效率方面有惊人的效果。
Introduction  虽然ViT的效果很好，但他在数据集量少的情况下，仍然不如卷积网络。这表明，transformer层可能缺少卷积网络所拥有的归纳偏置，因此需要大量的数据和计算量来弥补。最近很多工作，试图将归纳偏置来引入transformer，通过引入局部感受野、引入卷积层或带有卷积的FFN层。
我们的研究表明，卷积网络具有更好的归纳偏置先验，并且收敛速度快，而注意力机制的模型能力/容量更高，更能从大型的数据集上获益；结合卷积和注意力，可以更好的提高模型的泛化性和能力。我们提出两个想法：（1）逐通道的卷积可以有效的合并到注意力层中；（2）简单的堆叠卷积层和注意力层，可以提高模型的泛化性和能力。基于以上两点，我们提出了CoAtNet，结合了卷积和transformer的优势。
Model  我们将如何最有效的组合卷积和注意力这个问题分为两部分：
（1）如何在一个block中结合；
（2）如何将不同类型的block垂直叠加在一起，形成一个完整的网络。
Merging Convolution and Self-Attention  对于卷积，我们关注MBConv（如下图所示），MBConv主要有两个特点：
（1）采用了Depthwise卷积，因此相比于传统卷积，Depthwise Conv的参数能够大大减少；
（2）采用了“inverted bottleneck”的结构，即先升维再降维，在原来的resnet结构中，输入维度是大的，先降维，然后为了能够相加，再升维，在transformer block的FFN层也是先升维再降维。
除了inverted bottleneck的相似性，我们还注意到，深度卷积（逐通道卷积）与自注意力机制都可以表示为一个预先定义的感受野中的每维的加权和。
具体来说，卷积依赖一个固定的卷积核从局部的感受野接收信息： $$ y_{i}=\sum_{j\in L(i)}w_{i-j}\odot x_{j} $$ 其中，$x_{i},y_{i}$分别表示位置$i$的输入和输出，$L(i)$表示$i$的局部邻域。
相比之下，自注意力的感受野为整个空间位置，并归一化两两的相似性： $$ y_{i}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})} x_{j} $$ 其中，$G$为全局的空间。
我们先分析一下他们的优势和劣势：
（1）逐通道卷积核$w_{i-j}$是一个独立于输入的静态的值，而注意力权重$A_{i,j}$是依赖于输入的动态的值，因此，自注意力更容易捕获到不同位置的复杂交互信息，但是这种灵活性，带来了过拟合的风险，特别是在数据有限的情况下；
（2）对于给定的任何位置对$(i,j)$，卷积权重$w_{i-j}$只关注它们之间的相对位移，即$i-j$，这被称为平移不变性，而transformer缺乏这一特性，这也可以解释为什么在数据集不多的情况下，卷积网络通常比transformer更好；
（3）感受野的大小是注意力机制核卷积之间最重要的区别，更大的感受野可以提供更多的上下文信息，可能会提升网络的性能，因此，在视觉中使用自注意力机制是为了全局的感受野，然而自注意力机制的计算复杂度为二次方，也是需要权衡的。 上表为卷积和自注意力的优势比较，那么一个理想的模型应该是结合以上三个优势，基于上述两个公式，一个简单的想法就是：将静态全局卷积核与自注意力权重矩阵相加，在softmax之前或者之后，即： $$ y_{i}^{post}=\sum_{j\in G}(\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})}+w_{i-j}) x_{j} $$ $$ y_{i}^{pre}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j}+w_{i-j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k}+w_{i-k})} x_{j} $$ 为了在不增加参数数量的情况下引入全局卷积核，我们将$w_{i-j}$设置为一个标量，而不是公式1中的一个向量。我们使用pre-normalization作为CoAtNet模型的关键组成部分。
Vertical Layout Design  将卷积和注意力结合起来之后，接下来要考虑如何堆叠整个网络，如果直接使用上述公式，其复杂度是二次方的，会影响速度，因此，我们考虑了三种方式：</description>
    </item>
    
    <item>
      <title>VOLO</title>
      <link>https://gdy0924.github.io/posts/volo/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/volo/</guid>
      <description>VOLO: Vision Outlooker for Visual Recognition
Abstract  限制ViT的性能的主要原因是：对细粒度的特征编码效率较低，为了解决这个问题，我们提出新的outlook attention，并提出了一个简单的架构Vision Outlooker (VOLO)。与粗粒度的全局注意力不同，outlook attention能够有效的提取细粒度特征和上下文信息。
Introduction  限制ViT的一个主要因素是：分割得到的patch token，提取细粒度特征和引入上下文编码是低效率的。在本文中，我们提出了一种新的注意力机制：Outlooker，能够有效的提取细粒度特征和上下文信息；同时，利用线性投影，而摆脱了二次复杂度。
基于Outlooker，我们提出了一个简单的模型架构：VOLO。VOLO通过一个两阶段的体系结构，实现了细粒度的token表示和全局信息的聚合；具体来说，给定一个224×224的图像，在使用自注意力机制在粗粒度上构建全局依赖以前，先在较小的patch上，使用多个Outlooker在细粒度特征上进行token表示。
Method  可以分为两阶段：
（1）堆叠的Outlooker，以生成细粒度级别的token表征；
（2）一系列transformer block聚合全局信息。
Outlooker  Outlooker由一个对空间信息编码的outlook attention层和一个用于通道之间信息交互的MLP层组成。
对对于给定的C维输入$X\in \mathbb{R}^{H\times W\times C}$，Outlooker可以写作： $$ \tilde{X} = OutlookAtt(LN(X))+X $$ $$ Z=MLP(LN(\tilde{X}))+\tilde{X} $$
Outlook Attention 如上图所示，主要思想有两点：
（1）每个空间位置的局部特征足以生成注意力权重系数来聚合其临近的特征；
（2）密集的局部空间特征聚集可以有效的编码细粒度信息。
对于每个空间位置$(i,j)$，outlook注意力在以$(i,j)$为中心、大小为$K×K$的局部窗口内，计算相邻位置的相似性。与自注意力机制中使用Q-K矩阵相乘来得到权重系数不同，outlook通过reshape来简化这个过程。
对于给定的输入$X$，每个C维的token首先经过线性投影，使用两个线性层，$W_{A}\in \mathbb{R}^{C\times K^{4}}$和$W_{V}\in \mathbb{R}^{C\times C}$，分别得到outlook权重$A\in \mathbb{R}^{H\times W\times K^{4}}$和value$V\in \mathbb{R}^{H\times W\times C}$，$V_{\Delta _ {i,j}}\in \mathbb{R}^{C\times K^{2}}$表示以$(i,j)$为中心的局部窗口内的值： $$ V_{\Delta _ {i,j}}=\lbrace V_{i+p-\left \lfloor \frac{K}{2} \right \rfloor,j+q-\left \lfloor \frac{K}{2} \right \rfloor} \rbrace $$ Outlook attention</description>
    </item>
    
    <item>
      <title>Early Convolutions Help Transformers See Better</title>
      <link>https://gdy0924.github.io/posts/early-convolutions-help-transformers-see-better/</link>
      <pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/early-convolutions-help-transformers-see-better/</guid>
      <description>Early Convolutions Help Transformers See Better Abstract  ViT对优化器的选择非常敏感，如Adam vs. SGD，并且对优化器的超参数和训练的时间长度也很敏感，相比之下，卷积网络更容易优化。在本文中，我们推测是由于ViT模型的patchify stem，对输入图像应用卷积核大小为p×p、步幅为p的卷积。这种大卷积核和大步幅，与卷积的设计思路是相违背的。为了测试这种设计是否会导致问题，我们用少量堆叠的3×3大小、步幅为2的卷积代替ViT中的方式。虽然提出的模型和ViT大多数部分计算相同，但是我们发现这种早期的细小的变化导致了模型在对优化器的敏感性和最终的准确性的不同。在ViT中使用卷积stem显著提高了优化器的稳定性和性能。
Introduction  ViT使用多头自注意力机制的全局处理代替了卷积中的局部处理，我们希望这种设计有潜力提高视觉表征。但是，ViT与卷积网络还是有很大差异，ViT对优化器的选择、针对特定数据集的超参数、训练的时间长度、网络的深度等等，都很敏感。
相比之下，卷积网络就更好优化。在本文中，我们加深问题出现在ViT早期对图像的处理阶段，即ViT将图像分割为p×p大小的不重叠的patch，patchify stem通过卷积核大小为p×p、步幅为p的卷积来实现，但是这种大卷积核和大步幅，与卷积的设计思路是相违背的，卷积中，最佳的收敛方式是小卷积核的堆叠，如卷积核大小为3×3，步幅为2。
为了验证这一假设，我们将ViT早期处理图像的方式进行改变，使用5层卷积来代替，如下图所示。为了对其计算量，删掉了一个transformer block。我们观察到，尽管两种ViT设计中的绝大多数计算量是相同的，但早期处理的微小变化导致了两个模型在对优化器的敏感性和最终的准确性方面有所不同。
我们推测，将卷积添加在ViT的早期阶段可能是一个关键，与hybrid ViT相比，hybrid ViT使用了40个卷积层，与原始的ViT并没有改进，与ConViT（ConViT:Improving vision transformers with soft convolutional inductive biases.）的发现一致：早期的transformer块比后来的transformer块更喜欢学习局部信息。
探索CNN与ViT的混合模型并不是我们的目标，相反，我们证明了简单的使用小卷积就足以显著改善模型的优化。
Related Work Convolutional neural networks (CNNs)  卷积网络通常使用SGD进行训练，并且是很容易优化的。
Self-attention in vision models  transformer使用多头自注意力机制，对全局信息进行处理，比卷积更加通用。
Vision transformer (ViT)  ViT将图像分割成不重叠的大小为16×16的patch，通过线性层的到patch embedding输入到模型中，在使用大规模数据集时获得了很好的效果，但其对一些训练细节很敏感，如相比于SGD，用Adam的效果更；与CNN相比，ViT通常时更难优化的。
ViT improvements  更多的正则化和更强的数据增强，可以提高ViT的精度，我们也探索了其他的一些细节。
Discussion  与引入局部先验不同，我们的研究重点是对ViT进行最小的修改来提高可优化性，并得到结果：只在stem上添加5层卷积，即可使ViT通过Adam和SGD进行优化，同时对学习速率和权重衰减等超参数不那么敏感。
Vision Transformer Architectures  我们将输入transformer块之前的网络部分称为stem。ViT的stem是一个步幅为p、大小为p×p的卷积，称之为patchify stem，我们保留卷积的stem，使用多层的重叠卷积来代替。
我们使用最经典的堆叠的3×3卷积，并最后添加一个1×1卷积以匹配维度，将224×224的图像下采样到14×14，并且遵循3×3卷积，步幅为2。
将CNN与ViT进行混合，将resnet-50作为ViT的stem，共有40层卷积层，但在我们的工作中，只有5-7层，并且我们的目标不是设计混合ViT模型，而是研究遵循卷积网络设计思想，用最小的卷积stem来代替patchify stem的效果。
下表为ViT与本文提出的修改ViT的模型结构：$ViT_{p}$：原始的ViT模型；$ViT_{c}$：用卷积替换的ViT模型；为了匹配复杂度，$ViT_{c}$中减少了一个transformer block。 Stability Experiments  RegNetY是效果最好的CNN网络，并且很好优化。</description>
    </item>
    
    <item>
      <title>Visual Transformers综述</title>
      <link>https://gdy0924.github.io/posts/visual-transformers%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/visual-transformers%E7%BB%BC%E8%BF%B0/</guid>
      <description>A Survey of Visual Transformers
Abstract  transformer是一种基于注意力机制的encoder-decoder模型，首次在NLP领域提出，后来被应用在CV领域中，并证实了transformer的有效性。在本文中，根据三种不同的任务分类，总结了100多个不同的transformer模型，并提出了未来三个有前景的研究方向。
Introduction  transformer利用注意力机制，已经成为许多NLP任务的主要模型，最近的主要模型是自监督，首先在大型数据集上进行预训练，然后针对小样本进行下游任务的微调，如GPT、BERT等。
在CV领域，transformer出现之前，一直是卷积网络占据主导，后来通过向卷积网络中添加注意力模型或使用纯注意力模型，但在流行的基准上，纯注意力模型仍然低于最先进的CNN模型。 如上图所示，在过去一年多中，已经提出了数百个基于transformer的各种视觉模型，我们将基本任务分为三类：分类、检测和分割；同时根据数据流进行分类：图像、点云和多流数据；如下图所示：
本文中，旨在对视觉transformer进行更全面的回顾和更系统的分类：
（1）Comprehensiveness &amp;amp; Readability（全面性和可读性）：选择具有代表性的进行详细介绍，简单介绍其他工作，从单一角度对模型进行分析，并对他们的联系进行分析；
（2）Intuitive Comparison（更直观的比较）：对不同的数据集和限制进行了多个横纵对比，并为每个任务总结了一系列的组件，包括：拥有层次backbone的浅层局部卷积，用于检测器的空间注意力加速，用于分割的通用掩码预测机制；
（3）In-depth Analysis（深入分析）：从以下几个方面提供讲解：从传统的序列任务到视觉任务的转换过程，视觉transformer和其他神经网络模型之间的关系，在不同任务中采用的可学习embedding之间的相关性（class token、object query、mask embedding），最后，总结了一些未来的研究方向。
Original Transformers  原始的transformer应用于sequence-to-sequence的自回归任务中，使用encoder-decoder架构，使用多头自注意力机制和前馈层完全代替了递归和卷积，如下图所示：
(Multi-Head) Attention Mechanism  单头自注意力机制分为两部分：将输入序列映射成三个不同的向量：Q、K和V；利用Q和K进行查询，将注意力分数分配给V，并更新输出向量，其计算公式如下所示： $$ Q=XW^{Q},K=XW^{K},V=XW^{V} $$ $$ Attention(Q,K,V)=Softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$ 多头自注意力机制就是并行多个单头自注意力机制，并对输出结果拼接起来得到最终输出。
Position-wise Feed-Forward Networks  将多头自注意力机制的输出经过两个前馈层FFN： $$ FFN(x)=RELU(W_{1}x+b_{1})W_{2}+b_{2} $$
Positional Encoding  由于transformer对输入进行并行的处理，因此忽视了序列的顺序，为了引入顺序信息，提出添加一个位置embedding。
Transformer Model 上图展示了encoder-decoder架构的整体transformer模型，由$N$个encoder块组成，每个encoder块包含两个子层：MHSA和FFN；还有$N$个decoder块，与encoder相比，decoder块中添加了多头交叉注意力层，以聚合decoder的embedding和encoder的输出。所有子层都采用残差连接和层归一化。
Transformer For Classification 随着transformer在NLP领域的发展，许多研究试图将transformer用于图像分类，如上图所示。首先介绍全注意力网络和ViT，其证明了transformer的有效性；接着讨论了利用transformer增强卷积网络的方式提高卷积网络的性能；根据卷积网络的层次结构，分层transformer用金字塔结构取代了固定分辨率的柱状结构；最后对其进行了直观的比较和讨论。
Original Visual Transformer  SANet用注意力完全取代卷积，提出了Stand-Alone self-attention network，给定resnet架构，将每层中的卷积替换为局部空间自注意力层；Fully-Attentional Network由一个transformer架构和二次位置编码组成，并证明了一个卷积层可以用一个带有相对位置编码的多头自注意力层来近似。
ViT进一步探索了大规模预训练transformer的有效性，ViT将输入图像分割为patch，并将其flatten成为序列，添加位置embedding作为输入，如下图所示。与BERT类似，引入class token进行最终的分类。然而，在有限的训练样本限制下，ViT泛化能力容易受到限制。 Transformer Enhanced CNNs  最近的一些方法试图将transformer集成到卷积网络中，以增强特征表示。</description>
    </item>
    
    <item>
      <title>T2T</title>
      <link>https://gdy0924.github.io/posts/t2t/</link>
      <pubDate>Wed, 08 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/t2t/</guid>
      <description>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
Abstract  ViT将图像分割为patch，经过多个transformer层对特征进行提取，实现分类任务。但是，在中型数据集上从头开始训练时，ViT的性能不如CNN网络。可能有以下几个原因：（1）简单的将图像分割为patch作为token，无法建模重要的局部信息，如相邻像素之间的边和线等，导致训练样本效率低；（2）ViT的backbone中有冗余的注意力计算，导致计算量大，导致对于固定计算预算和有限训练样本不能提取更加丰富的特征。
为了解决上述问题，我们提出了新的架构Tokens-To-Token Vision Transformer（T2T-ViT），包括：（1）一个层级的Tokens-To-Token的转换，通过递归将相邻的tokens聚合成一个token，即Tokens-To-Token，这样就可以对token周围的局部结构进行建模，并减少token长度；（2）基于CNN架构的deep-narrow视觉transformer架构。
Introduction  虽然ViT证明了transformer结构在视觉任务中很有前景，但其在中小型数据集从头训练时，效果不如卷积网络。主要有两个限制：（1）输入图像直接生硬的分割为patch，无法建模局部结构，如边和线，因此需要更多的训练样本；（2）ViT的backbone过于冗余，没有与CNN设计相类似，因此导致特征提取丰富度有限并且模型训练困难。 为了验证假设，对ViT和ResNet的特征图进行了可视化，如上图所示：可以看出，ResNet捕捉的特征从局部（conv1）逐渐延伸到全局（conv25），然而，ViT的局部信息建模的很差，但全局信息被捕获到了。因此，可以说ViT直接分割图像为patch，忽视了局部结构；ViT中许多通道为0（红色框所示），表明ViT的backbone不如ResNets。
我们提出了新的transformer架构，解决上述问题。（1）与ViT中的直接分割patch不同，我们提出了一个渐进的模块，逐步将相邻的tokens聚合为一个token，可以对周围token的局部信息进行建模，并迭代地减少token的长度，具体地说，在T2T模块中，transformer层的输出被reshape成图像，然后进行有重叠的分割，最后通过flatten将patch聚合在一起；（2）为了找到更高效的backbone，参考CNN架构设计，提出了更深更窄的transformer backbone。
Contributions：
（1）提出的transformer架构（T2T+高效backbone）在不使用预训练的情况下，效果好于卷积网络；
（2）提出了一种新的渐进的token方法，可以建模token之间的局部结构；
（3）证明CNN的更深更窄的架构同样适用于transformer架构，并提出更高效的transformer backbone。
Related Work Transformers in Vision  ViT证明了纯transformer架构也可以在图像分类上有很好的效果，然而，ViT依赖于大规模数据集，需要进行模型预训练，并且需要大量的计算资源；我们提出的T2T-ViT不需要预训练，也不依赖于大规模数据集。
Tokens-to-Token ViT  为了解决ViT中简单token和backbone低效的问题，提出了Tokens-to-Token Vision Transformer（T2T-ViT），可以逐步将图像转化为patch，并使用更加高效的backbone。
T2T-ViT由两部分组成：（1）层级的Tokens-to-Token模块，对图像的局部信息进行建模，逐步减少token的长度；（2）更高效的T2T-ViT backbone。
Tokens-to-Token: Progressive Tokenization  Tokens-to-Token模块是为了解决ViT中简单分割patch的问题，逐步将图像分割为patch，从而对局部信息进行建模，迭代的减少token的长度。每个T2T包含两个步骤：Re-structurization和Soft Split (SS)，如下图所示： Re-structurization  来自前一层的输出token序列，是通过MSA和MLP处理后得到的，即： $$ T^{&#39;}=MLP(MSA(T)) $$ 然后得到的$T^{&#39;}$将被reshape成空间维度中的图像： $$ I=Reshape(T^{&#39;}) $$ Reshape是将$T^{&#39;}\in \mathbb{R}^{l\times c}$转换为$I\in \mathbb{R}^{h\times w\times c}$，其中，$l$是$T^{&#39;}$的长度，$h,w,c$是图像的高、宽和通道数，并且$l=h×w$
Soft Split  在reshape成图像$I$之后，对其图像进行Soft Split，以对局部信息进行建模，减少token的长度。
将图像分割为重叠的patch，每个patch都可以与相邻的patch有关联，分割得到的tokens被连接为一个token，从而从周围的像素中聚合局部信息。</description>
    </item>
    
    <item>
      <title>TNT</title>
      <link>https://gdy0924.github.io/posts/tnt/</link>
      <pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/tnt/</guid>
      <description>Transformer in Transformer
Abstract  transformer利用注意力机制学习数据的特征，在视觉transformer中，通常将图片划分为局部patch，然后计算patch之间的关系表示。由于图像具有丰富的细节信息，因此分割patch的粒度不足以提取不同尺度和位置的物体信息。在本文，我们提出局部patch的内部信息对构建高性能的transformer是很重要的，提出了一种新的架构Transformer iN Transformer（TNT）。我们将局部patch视为&amp;quot;visual sentences&amp;quot;，将更小的patch视为&amp;quot;visual words&amp;quot;，每个单词的注意力将用给定句子中的其他单词来计算，将单词和句子的特征进行聚合，以提高精度。
Introduction  虽然transformer在视觉领域已经有了许多不错的模型，但是在目前的模型中，大多遵循ViT中的表示方法，即将输入图像分割为patch，这种方式可以有效的捕捉视觉序列信息，并计算不同patch之间的注意力。局部patch可以找到它们之间的关系和相似性，但是，一些子patch也有很高的相似性，因此，我们提出了一种更精细的图像分割方式，生成视觉序列，以提高精度。
在本文中，我们提出了一种新的架构：Transformer iN Transformer（TNT），如下图所示。为了增强视觉表征能力，首先将输入图像划分为patch，即visual sentences，然后再将patch进行划分，形成visual words。除了之前的transformer架构提取visual sentences的特征和注意力计算，我们在架构中嵌入了一个子transformer，以提取较小的visual words的特征和细节。每个visual sentences中的visual words使用共享网络来计算注意力，然后，将visual words聚合成相应的visual sentences，该部分的class token也用于后续的分类。通过TNT模型，可以提取细粒度的视觉信息。我们对基准测试和下游任务都进行了一系列实验，证明了TNT的有效性。 Approach Preliminaries  在transformer中，包括多头自注意力机制MSA、MLP和层归一化LN，整个计算流程如下所示： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$ $$ MLP(X)=FC(\sigma (FC(X))),FC(X)=XW+b $$
Transformer in Transformer  给定一个2维的图像，将其均匀的分割成$n$个patch，$X=[X^{1},X^{2},&amp;hellip;,X^{n}]$，ViT使用transformer来处理patch序列，而没有考虑patch内部的结构。我们提出的TNT网络，学习图像的全局和局部信息。在TNT中，将patch看作图像的visual sentences，每个patch被进一步划分成$m$个子patch，即：一个visual sentences由一系列的visual words组成： $$ X^{i}[x^{i,1},x^{i,2},&amp;hellip;,x^{i,m},] $$ 其中，$x^{i,j}\in \mathbb{R}^{s\times s\times 3}$表示第i个visual sentences中的第j个visual words，$(s,s)$是子patch的尺寸大小。通过线性层，将visual words转换为一系列的word embedding： $$ Y^{i} \rightarrow [y^{i,1},y^{i,2},&amp;hellip;,y^{i,m},],y^{i,j}=FC(Vec(x^{i,j})) $$ 其中，$Vec()$是向量化操作。
在TNT中，有两个数据流：一个数据流处理visual sentences，另一个数据流处理每个sentence中的visual words。对于word embedding，使用transformer层来处理： $$ Y_{l}^{&amp;lsquo;i}=Y_{l-1}^{i}+MSA(LN(Y_{l-1}^{i})) $$ $$ Y_{l}^{i}=Y_{l}^{&amp;lsquo;i}+MLP(LN(Y_{l}^{&amp;lsquo;i})) $$ 转换后的word embedding表示为$Y_{l}=[Y_{l}^{1},Y_{l}^{2},&amp;hellip;,Y_{l}^{n}]$，可以看作是inner transformer block，表示为$T_{in}$。这个过程计算两个visual words之间的注意力来捕捉关系，如在人脸的一个patch中，眼睛对应的word与眼睛的其他word更相关。</description>
    </item>
    
    <item>
      <title>FLASH</title>
      <link>https://gdy0924.github.io/posts/flash/</link>
      <pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/flash/</guid>
      <description>Transformer Quality in Linear Time
Abstract  针对transformer处理长序列文本的弱点，提出了一个简单的层：门控注意力单元（ gated attention unit），使用一个单头的注意力即可；提出了一种线性近似的方法来实现这一层；该模型称为FLASH（Fast Linear Attention with a Single Head）。
Introduction  由于transformer的二次复杂度，大多数模型限制在较短的上下文尺寸上，这种限制使得transformer无法处理长期信息。已经有许多工作提出更有效地加速transformer，我们从实践的角度来研究这个问题，并发现现有的有效的注意模型至少存在以下缺点之一：
（1）Inferior Quality：当对transformer进行一些简单的修改之后，有效的注意力模型或多或少会导致效果的下降；
（2）Overhead in Practice：由于有效的注意模型经常使transformer层复杂化，并需要大量的数据/内存格式化操作，因此在GPU或TPU等加速器上，它们的理论复杂性和实验速度之间可能存在明显的差距；
（3）Inefficient Auto-regressive Training：大多数注意力线性化模型在推理过程中都喜欢快速的decoder，但在语言建模等自回归任务上的训练速度可能非常慢，这主要是由于类似RNN的顺序状态更新，使得在训练期间无法充分利用现代加速器。
我们提出了一个新的模型：在效果上和transformer相同，而且具有线性可扩展性。提出模型称为FLASH，分两个步骤提出：
（1）提出了一个有效近似的新的层：引入门控机制来弱化自注意力机制，形成了门控自注意力单元（Gated Attention Unit，GAU），与transformer层相比，GAU的每一层都很简单，更重要的是，其效果对注意力的依赖性较小，GAU的单头、无softmax的模型与transformer效果相同；
（2）提出了一种有效的方法来近似GAU中的二次复杂度，使得其复杂度变为线性的，关键思想就是将token分块，在块中使用二次注意力，块间使用线性注意力。
Gated Attention Unit 我们提出了门控注意力单元（Gated Attention Unit，GAU），比transformer更简单但更有效的层。
Vanilla MLP  transformer中的MLP层可以表示为： $$ O=\phi (XW_{u})W_{o} $$ 其中，输入为$X\in \mathbb{R}^{T\times d}$。
Gated Linear Unit (GLU)  GLU是一种MLP的变体，并已经被应用于transformer中，如上图中的左边所示。 $$ U=\phi_{u} (XW_{u}),V=\phi_{v} (XW_{v}) $$ $$ O=(U\odot V)W_{o} $$ 其中，$\odot$表示元素级乘法，在GLU中，每一个$u_{i}$都被另一个由相同token所关联的$v_{i}$所门控。
Gated Attention Unit (GAU)  关键思想是将注意力机制和GLU构建为一个统一的层，并尽可能多地共享它们的计算量，如上图中的右边所示。</description>
    </item>
    
    <item>
      <title>CrossViT</title>
      <link>https://gdy0924.github.io/posts/crossvit/</link>
      <pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/crossvit/</guid>
      <description>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification
Abstract  与卷积网络相比，Vision Transformer在图像分类方面取得了很好的效果。因此，本文研究了如何在transformer中学习多尺度的特征表示。提出了一种双分支transformer来融合不同大小的图像patch，以产生更强的图像特征，该方法利用两个不同分支分别处理小patch和大patch token，然后这些token通过注意力多次融合以相互补充。为了减少计算量，提出了一个简单而有效的基于交叉注意力的token融合模块，对每个分支使用一个token作为query，与其他分支交换信息。
Introduction  在本文中，我们研究了如何让transformer模型学习多尺度的特征表示，以进行图像识别。基于多分支CNN网络的有效性，如Big-Little Net和 Octave convolutions，我们提出了一种双分支transformer来处理不同大小的图像patch，以产生更强的视觉特征用于图像分类。我们利用两个不同复杂度的网络分支，分别处理大patch和小patch，这些token被融合在一起以相互补充。
该研究的主要重点是：适合于视觉transformer的特征融合方法，我们通过一个有效的交叉注意力模块来实现，其中每个transformer分支创建一个non-patch token作为agent，通过注意力机制与另一个分支交换信息。
Contributions：
（1）提出了一种新的双分支视觉transformer来提取多尺度特征表示用于图像分类；
（2）提出了一种简单而有效的基于交叉注意力的token融合方法，并且复杂度是线性的；
（3）性能优于或等于基于ViT的一些并行工作。
Related Works  我们的工作涉及三个主要的研究方向：带有注意力机制的卷积神经网络、视觉transformer和多尺度神经网络。
CNN with Attention  不少工作将CNN与不同形式的自注意机制相结合，尽管有很好的结果，但由于其复杂性，关注范围限制在局部区域。与将卷积和自注意机制相结合的方法相比，我们的工作是完全建立在自注意力网络之上的，如ViT。
Vision Transformer  受transformer在机器翻译方面成功的启发，仅依赖于transformer层的无卷积模型已经在计算机视觉中传播。ViT是第一个基于transformer的方法来超过卷积网络进行图像分类的模型。与其他方法不同，我们提出了一种双路径架构来提取多尺度特征，以使用视觉transformer进行更好的视觉表示。
Multi-Scale CNNs  多尺度特征表示在计算机视觉很常用，例如，图像金字塔，尺度空间表示，从粗粒度到细粒度的方法等。多尺度特征表示被用于多尺度的目标检测和识别。虽然多尺度特性已经证明有利于卷积网络，但它对视觉transformer的适用性仍然是一个新的未得到解决的问题。
Method  我们的方法是建立在ViT的基础上的。
Overview of Vision Transformer  ViT将图像分割成patch，线性投影成token，额外添加一个class token，将token序列加入位置embedding后输入网络，最后使用class token进行分类。transformer layer包括多头自注意力层MSA和前馈层FFN。
Proposed Multi-Scale Vision Transformer  patch的会影响ViT的准确性和复杂度：使用细粒度的patch，ViT可以表现得更好，但会导致更高的FLOPs和内存消耗（patch大小为16的ViT比patch大小为32的ViT的结果高出6%，但FLOPs也高出4倍）因此，我们提出的方法试图利用细粒度patch的优势，同时平衡复杂性。即引入了一个双分支ViT，其中每个分支处理不同的patch大小，然后提出了一个简单而有效的模块来融合不同分支之间的信息。 如上图所示，该网络由K层多尺度Transformer Encoder组成，每个encoder包含两个分支：
（1）L-Branch：大的（主要的）分支，处理粗粒度的patch（$P_{l}$），具有更多的Transformer Encoder和更大的embedding维度；
（2）S-Branch：小的（互补的）分支，处理细粒度的patch（$P_{s}$），具有更少的Transformer Encoder和更小的embedding维度。
两个分支被融合$L$次，并使用额外添加的class token进行预测。
Multi-Scale Feature Fusion 有效的特征融合是学习多尺度特征表示的关键。我们提出了四种不同的融合策略，如上图所示。$x^{i}$是分支$i$的token序列，$i$可以是大（主要）分支的$l$，也可以是小（互补）分支的$s$，$x_{cls}^{i}$和$x_{patch}^{i}$分别代表分支$i$的class token和patch token。</description>
    </item>
    
    <item>
      <title>LongFormer</title>
      <link>https://gdy0924.github.io/posts/longformer/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/longformer/</guid>
      <description>Longformer: The Long-Document Transformer
Abstract  基于transformer的模型由于自注意力操作，无法处理长序列，因为其计算量为$O(L^{2})$。为了解决这一问题，我们引入LongFormer，该注意力机制的计算量随序列的长度呈线性增长；其将局部窗口注意力（local windowed attention）与任务特定的全局注意力（task motivated global attention）相结合。
Introduction  自注意力机制可以从整个序列中捕获上下文信息，虽然效果很好，但其计算量和内存使用量很大，与序列长度的平方成正比。为了解决上述问题，我们提出了改进的transformer架构，其自注意力操作的计算量与序列长度呈线性增长，如下图所示： 对于长文本分类、问题回答等任务，现有的方法是将上下文划分成segment，但这会导致跨segment之间的信息丢失，而为了解决这个问题，通常使用更复杂的架构来进行就交互；LongFormer能够使用多层注意力机制来构建整个上下文的表示，从而减少对特定任务的架构需要。
LongFormer中的注意力机制结合了局部窗口自注意力和任务特定的全局注意力，局部自注意力用于构建上下文表征，全局自注意力构建用于预测的全序列表征。
我们还引入了LongFormer的变体，拥有encoder-decoder架构，用于序列到序列的学习，称为Longformer-Encoder-Decoder （LED），在encoder中使用LongFormer的有效的注意力模式。
Related Work Long-Document Transformers ltr： left-to-right，即以从左到右移动的块的形式处理文档。
sparse：定义某种形式的稀疏注意力模块，来避免二次注意力的矩阵乘法，我们就属于这种方式。
Task-specific Models for Long Documents  长序列的文本通常需要截断或分块，而LongFormer不需要，采用一种简单的方法来连接可用的上下文。
Longformer  原始的transformer模型中的自注意力机制，具有$O(L^{2})$的时间和内存复杂度。为了解决这个问题，我们通过指定关注的输入位置对来稀疏原始的自注意力矩阵。与原始的自注意力不同，该注意力模式与输入序列呈线性相关。
Attention Pattern Sliding Window  如上图（b）所示，考虑到局部信息的重要性，在该注意力模式中，在每个token周围使用一个固定大小的窗口来进行关注。使用多个局部窗口注意力的堆叠层，可以形成一个很大的感受野，因此在顶层可以访问所有的输入位置，并构建整合了所有输入信息的表征，类似于CNN。对于给定的固定窗口大小$w$，每个token分别关注其左右两边$\frac{1}{2}w$个token。该部分的复杂度为$O(L×w)$。在具有$l$层的transformer中，其顶层的感受野大小为$l×w$。
Dilated Sliding Window  如上图（c）所示，为了在不增加计算量的情况下，进一步增加感受野，可以在滑动窗口中引入&amp;quot;扩张&amp;quot;，类似于空洞卷积的思想，即窗口拥有扩张率为$d$的gap，那么感受野就是$l×d×w$。
在多头注意力机制中，设置每个注意力头拥有不同的扩张率，即一些注意力头更专注局部信息，一些更关注更长的信息，从而提高性能。
Global Attention  如上图（d）所示，针对不同的任务，其最优的输入表征是因任务而异的。对于掩码语言建模，模型使用局部上下文来预测mask部分；针对分类任务，模型将整个序列的表示聚合为一个class token中；对于QA任务，问题与文本是连接在一起的，允许模型通过自注意力机制来将问题与文档进行比较。
局部窗口和扩张注意力机制，无法学习特定任务的表征。因此，我们在少数的预先选择的输入位置上添加“全局关注”（global attention），即一个具有全局关注的token能够关注到序列中的所有token，序列中的所有token也可以关注该位置。
如：在分类任务中，对class token使用全局注意；在QA任务中，对问题使用全局注意。由于这类token数量比序列长度要少，因此，局部与全局的注意力结合后的复杂性依旧是$O(L)$。
Linear Projections for Global Attention  原始的注意力计算公式为： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$ 我们使用两组投影：$Q_{s}$、$K_{s}$和$V_{s}$计算局部窗口自注意力中的注意力分数，$Q_{g}$、$K_{g}$和$V_{g}$计算全局注意力中的注意力分数。
Implementation  实现了一种功能齐全、高度优化的自定义CUDA内核。</description>
    </item>
    
    <item>
      <title>PVT</title>
      <link>https://gdy0924.github.io/posts/pvt/</link>
      <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/pvt/</guid>
      <description>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
Abstract  本文提出了一个简单的、无卷积的网络结构，对许多密集预测任务都有用，与最近提出的图像分类的ViT网络不同，我们引入金字塔视觉transformer（Pyramid Vision Transformer，PVT），解决了将transformer用于密集预测任务的差异。
PVT有以下几个优点：
（1）与ViT生成低分辨率的输出，并且有较高的计算和内存成本，PVT不仅可以产生高分辨率的输出，而且通过渐进收缩金字塔模块（progressive shrinking pyramid）来减少计算量；
（2）PVT继承了CNN和transformer的优点，使其成为各种视觉任务的不需要卷积的backbone，可以直接代替CNN backbones；
（3）通过大量实验证明了PVT的有效性。
Introduction 上图为不同架构的比较：（a）CNN backbones使用金字塔架构来进行密集预测任务；（b）ViT；（c）引入CNN中的金字塔架构，提出PVT，可以作为许多任务的backbone。
最近提出的ViT，使用无卷积模型来代替CNN backbones，如上图（b）所示：ViT为柱状结构，虽然适用于图像分类，但不适用于像素级密集预测任务，如目标检测、分割等。原因有两点：（1）ViT的输出特征图是单尺度和低分辨率的；（2）其计算量和内存使用量很高。
为了解决上述问题，本文提出一个基于Transformer的架构，PVT，如上图的（c）所示，克服了传统transformer的困难：（1）以细粒度的图像patch作为输入（每个patch4×4像素），来学习高分辨率的表示；（2）随着网络的加深，通过渐进收缩的金字塔模块来减少transformer的序列长度，减少计算量；（3）采用空间注意力减少层（SRA）来进一步减少计算量和内存使用量。
优势：
（1）相比于CNN网络的局部感受野随着网络的加深而增加，PVT拥有全局的感受野；
（2）与ViT相比，金字塔架构更适合密集预测任务；
（3）可以与其特定任务的transformer decoder相结合，例如：PVT+DETR。
Contributions：
（1）提出PVT，用于密集预测任务的完全基于transformer的backbone；
（2）通过渐进缩小的金字塔模块和空间注意力减少层，来减少资源消耗，使PVT学习多尺度和高分辨率的特征；
（3）通过实验证明PVT结构的有效性。
Related Work CNN Backbones  与发展成熟的CNN不同，视觉transformer的发展仍在早期阶段，在本文中，我们通过提出一种适用于大多数视觉任务的transformer backbone来扩展视觉transformer的应用范围。
Dense Prediction Tasks  密集预测任务的目标是对特征图进行像素级分类或回归，目标检测和语义分割是两个具有代表性的密集预测任务，密集预测任务十分依赖于高分辨率和多尺度的特征图。DETR将CNN backbone和transformer decoder相结合，搭建了一个端到端的目标检测器。
Self-Attention and Transformer in Vision  与之前的模型不同，该工作将特征金字塔架构引入transformer，为密集预测任务提供一个transformer backbone。
Pyramid Vision Transformer (PVT) Overall Architecture  目标是将金字塔结构引入transformer框架，使其为密集预测任务生成多尺度特征图。如上图所示：
该架构共包含四个阶段，生成不同尺度的特征图；每个阶段都是相似的架构，一个patch embedding层和$L_{i}$个Transformer encoder层。</description>
    </item>
    
    <item>
      <title>Transformer-XL</title>
      <link>https://gdy0924.github.io/posts/transformer-xl/</link>
      <pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/transformer-xl/</guid>
      <description>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
Abstract  Transformer具有学习长期依赖的能力，但是在语言建模中，会受到固定长度上下文的限制。我们提出了transformer-xl，由一种segment级递归机制和一种新的位置编码方案组成。该方法不仅能够捕获较长期的依赖关系，而且还可以解决上下文碎片化问题。因此，transformer-xl学习的依赖比RNN和transformer都要长，在短序列和长序列上都有更好的性能。
Introduction  语言建模需要比较长期的依赖，LSTM模型平均使用200个上下文词，注意力机制使得学习长期依赖称为可能。有人提出使用辅助损失来训练更深的transformer网络，但是该模型只能捕获定义好的上下文长度，而不能捕获预定义外的任何长期关系。同时，固定的长度片段，没有考虑句子或其他的语义边界，因此，该模型不能够很好地预测前几个符号所需的必要的上下文信息，导致优化效率不足，性能较差。
上述问题称为“上下文碎片化”（context fragmentation）。为了解决上述固定长度的上下文的局限性，我们提出transformer-xl（extra long）模型，将递归的概念引入自注意力机制中。没有为每个segment的开头重新计算隐藏状态，而是使用在前面segment中获得的隐藏状态，重用之前的状态作为当前segment的记忆，从而建立一个循环连接。上述方法使得建模长期的依赖关系成为可能，解决了context fragmentation问题，同时证明了相对位置编码的必要性，并提出了一个简单且更有效地相对位置编码。
Contributions：
（1）在注意力机制中引入递归；
（2）提出一种新的位置编码方案。
Related Work  为了在语言建模中捕获长期的上下文，一些工作直接将更广泛的上下文的表示作为额外的输入，输入到网络中。另外，在通用序列建模中，如何捕获长期依赖性一直是在研究的问题，LSTM可以可获更长的依赖性，并且泛化性能好，许多研究来解决其梯度消失问题和更好的初始化等。
与它们不同的是，我们的工作是基于transformer架构，并表明语言建模需要学习长期依赖的能力。
Model Vanilla Transformer Language Models 为了将transformer或自注意力机制应用于语言建模，其核心问题是如何训练transformer有效地将任意长度的上下文编码为固定大小的表示。在给定无限的内存和计算量的情况下，一个简单的解决方案是使用一个无条件的transformer解码器来处理整个上下文序列，然而，由于资源有限，在现实实验中是不可行的。
一种可行但粗糙的近似方法是将整个语料库分割成更短的片段segment，并且只在每个片段segment内训练模型，忽略之前segment中的所有上下文信息，这是原始的transformer采用的方法，如上图（a）所示；这种方式信息流不会在segment之间流动，存在两个限制：（1）可能学习到的最大依赖长度是segment的长度，segment长度通常为几百个，因此，虽然transformer受梯度问题影响较小，但并不能充分利用这一优势；（2）出于效率的考虑，在划分segments的时候，不考虑句子的自然边界，而是根据固定的长度来划分序列，导致分割出来的segments在语义上是不完整的，这样会导致context fragmentation问题。
在预测的时候，会对固定长度的segment做计算，一般取最后一个位置的隐向量作为输出。为了充分利用上下文关系，在每做完一次预测之后，就对整个序列向右移动一个位置，再做一次计算，如上图（b）所示，这个过程确保每个预测利用了最长的上下位，缓解了context fragmentation问题，但这导致计算效率非常低。
Segment-Level Recurrence with State Reuse 为了解决使用固定长度上下文的局限性，我们在transformer中引入递归机制：在对当前segment进行处理的时候，缓存并利用上一个segment中所有layer的隐向量序列，而且上一个segment的所有隐向量序列只参与前向计算，不再进行反向传播，如上图的（a）所示。
假设两个长度为$L$的连续segment分别为$s_{\tau}=[x_{\tau,1},&amp;hellip;,x_{\tau,L}]$和$s_{\tau+1}=[x_{\tau+1,1},&amp;hellip;,x_{\tau+1,L}]$，由第$\tau$个segment$s_{\tau}$所产生的第$n$层隐藏状态序列为$h_{\tau}^{n}\in \mathbb{R}^{L\times d}$，其中$d$表示维数，那么第$\tau+1$个segment$s_{\tau+1}$所产生的第n层隐藏状态序列为： $$ \tilde{h}_ {\tau +1}^{n-1}=[SG(h_{\tau}^{n-1})\circ h_{\tau+1}^{n-1}] $$ $$ q_{\tau+1}^{n},k_{\tau+1}^{n},v_{\tau+1}^{n}=h_{\tau+1}^{n-1}W_{q}^{T}, \tilde{h}_ {\tau+1}^{n-1}W_{k}^{T},\tilde{h}_ {\tau+1}^{n-1}W_{v}^{T} $$ $$ h_{\tau +1}^{n}=Transformer-Layer(q_{\tau +1}^{n},k_{\tau +1}^{n},v_{\tau +1}^{n}) $$ 其中，$SG(\cdot )$表示stop-gradient，即不对$s_{\tau}$的隐向量做反向传播；$[h_{u}\circ h_{v}]$表示两个隐藏序列沿着长度的维度做拼接；$W$表示可学习的模型参数；在上述公式中，$\tilde{h}_ {\tau +1}^{n-1}$的维度为$2L×d$，$q$的序列长度为$L$，而$k$和$v$的长度为$2L$，最终经过transformer之后得到的序列长度依旧为$L$。
与标准transformer相比，关键的区别在于$k$和$v$用到了扩展的上下文$\tilde{h}_ {\tau+1}^{n-1}$，即从上一个segment中存储的$\tilde{h}_ {\tau}^{n-1}$。如上图的（a）中的绿色线所示。</description>
    </item>
    
    <item>
      <title>MAE</title>
      <link>https://gdy0924.github.io/posts/mae/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/mae/</guid>
      <description>Masked Autoencoders Are Scalable Vision Learners
Abstract  本文证明了掩码自动编码器(Masked Autoencoders, MAE)是一种可扩展的自监督学习模型，其方法很简单：随机mask输入图像的patch，并重建缺失的像素。首先，开发了一个非对称的encoder-decoder架构，encoder只对可见的patch进行处理（即没有mask的patch），一个轻量级的decoder，从潜在的表示和mask token种重构原始图像。其次，我们发现mask更高比例的输入图像（如75%），能够产生更有意义的自监督任务。
Introduction  基于GPT中的自回归语言建模和BERT中的mask自编码的解决方案在概念上很简单：它们mask部分数据，并学习预测被mask的内容。mask自编码的思想，是一种更一般的去噪自编码器的形式，也适用于计算机视觉。这会有个问题：是什么使mask的自编码在视觉和语言领域之间有所不同？
我们试图从以下角度来回答这个问题：
（1）到目前为止，两者的架构都是不同的：在视觉中，卷积在很长时间都占了主导地位。卷积通常在规则的网格上运行，将mask token和位置embedding等添加到卷积网络中并不简单，但是随着ViT的提出，该问题已经得到了解决；
（2）语言于视觉之间的信息密度是不同的：语言具有高度语义性和信息密集性，当训练一个模型预测句子中缺失的一些单词时，该任务可以诱导复杂的语言理解；相反，图像具有大量的空间冗余信息，如一个缺失的patch可以从相邻的patch中恢复，而很少有高度理解的部分；为了克服上述这种差异，我们提出mask非常多的patch，这种方式在很大程度上减少了冗余，创造了一个具有挑战性的自监督任务；
（3）自编码器中的decoder，将潜在的表示映射回输入端，在重建文本和图像之间起的作用不同：在视觉中，decoder重建像素，其输出的语义级别比较低；但是在语言中，预测包含丰富语义信息的单词，因此，对于图像，decoder的设计在确定学习到的语义水平中起着关键作用。
基于上述分析，我们提出了一种简单的、有效的、可扩展的mask自编码器（Masked Autoencoders,MAE），用于学习视觉表示。
MAE从输入图像中随机mask patch，并重建像素空间中mask掉的patch。非对称的encoder-decoder架构，其中，encoder只对未mask的patch进行操作，轻量级的decoder从潜在表示和mask token中重建输入，如下图所示： Related Work Masked language modeling  掩码语言模型和自回归模型是自然语言处理领域非常成功的方法，如：Bert、GPT。该方法mask输入中的某些内容，并训练模型来预测mask掉的内容，这种训练可以很好的推广到各种下游任务中。
Autoencoding  自编码中包含一个encoder和一个decoder，encoder将输入映射到潜在表示，decoder重构输入。MAE是去噪自编码的一种形式，但在许多方面与经典的DAE不同。
Masked image encoding  掩码图像编码从被掩码的图像中学习表示。
Self-supervised learning Approach  MAE是一种自编码模型，可以重建原始输入的部分内容。包含一个encoder，将输入映射到一个潜在的表示，一个decoder，从潜在的表示中重构原始输入；与原始的自编码器不同，我们设计的是非对称的结果，即encoder只对未mask的输入进行操作，轻量级的decoder从潜在表示和mask token中重建完整的输入。
Masking  与ViT相同，我们将输入图像分割成规则的patch，接着对patch进行随机采样，对其进行mask。该随机采样的比例很高，即被mask掉的patch比例很高，在很大程度上消除了冗余，从而创建了一个不容易通过从可见的相邻patch进行推断来解决的任务。
MAE encoder  encoder是一个ViT架构，但只应用于未mask的patch。与ViT相同，将patch embedding和位置embedding相加作为输入，然后通过一系列的transformer block来处理输入，但是，该encoder只处理整个输入图像的一小个子集。mask patch被移除了，这使得只是用一小部分的计算量和内存来训练一个大的encoder。完整的输入图像由轻量级的decoder来处理。
MAE decoder  MAE中decoder的输入是完整的token集合，包括encoder处理的token和mask token。每个mask token是一个共享的、可学习的向量，用来表示该patch是否需要预测。同时向完整的token集合中加入位置embedding，输入到另外的transformer block中。
MAE的decoder仅在预训练阶段用于实现像素重建，因此是独立于encoder的；并且decoder比encoder更窄更浅，计算量更小。通过这种不对称的设计，完整的token集合只由轻量级的decoder来处理，大大减少了预训练的时间。
Reconstruction target  MAE通过预测每个掩码patch的像素值类重建输入图像，encoder中输出的每个元素代表一个patch的像素向量，decoder的最后一层是一个线性层，其输出通道的数量等于一个patch中像素的数量。decoder的输出经过reshape，重建成原始图像。损失函数使用MSE，并且只计算mask patch上的损失。
我们还提出了一个变体，其重建目标是每个mask patch的归一化像素值，在实验中，使用归一化像素值作为重建目标，提高了数据表示的质量。</description>
    </item>
    
    <item>
      <title>Reformer</title>
      <link>https://gdy0924.github.io/posts/reformer/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/reformer/</guid>
      <description>REFORMER: THE EFFICIENT TRANSFORMER
原文链接：DeiT
上表为内存使用量和时间复杂度的比较，其中$l$为序列的长度，$b$为batch size的大小，$n_{h}$为头数，$n_{c}$为LSH的chunk数量，$n_{r}$为hash重复的次数。
Abstract  Transformer在许多任务上都实现了最先进的结果，但是其训练代价是很昂贵的，特别是在长序列上。因此，我们提出两种提高transformer的技术：（1）局部敏感哈希（locality-sensitive hashing），将复杂度从$O(L^2)$降低到$O(L log L)$；（2）使用可逆残差层而不是标准残差，使得在训练过程中只存储最后一层的激活值（激活函数的输入值），而不是N次，N为层数。
Introduction  基于transformer的大型模型甚至不能在单个GPU上进行微调，因为它们的内存需求需要多加速器硬件设置。transformer架构中的主要内存使用在以下几个方面：
（1）一个包含N层的模型，其内存比单层模型使用的内存大N倍，因为需要存储激活值以进行反向传播，即每一层的激活值都需要存储下来；
（2）transformer block中的全连接层FFN中的深度往往比$d_{model}$大得多，如：FFN的输入维度为512，那么中间的维度可能是2048，这也占了内存的很大部分；
（3）注意力机制在计算序列长度为$L$的序列时，在计算和内存的复杂度上都是$O(L^2)$。
我们提出以下方法来解决上述问题：
（1）可逆层：使得整个网络只用存储最后一层的激活值，而不是所有层，之前层的激活值可以通过最后一层推算出来；
（2）在FFN层中分割激活值，将他们以快的形式进行处理；
（3）基于局部敏感哈希的近似注意力，将复杂度从$O(L^2)$降低到$O(L log L)$。
LOCALITY-SENSITIVE HASHING ATTENTION Dot-product attention  transformer中使用点乘计算注意力，输入是维度为$d_{k}$的query和key，以及维度为$d_{v}$的value，计算公式如下： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$
Multi-head attention  使用可学习的矩阵$W_{Q}$、$W_{K}$和$W_{V}$来处理query、key和value。
Memory-efficient attention  假设Q、K、V的大小都是$L×d_{model}$，那么$QK^{T}$的计算结果的大小就是$L×L$的，当序列长度很长时，是需要一定的内存去存储的，这就限制了transformer的使用。
Where do Q, K, V come from?  Q、K、V是将输入经过三个不同的线性层来得到的。对于LSH注意力模型，我们希望query和keys（Q和K）是相同的，这可以通过一个使用同一个线性层来实现，这种方式称为shared-QK Transformer，并经过实验验证，这种方式并不会影响性能。
Hashing attention  对于LSH注意力模型，从Q=K和V开始，大小为$L×d_{model}$，并根据之前的公式，主要问题在于$QK^{T}$部分的计算。但是，我们最终是只关心softmax的结果，并且softmax是由最大元素来主导的。也就是说，对于每个query$q_{i}$，我们只需要关注在K中最接近$q_{i}$的那些key。比如：假设长度为64K，对于每个$q_{i}$，我们只考虑一个小子集，32或64个最接近的key的集合，这样就可以减少计算量，那么如何在K中找到“最近的邻居”呢？
Locality sensitive hashing  在高维空间中快速寻找最近邻的问题可以通过局部敏感哈希(LSH)来解决：使用一种哈希函数$H(x)$，使得两个个相似度很高的数据以较高的概率映射成同一个hash值，而令两个相似度很低的数据以极低的概率映射成同一个hash值，该函数称为局部敏感哈希。
我们通过随即投影来实现，如下图。为了得到，我们先创建一个随机矩阵，大小为$[d_{k},b/2]$，然后定义$h(x)=argmax([xR;-xR])$，其中$[u;v]$表示两个向量的连接。 在上图中，为角度局部敏感哈希，使用球投影点的随即旋转角度，在上图的二维描述中，上边一行的两个点x和y不太可能在一个中hash buckets，而在下边一行中的x和y的球投影是彼此接近的。
LSH attention 上图为LSH attention中：hash-bucketing、排序、chunking分块和计算attention结果的简化描述；下图中的(a)-(d)是步骤中的矩阵形式：</description>
    </item>
    
    <item>
      <title>DeiT</title>
      <link>https://gdy0924.github.io/posts/deit/</link>
      <pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/deit/</guid>
      <description>Training data-efficient image transformers &amp;amp; distillation through attention
原文链接：DeiT
Abstract  最近，基于注意力机制的神经网络用于计算机视觉领域，但是这些网络需要大量数据进行预训练，从而限制了其使用。
在该论文中，我们参考ViT网络，引入一种蒸馏方式，使得student网络可以通过注意力机制从teacher网络中进行学习。
Introduction  Vision Transformer（ViT）使用自注意力机制用于图像分类，以图像补丁作为输入，但是transformer架构在数据量不足的训练下不能很好地泛化，而这些模型的训练涉及大量的计算资源。
在本文中，我们提出DeiT网络，引入了一个一种基于token的策略，contributions：
（1）我们的方法证明使用不包含卷积层的神经网络可以在没有外部数据的情况下，可以与在ImageNet数据集上最好结果的模型相比较，并且我们提出的模型DeiT-S和DeiT-Ti有很少的参数；
（2）引入了一种新的基于蒸馏标记的蒸馏方式：复制由teacher网络得到的label，与类标记作用相同，这两种label相互作用以提高精度。
Related work Image Classification  虽然有许多尝试使用transformer来实现图像分类，但其性能不如卷积网络。最近，Vision Transformer（ViT）没有使用任何卷积，达到了卷积的水平，然而，ViT需要大量的数据进行预训练，才能使transformer架构有效。在本文中，不需要大型训练数据集，也实现了强大的性能，即只使用Imagenet1k数据集。
Transformer architecture Knowledge Distillation  知识蒸馏，由Hinton提出，是指student网络利用来自强大的teacher网络中得到的“soft”标签进行训练。soft标签（如：0.2，0.3，0.5）是softmax函数的输出，而不是hard标签（如：0，0，1），这种训练方式提高了student网络的性能（或者它可以看作是将teacher网络压缩成一个更小的模型——student网络）。一方面，teacher的soft标签将有类似于标签平滑的效果；另一方面，teacher的监督考虑了数据增强的影响：例如，一张很大的场景下有一只小猫缩在角落，其标签为猫，如果对其进行crop操作，这只猫就不会在crop后的图中了，这也改变了图片的标签。利用teacher网络中的soft标签可以用一种soft的方式将偏差转移到student网络中，例如：使用卷积网络作为teacher，在transformer中引入由卷积得到的偏差，是可以提升网络性能的。
Vision transformer: overview Multi-head Self Attention layers (MSA) $$ Attention(Q,K,V)=Softmax(\frac{QK^{T}}{\sqrt{d}})V $$ 其中，$Q=XW_{Q}$，$K=XW_{K}$，$V=XW_{V}$
Transformer block for images  Transformer中，MSA后跟着前馈网络（FFN），由两个线性层组成，中间有GeLU非线性激活函数，第一个线性层将维数从$D$扩展到$4D$，第二层将维数从$4D$减少为$D$。
ViT将图片当作序列一样处理，将图像分成大小为$16×16$的patch，作为transformer的输入，并结合位置信息（position embedding）。
The class token  可训练的向量，在第一层附加在patch tokens中，该向量通过transformer，最后用一个线性层来预测类别。这种架构迫使自注意力机制在patch tokens和class token之间传播信息：在训练时，监督信号只来自class token，patch tokens是模型唯一的变量输入。
Fixing the positional encoding across resolutions  使用较低分辨的图像对网络进行预训练，使用较高分辨率的图像对网络进行微调，可以加快训练。当增加输入图像的分辨率时，我们保持patch大小不变，那么输入patch的数量就会发生变化。采用插值法对位置embedding进行调整。</description>
    </item>
    
    <item>
      <title>SegFormer</title>
      <link>https://gdy0924.github.io/posts/segformer/</link>
      <pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/segformer/</guid>
      <description>原文链接：SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers
Abstract  我们提出了一个SegFormer，将transformer与多层感知器(MLP)解码器统一起来。SegFormer有两个特点：（1）其包括一个层次架构transformer编码器，输出多尺度特征，不需要位置编码，从而避免当测试分辨率与训练不同时，位置编码插值会导致性能下降；（2）避免了复杂的解码器，使用的MLP解码器融合来自不同层的信息，从而结合了局部和全局的注意力机制。将该方法进行扩展，获得了从SegFormer-B0到SegFormer-B5的一系列模型，达到了比以前的模型更好的性能和效率。
Introduction  ViT将transformer用于图像分类，将一个图像分割成多个线性embedding的patch，并将其输入到一个带有位置embedding的transformer中，在ImageNet上取得了令人印象深刻的性能。在语义分割中，SETR的提出证明了transformer的可行性。
SETR采用ViT作为backbone，并合并了几个CNN解码器来扩大特征分辨率。尽管ViT性能良好，但也有一些局限性：
（1）ViT输出单尺度低分辨率特征，而不是多尺度特征；
（2）它在大图像上的计算成本较高。
为了解决这些限制，提出了一种金字塔视觉transformer(pyramid vision Transformer，PVT)，这是一种具有金字塔结构的ViT的扩展，用于密集预测。PVT在目标检测和语义分割方面比ResNet有相当大的改进。
本文介绍了SegFormer，一个前沿的语义分割transformer框架，同时考虑了效率、准确性和鲁棒性。与以前的方法相比，我们的框架重新设计了编码器和解码器。
Contribution：
（1）一种新的无位置编码的分层transformer编码器；
（2）一种轻量级的仅用MLP作为解码器的设计，没有复杂的计算；
（3）在三个公开的语义分割数据集上的效率、准确性和鲁棒性方面都达到了最优。
首先，该编码器在对分辨率与训练图像不同的图像进行预测时，避免了位置编码插值，因此，该编码器可以很容易地适应任意分辨率的测试图像，而不影响性能；其次，分层结构使编码器能够同时生成高分辨率的细粒度特征和低分辨率的粗粒度特征，这与ViT只能生成固定分辨率的特征图形成了对比；最后，我们提出了一个轻量级的MLP解码器，其关键思想是利用transformer的内在特点，即较低层的注意力倾向于局部信息，而最高层的注意力是全局的的。通过融合来自不同层的信息，MLP解码器同时结合了局部和全局的信息。
Related Work Semantic Segmentation  语义分割可以看作是图像分类从图像级到像素级的一种扩展，最早的语义分割模型为FCN，一个全卷积的网络，以端到端的方式进行像素到像素的分类。之后，一些修改：扩大感受野；细化上下文信息；引入边界信息；引入注意力模块等。最近的方法（SETR）已经证明了基于transformer的架构对语义分割任务的有效性，但是，这些方法仍然需要大量计算。
Transformer backbones  ViT是第一个完全利用transformer在图像分类中达到最优性能的工作；随后，DeiT进一步探索了一种数据高效的训练策略和一种蒸馏方法；PVT是第一个在transformer中引入金字塔结构的工作；Swin、CvT、CoaT、LeViT和Twins等方法增强了特征的局部连续性，去掉了固定大小的位置embedding，提高了transformer在密集预测任务中的性能。
Transformers for specific tasks  DETR是第一个使用transformer构建一个无NMS的端到端的目标检测框架。在语义分割方面，SETR采用ViT作为backbone来提取特征，取得了令人印象深刻的性能。然而，这些基于transformer的方法效率非常低，因此很难在实时应用程序中部署。
Method 如上图所示，主要包括两部分：
（1）分层transformer编码器：生成高分辨率粗粒度特征和低分辨率细粒度特征；
（2）轻量级的MLP解码器：融合多层特性，生成最终的语义分割结果。
首先给定一个大小为$H×W×3$的图像，我们首先将其分成大小为$4×4$的patch，与ViT使用大小为$16×16$的patch不同，使用较小的patch更有利于密集预测任务；然后，将这些patch作为分层transformer编码器的输入，以获得原始图像分辨率的{1/4、1/8、1/16、1/32}的多层次特征；将这些多层次特征传递给MLP解码器，以预测$\frac{H}{4}\times \frac{W}{4}\times N_{cls}$分辨率下的分割结果，其中$N_{cls}$是类别的数量。
Hierarchical Transformer Encoder  我们设计了一系列的Mix Transformer编码器(MiT)，MiT-B0到MiT-B5，具有相同的架构，但大小不同：MiT-B0用于快速预测的轻量级模型，而MiT-B5是获得最佳性能的最大模型。MiT的设计受ViT的启发，但为语义分割任务定制与优化。
Hierarchical Feature Representation  与ViT只能生成单分辨率的特征图不同，该模块的目标是，给定一个输入图像，生成类似CNN的多层次特征图。这些特性图同时提供了高分辨率的粗粒度特征和低分辨率的细粒度特征。具体来说，给定一个分辨率为$H×W×3$的输入图像，通过patch 合并（patch merging）获得层次化的特征图$F_{i}$，每个特征图的分辨率为$\frac{H}{2^{i+1}}\times \frac{W}{2^{i+1}}\times C_{i}$，其中$i\in \lbrace 1,2,3,4 \rbrace$，$C_{i+1}$ 比$C_{i}$大。</description>
    </item>
    
    <item>
      <title>SETR</title>
      <link>https://gdy0924.github.io/posts/setr/</link>
      <pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/setr/</guid>
      <description>原文链接：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers
Abstract  最近的语义分割方法采用encoder-decoder结构的全卷积网络：编码器逐步降低分辨率，并捕捉更抽象的特征和更大的感受野。但是由于上下文信息对于分割任务很重要，最近的研究集中在通过空洞卷积或引入注意力模块来增加感受野，不过都是在encoder-decoder结构中进行修改。
在本文中，我们将语义分割作为一个序列到序列的预测任务。具体来说：提出一个纯Transformer，即没有卷积和降低分辨率，将图像编码为一系列patch，在Transformer的每一层都使用全局上下文信息。该encoder可以与一个简单的decoder结合，提供一个强大的分割模型，称为SEgmentation TRansformer（SETR）。
Introduction  标准的全卷积网络具有encoder-decoder架构：encoder用于捕捉特征，decoder对由编码器产生的特征进行像素级分类。其中，encoder由堆叠的卷积层组成，特征图的分辨率逐渐降低，encoder能够学习更多的抽象特征，并且逐渐增大感受野。但是存在一些限制，即不能学习远程的依赖信息，感受野有限对于语义分割任务是不好的。
为了克服上述的限制，一种方法是直接修改卷积操作：大的卷积核大小，空洞卷积和图像/特征金字塔结构；另一种方法是将注意力模块引入FCN架构中：旨在建模特征图中所有像素的全局交互信息。当应用于语义分割任务时，一种常见的设计是将注意力模块与FCN体系结构相结合，并将注意层放置在顶部。针对上述两种方法，标准的encoder-decoder架构是不变的。最近，开始尝试摆脱卷积，关注纯注意力模型，但是，即使没有卷积，也不会改变模型结构的本质：encoder（下采样）-decoder（上采样）架构。
在本文中，我们提出用纯Transformer代替基于堆叠卷积层的编码器，从而产生一种新的分割模型，称为SEgmentation TRansformer（SETR）。该Transformer编码器将输入图像视为图像patch序列，并使用全局自注意力机制对该序列进行转换，用于特征学习。具体地说：首先将一个图像分解成一个固定大小的patch，形成patch序列；然后，对每个patch的像素向量应用线性变换层，得到一系列特征embedding向量作为Transformer的输入；得到Transformer输出的特征，使用decoder来恢复原始图像的分辨率。图像分辨率没有下采样，在Transformer的每一层都进行全局上下文建模。（灵感来源于ViT）
Contributions：
（1）将图像分割问题作为一个序列到序列的预测任务，以替换全卷积网络；
（2）利用Transformer框架，实现纯注意力机制的特征表示encoder；
（3）进一步提出了三种不同复杂性的decoder架构。
Related work Semantic segmentation  最近的许多研究都集中在解决有限的感受野/上下文信息建模问题：DeepLab引入空洞卷积，PSANet提出用于动态捕捉远程上下文的空间注意力模块；DANet提出空间注意力和通道注意力。所有这些方法都基于FCNs，特征编码和提取部分都是基于经典卷积网络，如VGG和ResNet。在该工作中，我们从不同的角度重新思考语义分割任务。
Transformer  AANet将卷积和自注意力机制集中训练，LRNet探索局部自注意力，以避免全局自注意力带来的大量计算，DETR使用Transformer进行目标检测，其中Transformer被附加在检测头中，STTR和LSTR分别采用Transformer进行视差估计和车道形状预测。最近，ViT是第一个纯Transformer图像分类模型，达到了最优的结果，为语义分割任务中使用纯Transformer的编码器设计提供了灵感。
Method FCN-based semantic segmentation  在基于FCN的语义分割网络中，encoder由堆叠的卷积层组成：第一层以图像作为输入，大小为$H×W×3$；后续层的输入是一个大小为$h×w×d$的三维张量，由于卷积操作的局域性，感受野随层数的增加而增加，取决于卷积核核的大小（通常是3×3）。因此，只有具有大感受野的更高层次才能在这个FCN架构中捕捉更多的上下文信息。然而，一旦达到一定深度，增加更多层的优势会迅速减少。因此，对上下文建模的感受野有限是经典FCN结构的限制。
最近，一些研究发现，将FCN与注意机制相结合是学习上下文信息的一种有效的策略。这些方法为例减少计算量，将注意力机制限制在较小尺寸的更高层次的特征图中，导致缺乏对较低层次特征图的学习，为了克服这一限制，我们提出了一种完全基于自注意力机制的编码器，称为SEgmentation TRansformers（SETR）。
Segmentation transformers (SETR) Image to sequence  SETR遵循NLP的输入-输出形式，具体地，Transformer，如上图的（a）所示，以一维的特征embedding序列$Z\in \mathbb{R} ^{L\times C}$作为输入，$L$为序列的长度，$C$为隐含通道的大小。因此，图像序列化需要将输入图像$x\in \mathbb{R} ^{H\times W\times 3}$转换为$Z$。
图像序列化的一种简单方法是将图像像素值flatten为一个大小为$3HW$的一维向量，对于大小为$480×480×3$的典型图像，得到的向量长度将为$691200$。考虑到Transformer的二次复杂度，这种高维向量不可能在空间和时间上同时被处理。因此，将每个像素作为转换器的输入是不可能的。
最终，决定设置Transformer的输入序列长度$L$为$\frac{H}{16}\times \frac{W}{16}=\frac{HW}{256}$。为了获得$\frac{HW}{256}$长的输入序列，我们将一个图像$x\in \mathbb{R} ^{H\times W\times 3}$均匀地划分成为$\frac{H}{16}\times \frac{W}{16}$个patch，每个patch的大小为$16×16$，然后将patch flatten为一个序列。利用线性变换函数$f:p\rightarrow e\in \mathbb{R} ^{C}$，将每个向量化的patch进一步映射到潜在的高维embedding空间，得到图像$x$的一维patch embedding序列。为了对patch的空间信息进行编码，每个位置$i$学习一个特定的embedding $p_{i}$，以形成最终的输入序列：$E=\lbrace e_{1}+p_{1},e_{2}+p_{2},&amp;hellip;,e_{L}+p_{L} \rbrace$。</description>
    </item>
    
    <item>
      <title>Attention U-Net</title>
      <link>https://gdy0924.github.io/posts/attention-u-net/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/attention-u-net/</guid>
      <description>Attention U-Net：Learning Where to Look for the Pancreas
原文链接：Attention U-Net
Abstract  我们提出了一种用于医学图像的attention gate（AG）模型，该模型可以自动学习如何关注不同形状和大小的目标对象。使用AGs训练的模型可以隐式的学习在输入图像中抑制不相关的区域，同时突出显示对特定任务有用的显著特征。AGs可以很容易地集成到CNN架构中，如U-Net，以最小的计算开销，同时提高模型的灵敏度和预测精度。
Introduction  由于人工、密集地标记大量医学图像是一项繁琐且容易出错的任务，自动医学图像分割在图像分析领域得到了广泛的研究。
随着卷积神经网络(CNNs)的出现，全卷积网络(FCNs)和U-Net是两种常用的架构。尽管这些结构具有良好的代表性，但当目标器官在形状和大小上表现出较大的差异时，这些结构依赖于多阶段的级联CNN架构。级联框架提取一个感兴趣的区域(ROI)，并对该特定的ROI做出密集的预测。
然而，这种方法导致了计算资源和模型参数的过度和冗余使用，例如：级联内的所有模型都重复提取类似的低级特征。为了解决这个普遍的问题，我们提出了一个简单而有效的解决方案，即attention gates(AGs)。
带有AGs的CNN模型可以以类似于FCN模型的训练标准方式从头开始训练，并且AGs自动学习在没有额外监督的情况下关注目标对象。此外，它们不引入显著的计算开销，也不需要像在多模型框架的情况下那样需要大量的模型参数。所提出的AGs通过抑制无关区域的特征，提高了模型对密集标签预测的灵敏度和准确性。这样，就可以在保持高预测精度的同时，消除使用额外定位模型的必要性。
在本文中，我们提出了一种新的自注意门控模块，可用于语义分割中的密集标签预测。此外，我们还探讨了AGs对医学图像分析的好处，特别是在图像分割的背景下。
Contributions：
（1）我们提出了基于网格的门控模块，可以使注意力系数更特定于局部区域，与基于全局特征向量的门控模块相比，提高了性能；
（2）首次在医学图像的CNN中使用Soft Attention，该模块可以替代分类任务中的Hard attention和器官定位任务中的定位模块；
（3）将U-Net改进为Attention U-Net，增加了模型对前景像素的敏感度，并设计实验证明了这种改进是通用的。
Related Work Attention Gates  一般AG会被用在自然图像分析、知识图谱、图像描述、机器翻译和分类任务上。Attention机制最开始是通过解释输出类别分数相对于输入图像的梯度来探索注意力图。可训练的Attention机制根据设计可分为Hard attention和Soft attention。Hard attention主要是由迭代区域候选和区域裁剪，通常是不可微的，因此会使模型很难训练，通常依赖强化学习的其他参数；Soft attention是基于概率的，可以正常反向传播梯度。逐通道的attention可以突出某些维度的特征，效果极好(如SENet)；注意力机制可以消除对外部信息（external gating information）的依赖。
Methodology Fully Convolutional Network (FCN)  全卷积网络已经成为医学图像的基准模型，如FCN、U-Net、DeepMedic等，其效果要优于传统的方法，主要是由于：SGD优化，卷积核的参数共享、卷积操作很好地捕捉到了医学图像中的结构信息。卷积层通过逐层捕捉局部信息，逐步提取高维图像表示$x^{l}$，最终，最终在高维空间的离散像素会具有语义信息和巨大的感受野。在第$l$层的输出处，依次通过一个线性变换和一个非线性激活函数，得到了特征图$x^{l}$。
在本文中，我们在一个标准的U-Net架构之上加入注意力模型。U-Net网络的优势主要是与在多个图像尺度上提取图像特征有关。粗粒度的特征图能够捕获上下文信息，并突出显示前景对象的类别和位置。在多个尺度上提取的特征图随后将通过skip连接进行合并，以结合粗粒度级别和细粒度级别的密集预测，如下图所示： Attention Gates for Image Analysis  为了捕获足够大的感受野，从而获取上下文信息，在标准的卷积网络中，特征图的尺寸经过下采样逐渐变小。这样，在粗粒度特征图中的特征才能以全局的尺寸建模位置和关系。但是对于小物体，形变程度大，很难降低false-positive预测。为了提高精度，当前的分割框架依赖于额外的目标定位模型，将任务简化为单独的定位和分割两步骤。其实，通过在一个标准的CNN模型中集成注意门(AGs)来也可以实现。这不需要训练多个模型和大量额外的模型参数，AG会抑制无关背景区域中的特征响应，而无需在网络之间裁剪ROI。
注意力系数（attention coefficients），$\alpha_{i}\in [0,1]$，是为了突出显著的图像区域和抑制任务无关的特征响应，以只保留与特定任务相关的激活，如下图所示。 AGs的输出是输入特征图和注意力系数的元素级相乘（对应元素逐个相乘）：$\hat{x}_ {i,c}^{l}=x_ {i,c}^{l}\cdot \alpha_{i}^{l}$。通常，为每个像素向量$x_{i}^{l}\in \mathbb{R}^{F_{l}}$计算一个注意力标量，其中，$F_{l}$对应于第$l$层中特征图的通道数。在多语义标签类中，我们提出使用多维注意力系数。因此，每个AG都学习专注于目标对象的一个子集，如下图所示： 每个像素$i$使用门控向量$g_{i}\in \mathbb{R}^{F_{g}}$来决定聚焦区域。门控向量包含上下文信息，以修剪低级特征响应。在本文中，选择使用加法注意力机制，相比于乘法注意力，其效果更好，公式如下： $$ q_{att}^{l}=\psi^{T}(\sigma_{1}(W_{x}^{T}x_{i}^{l}+W_{g}^{T}g_{i}+b_{g}))+b_{\psi} $$ $$ \alpha_{i}^{l}=\sigma_{2}(q_{att}^{l}(x_{i}^{l},g_{i};\Theta_{att})) $$ 其中，$\sigma_{2}$是sigmoid激活函数，AG由一组参数$\Theta_{att}$表示，包括：线性变换$W_{x} \in \mathbb{R}^{F_{l}\times F_{int}}$，$W_{g} \in \mathbb{R}^{F_{g}\times F_{int}}$，$\psi \in \mathbb{R}^{F_{int}\times 1}$和偏置项$b_{\psi} \in \mathbb{R}$，$b_{g} \in \mathbb{R}^{F_{int}}$。线性变换是通过1x1卷积来实现的。</description>
    </item>
    
    <item>
      <title>DeepLab系列</title>
      <link>https://gdy0924.github.io/posts/deeplab%E7%B3%BB%E5%88%97/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/deeplab%E7%B3%BB%E5%88%97/</guid>
      <description>V1 SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS 原文链接：DeepLab V1
Abstract  深度卷积神经网络(DCNNs)最近在视觉任务中显示出了最先进的性能，如图像分类和目标检测。该工作结合了DCNNs和概率图形模型的方法，用于解决像素级分类的任务，即语义分割。我们通过将DCNN的最后一层与一个全连接的条件随机场(CRF)相结合，克服了深度网络的定位能力弱的特性。
Introduction  将DCNN应用在语义分割任务中有两个挑战：降采样，空间“不敏感”（不变性）。降采样是指在网络每层重复进行最大池和降采样（步长）导致图像分辨率降低；而通过“空洞”卷积算法，能够有效地密集计算DCNN响应。
第二个问题是指从分类器获得以对象为中心的预测需要对空间转换的不变性，这限制了DCNN模型的空间准确性。我们通过使用全连接条件随机场(CRF)，提高了模型捕捉细节的能力。在我们的工作中，我们证明了当与基于DCNN的像素级分类器相结合时，它会有最先进的结果。
DeepLab三个主要优势是：
（1）速度：利用“空洞卷积”算法；
（2）精度：我们获得了最先进的结果；
（3）简单：我们的系统是由两个相当完善的模块组成，DCNNs和CRFs。
Convolutional Neural Networks For Dense Image Labeling Efficient Dense Sliding Window Feature Extraction With Hole Algorithm 利用空洞卷积算法有效地提取密集滑动窗口特征
密集的空间结果有助于特征提取器的成功，作为实现这一目标的第一步，我们将VGG-16的全连接层转换为卷积层，并在原始分辨率的图像上以卷积方式运行网络。然而，这是不够的，因为它产生非常稀疏计算的检测结果（步幅为32像素，即下采样32倍）。为了使下采样变为8倍，修改为：删除最后两个max pooling层（或者stride修改为1），由于max pooling的修改，影响了其后的卷积层，使其视野分别下降了 2x 和 4x，为了保持其原来的视野，便将其改成空洞卷积，dilation 分别为 2 和 4，这种方法普遍适用，并允许我们在任何下采样中有效地计算密集的CNN特征映射，而不引入任何其他的参数。
训练时将预训练的VGG16的权重做fine-tune，损失函数取是输出的特征图与ground truth下采样8倍做交叉熵和；测试时取输出图双线性上采样8倍得到结果。我们使用简单的双线性插值来将最终的特征图的分辨率提高8倍，而计算成本可以忽略不计。FCN网络中没有使用空洞卷积算法，并且在CNN输出处产生非常粗糙的特征图（下采样率为32倍），这迫使他们使用学习的上采样层（转置卷积），显著增加了他们系统的复杂性和训练时间。
CONTROLLING THE RECEPTIVE FIELD SIZE AND ACCELERATING DENSE COMPUTATION WITH CONVOLUTIONAL NETS 利用空洞卷积控制感受野的大小，加速密集计算
最近的基于DCNN的图像识别方法依赖于在图像集大规模分类任务上预先训练的网络。这些网络通常具有较大的感受野大小：对于我们考虑的VGG-16网络，它的感受野是224×224（零填充），如果网络被卷积应用，则为404×404像素。在将网络转换为完全卷积层后，第一个全连接层大小为7×7×4096，成为我们密集预测结果的计算瓶颈。
我们通过对第一个FC层（通过简单的抽取）进行空间下采样到4×4（或3×3）的空间大小来解决这个实际问题。这将网络的感受野减少到128×128（零填充）或308×308（卷积模式），并将第一FC层的计算时间减少了2−3倍。具体来说：给定一个306×306输入图像，它在测试期间以大约8帧/秒的速度在网络顶部产生39×39密集的原始特征分数。训练期间的速度是3帧/秒。我们还成功地尝试了将全连接层的通道数从4096个减少到1024个，在不牺牲性能的情况下，大大减少了计算时间和内存占用。
Detailed Boundary Recovery：Fully-Connected Conditional Random Fields and Multi-Scale Prediction 详细的边界恢复：全连接条件随机场和多尺度预测</description>
    </item>
    
    <item>
      <title>SegNet</title>
      <link>https://gdy0924.github.io/posts/segnet/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/segnet/</guid>
      <description>原文链接：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
Abstract  提出了一种新的深度全卷积神经网络结构，用于像素级语义分割，称为SegNet：由一个编码器和一个解码器，以及一个分类层组成。其中，编码器的结构与VGG-16中的13个卷积层完全相同；解码器用于将低分辨率特征图映射到与输入分辨率相同的特征图，以进行像素级分类。
SegNet创新之处是：利用解码器对其较低分辨率的特征图进行上采样。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引来进行非线性上采样。得到的上采样的映射是稀疏的，然后与可训练的卷积核进行卷积，以生成密集的特征映射。
Introduction  语义分割有广泛的应用，从场景理解、推断对象之间的支持关系到自动驾驶。最近的一些方法尝试直接采用用于类别预测的深度神经网络结构来进行像素级标记，但结果不够精确。这主要是因为最大池化和下采样降低了特征图的分辨率。因此，SegNet的动机是将低分辨率特征图映射到与输入分辨率相同以进行像素级分类。这种映射必须产生对精确的边界定位有用的特征。
SegNet中的编码器与VGG16中的卷积层相同，删除了VGG16末端的全连接层。SegNet的关键组成部分是解码器网络，由逐层的编码器组成，每个编码器都对应着一个解码器。其中，解码器使用从对应的编码器传递过来的最大池化索引，对其输入的特征图进行非线性上采样。
在解码过程中重用最大池化索引有几个优点：（1）改进边界划分；（2）减少参数数量；（3）这种形式的上采样可以兼容任何的编码器-解码器结构，只需要很少的修改。
Relative Work  深度卷积神经网络在图像分类方面的成功，许多研究者尝试将用于分类的网络应用于分割。更新的深度架构专门为分割而设计，通过卷积编码和映射低分辨率的图像表示到像素级预测，提高了最先进的技术。
全卷积网络（FCN）中的每个解码器对其输入特征图进行上采样，并将它们与相应的编码器特征图结合起来，以产生到下一个解码器的输入。通过对FCN添加一个递归神经网络（RNN），并在大数据集上对其进行微调，进一步提高了FCN的预测性能：RNN层模拟了CRFs清晰的边界划分能力，同时利用了FCN的特征表示能力。
多尺度融合的深度网络架构也在不断被提出，有两种类型：（1）使用几个不同尺度的输入图像和相应的深度特征提取网络；（2）组合来自单个深度架构的不同层的特征图。常见的想法是：使用在多个尺度上提取的特征来提供局部和全局上下文信息，并且使用早期编码层的特征图以保留更多的细节信息，从而生成更清晰的预测边界。
我们的工作受到无监督特征学习架构的启发，其关键的模块是一个编码器-解码器网络。编码器通过卷积、非线性激活、最大池化和下采样来获得特征图，同时在池化过程中，最大位置的索引被存储并传递给解码器。解码器通过使用存储的集合索引来对特征图进行上采样：它使用一个可训练的卷积操作对这些上采样映射进行卷积，以重建输入图像。
Architecture SegNet由一个编码器网络和一个相应的解码器网络，以及最后一个像素级分类层组成，如上图所示。编码器网络由13个卷积层组成，分别对应VGG16网络中的前13个卷积层。因此，我们可以从大数据集上训练的权值来初始化训练过程。删除全连接层以便编码器的最终输出保留更高分辨率的特征图。每个编码器层都具有与其相对应的解码器层，因此该解码器网络也具有13层。最终的解码器输出被输入到一个softmax分类器，以预测每个像素的类别概率。
编码器网络中的每个编码器进行卷积操作，以生成特征图，接着是Batch Normalization、ReLU和2×2大小（步幅为2）的最大池化（非重叠窗口）。随着下采样，图像分辨率降低，不利于分割时的边界预测，因此，在进行下采样之前，需要在编码器特征图中捕获和存储边界信息。如果推理过程中的内存不受约束，那么所有的编码器中的特征图都可以被存储，但通常不会这样实现。因此我们提出了一种方法：即只存储最大池化索引，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，这可以对每个2×2池窗口使用2位来完成。虽然这种较低的内存存储会导致轻微的精度损失，但仍然适用于实际应用。
解码器使用从相应的编码器层获取到的最大池化索引对其输入特征图进行上采样，以生成稀疏特征图，如下图所示。然后将这些特征图进行卷积，以生成密集的特征图。解码器与其对应的编码器，它们的通道数也是相同的，除了第一个编码器对应的解码器。第一个编码器的输入通道数是3，而其对应的解码器（最后一个）通道数对应的是类别的数量。 与SegNet相比，U-Net（用于医学领域）不重用池化索引，而是将整个特征图（以更多内存为代价）转移到相应的解码器，并将它们连接到上采样（通过反卷积）解码器特征图。另一方面，SegNet使用来自VGG网的所有预训练的卷积层权值作为预训练的权值。
Decoder Variants  总设计了8中不同的解码器变体架构，分别如下：
（1）Bilinear-Interpolation：使用固定的双线性插值权值的上采样方法，即不需要学习的上采样；
（2）SegNet-Basic： a smaller version of SegNet，4个encoder和4个decoder；
（3）SegNet-Basic-EncoderAddition：在解码器部分添加上相对应的编码器部分的特征图；
（4）SegNet-Basic-SingleChannelDecoder：在SegNet中，解码器对特征图进行上采样之后，会进行卷积操作，其通道数与对应编码器的通道数相同，对其变形就是设置通道数为单通道；
（5）FCN-Basic：与SegNet-Basic共享相同的编码器网络，但在解码器中使用反卷积进行上采样；
（6）FCN-Basic-NoAddition：不将编码器中的特征图添加到解码器中；
（7）FCN-Basic-NoDimReduction：在编码器中不进行降维操作；
（8）FCN-Basic-NoAddition-NoDimReduction：既不在编码器中进行降维，也不将编码器的特征图添加到解码器中。
上述8中变形结构的预测结果如下表所示，利用全局精度G、类平均精度C、平均交并比mIoU和语义轮廓度量BF来评价性能： Analysis  为了比较不同的解码器变体的定量性能，我们使用三个常用的性能度量：全局精度G，类平均精度C和平均交并比mIoU，计算语义轮廓得分BF就是评估f1-score，其中包括计算精度和召回率。
从上表中，我们可以看到，基于双线性插值的没有任何学习的上采样在所有精度指标上表现最差，而其余所有学习到的上采样（FCN-Basic及其变体）或上采样后的进行卷积（SegNet-Basic及其变体）的方法都表现得明显更好，强调了学习解码器的必要性。
当比较SegNet-Basic和FCN-Basic时，我们可以看到两者在这个测试的所有准确性测量上都表现得同样好。不同之处在于，SegNet在推理过程中使用了更少的内存，因为它只存储最大池索引；另一方面，FCN-Basic需要存储编码器中的特征图，会消耗更多的内存。同时，相比于使用了降维的FCN-Basic，SegNet-Basic中的特征维度会更多，因此，FCN-Basic的推理速度会更快。从另一个角度来看，SegNet-Basic中的解码器网络使其总体上成为一个比FCN-Basic更大的网络，这赋予了它更多的灵活性，因此在相同的迭代次数下获得了比FCN-Basic更高的训练精度。总的来说，当推理时间内存受到限制，但推理时间可能在一定程度上受损时，SegNet-Basic比FCN-Basic有优势。
SegNet-Basic在解码器方面与FCN-Basic-NoAddition最相似，两者都学会了生成密集的特征图，要么通过反卷积，要么通过上采样接着卷积。SegNet-Basic的性能更优越，部分原因是它的解码器尺寸更大。FCN-Basic-NoAddition的准确性低于FCN-Basic，这表明，捕获编码器特征图中的信息是至关重要的。
FCN-Basic-NoAddition-NoDimReduction模型的大小比SegNet-Basic略大，因为最终的编码器特征图没有被降维以匹配类别的数量。该模型的测试性能和训练精度都不如SegNet-Basic，这表明使用一个更大的解码器是不够的，但捕获编码器的特征图信息是很重要的，特别是细粒度的轮廓信息。
总的来说：
（1）当编码器特征图可以完全存储时，能够获得最佳性能。这一点最清楚地反映在语义轮廓度量(BF)中；
（2）当推理过程中的内存受到限制时，就可以存储压缩形式的编码器特征图（降维、最大池索引），并与适当的解码器一起使用，以提高性能；
（3）更大的解码器可以提高给定的编码器网络的性能。
Conclusion  我们提出了SegNet，一种用于语义分割的深度卷积网络架构。我们对SegNet及其它变体进行了分析，以揭示设计分割架构所涉及的实际权衡，特别是训练时间、内存和准确性。那些完全存储编码器网络特征图的体系结构表现最好，但在推理期间需要消耗更多的内存。另一方面，SegNet更有效，因为它只存储特征图的最大池化索引，并将其用于自己的解码器网络中，以获得良好的性能。</description>
    </item>
    
    <item>
      <title>U-Net&amp;U-Net&#43;&#43;</title>
      <link>https://gdy0924.github.io/posts/u-netu-net&#43;&#43;/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/u-netu-net&#43;&#43;/</guid>
      <description>U-Net: Convolutional Networks for Biomedical Image Segmentation
原文链接：U-Net
Abstract  深度网络的成功训练通常需要数千个带标签的训练样本。在本文中，我们提出了一种网络和训练策略，通过数据增强来更有效地使用可用的带标签样本。该网络结构由两条路径组成，一个是捕捉上下文的contracting path，另一个是能够实现精确定位的对称的expanding path组成。通过实验证明，这种网络可以从很少的图像中实现端到端的训练。
Introduction  在过去的两年里，深度卷积网络在许多视觉识别任务中都表现出色。但由于可用的训练集的大小和网络的大小，卷积网络的成功受到了限制。卷积网络的典型应用是在分类任务中，对图像的输出是一个单一的类别标签。然而，在许多视觉任务中，特别是在生物医学图像处理中，所需的输出应该包括定位，即为每个像素分配一个类标签。同时，大规模的训练图像在生物医学任务中通常是无法获得的。因此，有工作在滑动窗口设置中训练一个网络，通过在像素周围的局部区域（patch）作为输入来预测每个像素的类标签。首先，这个网络可以定位。其次，在patch方面的训练数据远远大于训练图像的数量。
显然，上述方法有两个缺点。首先，非常慢，因为网络必须为每个patch单独运行，而且由于重叠的patch，会有很多冗余。其次，在定位精度和上下文的使用之间存在一种权衡。较大的补丁需要更多的最大池化层，从而降低了定位精度，而较小的补丁只允许网络看到很少的上下文。
在本文中，我们提出了一个新的网络结构，即所谓的“全卷积网络”。我们对FCN结构进行修改与扩展，使其可以在很少的训练图像下运行，并产生更精确的分割，如下图所示，其主要思想是通过连续的层来添加一个contracting network，其中池化操作被上采样操作所取代。因此，这些层可以提高输出的分辨率。为了实现定位，将从contracting path获得的高分辨率特征图与上采样的输出相结合。 在该结构中，一个重要的修改是，在上采样部分，有大量的特征通道，这样可以将上下文信息传播到更高分辨率的层。因此，expanding path与contracting path是对称的，并形成一个u形结构。该网络没有全连接层，并且卷积层。为了预测图像边界区域中的像素，通过镜像输入图像来引入缺失的上下文。
由于可用的训练数据很少，我们通过对可用的训练图像进行数据增强。许多细胞分割任务中的另一个挑战是分离同一类别的接触对象（即细胞间的连接处），因此使用加权损失，即接触细胞单元之间的分离背景标签在损失函数中的权重很大。
Network Architecture  网络架构如上图所示所示。它由contracting path（左侧）和expanding path（右侧）组成。contracting path遵循卷积网络的典型架构：包括连续两个3x3的卷积，接着是通过ReLU和2x2的最大池化层（下采样），每经过一次下采样，特征通道数将增加一倍。expanding path中的每个block都先对对特征图的进行2倍的上采样，同时将特征通道数量减半，再与contracting path中相应裁剪的特征图进行concat拼接操作，接着是两个3x3卷积（跟着ReLU），这次卷积不改变图像大小。在最后一层，使用1x1的卷积将每个64个分量的特征向量映射到所需的类的数量，该网络总共有23个卷积层。（在上采样的过程中会丢失部分语义特征，通过拼接的方式，可以恢复部分的语义信息，从而保证分割的精度）
网络的输入是一张$572×572$的边缘经过镜像操作的图片，contracting path的输出是$32×32$大小的特征图，expanding path通过反卷积将特征图的尺寸扩大两倍，同时将其通道数数减半，和左侧对称的contracting path的特征图进行拼接，由于左侧contracting path和右侧expanding path的特征图的尺寸不一样，U-Net通过将contracting path的特征图裁剪到和expanding path相同尺寸的特征图。
Overlap-Tile  该策略的思想是：对图像的某一块像素点（黄框内部分）进行预测时，将原始的大图像分割成patch，patch的边缘在原图中可能不是边缘，需要该图像块周围的像素点（蓝色框内）提供上下文信息（context），以获得更准确的预测，如下图所示，预测部分为黄框，输入区域为蓝框。
这样的策略会带来一个问题，图像边界的图像块没有周围像素，因此周围像素采用了镜像扩充，如下图所示，白色框边缘外边部分为镜像填充的部分。
这样的操作会带来图像重叠问题，即第一块图像周围的部分会和第二块图像重叠。因此作者在卷积时只使用有效部分（valid part of each convolution），不使用padding，虽然卷积的时候会用到周围的像素点（蓝色框内），但最终传到下一层的只有中间原先图像块（黄色框内）的部分，并且使最终的输出图像对应要真是预测的部分（也可理解为连续的零填充会带来一些误差）。
Training  利用输入的图像及其对应的分割图，通过随机梯度下降训练网络。为了最小化开销并最大限度地利用GPU内存，我们支持大型输入块而不是大批处理大小，因此将批处理减少为单个图像。因此，我们使用了一个高动量（0.99）。
损失函数通过Softmax和交叉熵损失函数相结合， $$ E=\sum_{x\in \Omega }w(x)log(p_{l(x)}(x)) $$ 其中，$l:\Omega \rightarrow \lbrace 1,&amp;hellip;,K \rbrace$为每个像素的真实标签，$w:\Omega \rightarrow \mathbb{R}$是引入的权重系数，在训练过程中赋予边界像素更大的重要性。
我们预先计算出针对每个ground truth分割的权重图，以使训练数据集中某一类像素的权重更大，并迫使网络学习我们在接触细胞单元之间引入的小分离边界，如下图是的c和d： （上图：用显微镜记录玻璃上的HeLa细胞。（a）原始图像；（b）ground truth：不同的颜色表示HeLa细胞的不同实例；（c）生成的分割掩码：白色为前景，黑色为背景；（d）像素级的损失权重图，以迫使网络学习边界像素，越靠近红色，权重越大。）</description>
    </item>
    
    <item>
      <title>FCN</title>
      <link>https://gdy0924.github.io/posts/fcn/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/fcn/</guid>
      <description>FCN：Fully Convolutional Networks for Semantic Segmentation
原文链接：FCN
Abstract  卷积网络是一种能够产生特征层次结构的视觉模型，我们证明了通过卷积网络，经过端到端的训练，其性能超过了语义分割领域最先进的方法。建立一个“全卷积”网络，该网络可以接受任意大小的输入，并通过有效的推理和学习产生相应大小的输出。我们定义并详细描述了全卷积网络的结构，并与之前的模型建立了联系。我们将目前的分类网络（AlexNet，VGGnet和GoogLeNe）修改为全卷积网络，并通过微调将其学习到的表示转移到分割任务中。然后，形成了一种新的架构，它将深层、粗层的语义信息与浅层的信息结合起来，以产生准确和详细的分割。
Introduction  卷积网络正在推动图像识别技术的进步，起不仅在图像分类方面得到了改进，而且在具有结构化输出的任务上也取得了进展，包括bounding-box目标检测、部分和关键点的预测等。在从粗略推理到精细推理的过程中，很自然的下一步是对每个像素进行预测。先前的方法使用卷积网络进行语义分割，其中每个像素都标记其封闭对象或区域的类别，但这项工作有缺点。
我们提出了一个全卷积网络(FCN)，端到端进行训练，实现像素到像素的语义分割。据我们所知，这是第一个端到端的训练FCNs：（1）进行像素级预测，（2）来自监督预训练。现有网络的全卷积版本可以预测来自任意大小的输入的密集输出。通过前馈计算和反向传播对全图像进行学习和推理，网络内的上采样层能够在具有下采样池的网络中进行像素级预测和学习。
语义分割面临着语义和位置之间的内在紧张关系：全局信息解决了什么这一问题，而局部信息解决了在哪里这一问题。深度特征层次结构在一个局部-全局的金字塔结构中联合编码位置和语义信息，我们定义了一种新的“skip”结构，将深、粗的语义信息和浅、细的位置信息结合起来（）。
Related work  我们的方法利用了深度图像分类网络和迁移学习。首先是在各种视觉识别任务上进行转移，然后在目标检测领域，以及在混合分类器模型的实例和语义分割上。我们现在重新设计并微调分类网络，以实现语义分割。
Fully convolutional networks Dense prediction with convnets  最近的一些工作已经将卷积网络应用于密集的预测问题，这些方法的共同之处在于：限制和感受野的小模型，逐patch训练，通过超像素投影、随机场正则化、过滤或局部分类，由OverFeat引入用于密集输出的输入移位和输出交错，多尺度金字塔处理，tanh非线性激活。与这些现有的方法不同，我们采用和扩展了深度分类网络架构，使用图像分类作为有监督的预训练，并进行全卷积的微调，以简单有效地从整个图像输入和整个图像ground truth中学习。
Fully convolutional networks  卷积网络中的每一层都是一个大小为$h×w×d$的三维数组，其中$w$和$h$是空间维度，$d$是特征维度（通道数）。卷积神经是建立在平移不变性之上的，它们的基本组件（卷积、池化和激活函数）作用于局部输入区域，并且只依赖于相对的空间坐标。如果将某一层中位置$(x,y)$的数据向量表示为$x_{ij}$，$y_{ij}$为下一层的数据，其计算公式为： $$ y_{ij}=f_{ks}(\lbrace x_{si+\delta i,sj+\delta j} \rbrace _ {0\leq \delta i,0\leq \delta j}) $$ 其中，$k$为卷积核大小，$s$为步长，$f_{ks}$定义了层的类型，卷积或平均池化的矩阵乘法，最大池化，或激活函数等等。
接下来，我们将解释如何将分类网转换为生成粗略特征图的全卷积网络。对于像素级预测，我们需要将这些粗略输出连接回像素。
Adapting classifiers for dense prediction  典型的识别网络，包括LeNet、AlexNet及其后续的网络，接受固定大小的输入并产生输出。这些网络的全连接层具有固定的尺寸，放弃空间坐标。然而，这些全连接层也可以被视为：与覆盖其整个输入区域的内核进行的卷积操作（即卷积大小为整个输入大小）。这样做可以将它们转换成完全卷积的网络，以接受任何大小的输入和输出，如下图所示，得到的最终特征图就相当于对特定输入patch的预测 卷积模型的空间输出图使其可以实现语义分割等密集预测的任务，在每个输出单元上都有ground truth，前向和后向传播都是简单的，并且两者都利用了卷积固有的计算效率。
虽然我们将分类网络重新修改为全卷积网络，从而可以将任何大小的输入生成输出特征图，但输出维度通常通过下采样实现减少。分类网络的下采样是为了保持卷积核小和计算量的合理性，这使得分类网络的全卷积版本的输出非常粗糙。
Upsampling is backwards strided convolution  另一种连接粗略输出到密集像素的方法是插值。例如，简单的双线性插值通过计算距离最近的四个输入的线性映射得到每一个输出$y_{ij}$，该映射只依赖于输入和输出单元的相对位置。在某种意义上，采样因子为$f$的上采样与步长为$\frac{1}{f}$的卷积相同，只要$f$是整数，那么上采样方法就是输出步长为$f$的backwards convolution（有时称为deconvolution）。因此，可以在网络内进行反向传播，以实现端到端学习。deconvolution层的卷积核是不需要固定的（例如，双线性上采样），是可以学习的。在我们的实验中，发现网络中的上采样对于学习密集预测是快速和有效的。
Segmentation Architecture  我们将ILSVRC分类器转换成FCNs，并通过上采样和像素级损失进行密集预测。我们通过微调来训练分割。接下来，我们引入了一个新的”skip“架构，它结合了粗略的、语义的信息和局部的信息来重新细化预测。</description>
    </item>
    
    <item>
      <title>Vision Transformer</title>
      <link>https://gdy0924.github.io/posts/vit/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/vit/</guid>
      <description>AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
原文链接：ViT
Abstract  虽然Transformer架构已经成为自然语言处理任务的实际标准，但它在计算机视觉中的应用仍然有限。在视觉领域中，注意力机制要么与卷积网络结合，要么用于替换卷积网络的某些部分，同时保持其整体结构。在本文中，证明了依赖CNN是不必要的，一个直接应用于图像patch序列的transformer可以很好完成图像分类任务。使用大量数据进行预训练，并转移到多个中型或小型图像识别进行基准测试，与最先进的卷积网络相比，Vision Transformer(ViT)获得了良好的结果，同时需要更少的计算资源来训练。
Introduction  自注意力机制，特别是transformer，已经成为自然语言处理的首选模型。主要的方法是在大型文本语料库上进行预训练，然后在较小的特定任务数据集上进行微调。然而，在计算机视觉中，CNN架构仍然占主导地位。受NLP成功的启发，许多工作尝试将自注意力机制与卷积架构相结合，但是在大规模图像识别中，经典的ResNet类架构仍然是最先进的技术。
受NLP中transformer成功的启发，我们将transformer架构直接应用于图像，并实现最少的修改。为此，我们将一个图像分割成一些patch，并将patch的线性embedding序列作为transformer的输入。patch的处理方式与NLP中token的处理方式相同。我们以有监督的方式训练该模型进行图像分类。
当在中等大小的数据集，如ImageNet上进行训练时，模型产生的精度比相当大小的ResNets低几个百分点。但是，通过在更大的数据集上进行训练，我们发现，ViT在足够规模的预训练和转移到数据点较少的任务时获得了优秀的结果。
Relative Work  Transformer已成为许多NLP任务中的最新方法，通常利用transformer在大型语料库上进行预训练，然后对特定的任务进行微调。
与该工作最相关的是：论文《 On the relationship between self-attention and convolutional layers》中提出的模型，该模型从输入图像中提取大小为2×2的patch，并在顶部应用自注意力机制。这个模型与ViT非常相似，但我们的工作进一步证明，大规模的预训练使transformer能够与最先进的卷积网络竞争（甚至更好）。此外，该模型使用了2×2像素的小patch尺寸，这使得该模型仅适用于小分辨率图像，而我们也处理中等分辨率图像。
Method Vision Transformer (ViT) 上图为该模型的整体架构：transformer接收一个一维的token embedding序列，为了处理二维图像，我们将图像$x\in \mathbb{R}^{H\times W\times C}$重塑为展平的2维patch序列$x_{p}\in \mathbb{R}^{N\times (P^{2}\cdot C)}$，其中$(H,W)$是原始图像的分辨率，$C$为通道数量，$(P,P)$是图像patch的分辨率，$N=\frac{HW}{P^{2}}$是patch的数量，也是transformer的有效输入序列长度。transformer在所有的层中使用恒定的潜在向量大小$D$，因此，我们将补丁展平，并通过可训练的线性投影映射到D维，如下面公式所示，我们将这个投影的输出称为patch embedding。 $$ z_{0}=[x_{class};x_{p}^{1}E;x_{p}^{2}E;&amp;hellip;;x_{p}^{N}E]+E_{pos},E\in \mathbb{R}^{(P^{2}\cdot C)\times D},E_{pos}\in \mathbb{R}^{(N+1)\times D} $$ 与BERT的$[class]$token类似，我们在embedding的patch序列中加入了一个可学习的embedding：$z_{0}^{0}=x_{class}$，其为transformer的encoder的输出层的状态$z_{L}^{0}$，作为图像的表示$y$，如下面公式所示。在预训练和微调过程中，一个分类头都被附加在$z_{L}^{0}$上：在预训练时，分类头由一个隐藏层的MLP实现，在微调时由一个单一的线性层实现。（即用最终的输出状态$z_{L}^{0}$来分类） $$ y=LN(z_{L}^{0}) $$ 位置embedding被添加到patch embedding中，以保留位置信息。我们使用标准的可学习的一维位置embedding，因为我们没有观察到使用更先进的二维位置embedding能显著的提高性能，所得到的embedding向量序列作为编码器的输入。
transformer的encoder由Multi-Head自注意（MSA）和MLP块（MLP）组成。在每个块之前应用层归一化（Layernorm，LN），在每个块之后应用残差连接。 $$ {z_{t}}&#39;=MSA(LN(z_{t-1}))+z_{t-1} $$ $$ z_{t}=MLP(LN({z_{t}}&#39;))+{z_{t}}&#39; $$ Hybrid Architecture</description>
    </item>
    
    <item>
      <title>SSD</title>
      <link>https://gdy0924.github.io/posts/ssd/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/ssd/</guid>
      <description>SSD: Single Shot MultiBox Detector
原文链接：SSD
Abstract  提出了一种利用一个深度神经网络来实现目标检测的方法：SSD，根据不同的高宽比和每个特征图定位的尺寸大小，将bounding-box的输出空间离散为一组默认框。在预测时，该网络为每个默认框中每个对象类别的存在生成分数，并对该框进行调整，以更好地匹配对象形状。此外，该网络结合了不同分辨率的多个特征图进行预测，以能够处理不同大小的对象。相比于需要候选框生成的方法，SSD很简单：完全取消了候选区域生成和后续的像素/特征重采样阶段，将所有的计算过程封装在一个网络中。
Introduction  目前最先进的对象检测方法流程是：生成候选框，为每个框重新采样像素或特征，将其输入到一个分类器中。以上方法速度很慢。
本文提出了第一个基于深度网络的对象检测器，不需要为候选框重新采样像素或特征，并且可以达到一样的精度（精度要比YOLO高）。速度的提升来自于消除候选框生成和后续的像素或特征重采样阶段。
我们的改进包括使用一个小的卷积核来预测bounding-box的对象类别和偏移量，为不同的高宽比检测使用单独的预测器（卷积核），并将这些卷积核应用在网络后期的多个特征图上，以便在多个尺度上检测。通过这些修改，特别是在不同尺度上使用多层进行预测，可以使用相对较低分辨率的输入来实现高精度，进一步提高检测速度。
Contributions：
（1）引入SSD，多个类别的单阶段检测器，比之前的单阶段检测器YOLO更快更精确，与更慢的Faster R-CNN精度相同；
（2）SSD的核心是为一组固定的默认边界框预测类别和偏移量，通过使用较小的应用在特征图上的卷积核；
（3）为了获得较高的检测精度，我们从不同尺度的特征图中生成不同尺度的预测，并通过长宽比明确地分离预测；
（4）以上的改进形成了简单的端到端训练和高精度，甚至在低分辨率的输入图像上，进一步提高了速度和准确性。
The Single Shot Detector (SSD) Model  SSD方法是基于前馈卷积网络的，该网络生成固定大小的bounding-box集合，并获得这些框中对象类实例的分数，然后通过NMS产生最终的结果。早期的网络是基于高质量图像分类的标准结构（去掉最终的分类层），我们将称之为base network，我们向网络中添加辅助结构，以生成具有以下特征的检测器：
Multi-scale feature maps for detection
在base network的末端添加卷积特征层，通过这些层，特征图的大小逐步减小，并在多个尺度的特征图上进行检测，对于每一个特征层，用于预测检测的卷积模型是不同的。（Overfeat和YOLO都在单尺度特征图上检测的）。
Convolutional predictors for detection
每个添加的特征层（或是来自base network的现有特征层）都可以使用一组卷积核生成一组固定的检测预测，如下图所示。对于大小为$m×n$、通道数为$p$的特征层，使用$3×3×p$的卷积核，生成一个类别的分数或相对于默认框的偏移量，在$m×n$的每个位置应用卷积核，产生一个输出值。边界框偏移输出值是相对于相对于每个特征图位置的默认框位置来测量的（参见YOLO的架构，它在此步骤中使用的是全连接层而不是卷积）。 Default boxes and aspect ratios
我们将一组默认的边界框与每个特征图的网格关联起来（类似于Faster R-CNN中的anchor）。默认框以卷积的方式滑过特征图，这样每个框相对于其对应网格的位置就是固定的。在每个特征图的网格中，我们预测相对于网格中默认框的偏移量，以及每个框中存在类实例的类别分数。
在每个网格生成$k$个默认框，计算$c$个类分数和相对于原始默认框的4个偏移量，特征图中的每个网格生成$(c+4)×k$个输出，即一个$m×n$大小的特征图产生$(c+4)×k×m×n$个输出，我们的默认框与anchor相同，不过我们将其应用在不同分辨率的特征图上。
default box
如下图所示： （a）表示SSD在训练期间只需要一个输入图像和每个object的ground truth。以卷积的方式，在特征图的每个位置生成一组不同高宽比的默认框（如图中一组为4个），并且应用在不同尺度的特征图上（（b）为$8×8$，（c）为$4×4$）。对于每个默认框，预测所有对象类别和偏移量。
Training  训练SSD与训练两阶段的检测模型的区别在于，ground truth的信息需要被分配给固定的检测器的输出集合中的特定输出。
Matching strategy
在训练过程中，我们需要确定哪些默认框与ground truth相对应，对于每个ground truth框，我们从不同位置、高宽比和尺寸的默认框中进行选择。首先将每个ground truth框与具有 the best jaccard overlap的默认框进行匹配。与MultiBox不同的是，我们将与ground truth的jaccard重叠高于阈值（0.</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://gdy0924.github.io/posts/bert/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/bert/</guid>
      <description>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
原文链接：BERT
Abstract  引入了一种新的语言表示模型，称为BERT（Bidirectional Encoder Representations from Transformers）。与最近的语言表示模型不同，BERT是从未标记文本中预训练出的双向表示，通过联合所有层中的左右上下文。因此，预训练的BERT模型可以通过一个额外的输出层来进行微调，从而为广泛的任务创建最先进的模型。
Introduction  预训练的语言模型已被证明对许多自然语言处理任务都有效。目前有两种策略为下游任务应用预训练模型：基于特征的和微调的。基于特征的方法使用特定的任务框架，包括预训练的表示作为额外的特征。微调的方法引入最少的参数，通过简单的微调所有的预训练参数来训练下游任务。我们认为，目前的技术限制了预训练的表示，特别是对于微调方法，主要的限制是标准语言模型是单向的，这限制了在预训练过程中可以使用的架构的选择。
在本文，我们改进了基于微调的方法，提出了BERT（Bidirectional Encoder Representations from Transformers），其通过使用预训练的掩码语言模型（masked language model，MLM）减轻了前面提到的单向性约束。掩码语言模型从输入中随机掩码一些token，其目的是仅根据其上下文来预测掩码词的原始词汇表id。与从左向右的预训练的语言模型不同，MLM使表示能够融合左右上下文，这使我们能够对双向Transformer进行预训练。除了掩码语言模型，我们还使用了“预测下一个句子”任务，联合预训练文本对的表示。
Contributions：
（1）证明了双向预训练对语言表示的重要性；
（2）证明了预训练表示可以减少对许多精心设计的任务特定架构的需要，BERT是第一个基于微调的表示模型，它在大型句子级和标记级任务上取得了最优的效果。
Related Work Unsupervised Feature-based Approaches  学习广泛适用的单词表征一直是一个活跃的研究领域，包括非神经方法和基于神经网络的方法。预先训练好的单词embedding是现代自然语言处理领域的一个组成部分，与从头开始学习的embedding相比，它提供了显著的改进。为了对单词embedding向量进行预训练，我们使用了从左向右的语言建模目标，以及在左右上下文中区分正确和不正确单词的目标。
Unsupervised Fine-tuning Approaches  最近，产生上下文标记表示的句子或文档编码器可以从未标记的文本中预训练得到，并针对有监督的下游任务进行了微调。这些方法的优点是很少需要从头开始学习参数。从左到右的语言建模和自动编码器已用于预训练这些模型。
Transfer Learning from Supervised Data  有研究表明，可以从大型数据集的监督任务中进行有效转移，如自然语言推理和机器翻译。计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，其中有效的方法是对使用ImageNet预训练的模型进行微调。
BERT  我们将在本节中详细的介绍BERT。在我们的框架中，共有两个步骤：预训练和微调。在预训练过程中，该模型在不同的预训练任务上对未标记的数据进行训练；为了进行微调，首先使用预先训练好的参数初始化BERT模型，并使用从下游任务中获得的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。下图是以问答为例的示意图： BERT的一个显著特点是针对不同任务使用统一框架，即预训练的架构和最终的下游任务架构之间的差别很小。
Model Architecture：BERT的模型架构是一个多层双向Transformer编码器，不再详细介绍，详见transformer（Attention is all you need）。在该项目中，层数（transformer块）由$L$表示，隐藏层大小为$H$，自注意力的头数为$A$，我们的结果主要是在两个模型大小上的：$BERT_{BASE} (L=12,H=768,A=12)$和$BERT_{LARGE}(L=24,H=1024,A=16)$。 Input/Output Representations：为了使BERT能够处理各种下游任务，我们的输入表示能够明确地在token序列中表示单个句子和一对句子（如：&amp;lt;Question，Answer&amp;gt;）。在该项目中，一个“sentence”可以是一个连续文本的任意跨度，而不是一个实际的语言句子；一个“sequence”指的是BERT的输入token序列，它可以是单个句子或两个句子放在一起的。
使用WordPiece embedding，其中包含30,000个词汇token，每个序列的第一个token是特殊的分类标记[CLS]。与该token对应的最终隐藏状态被用作分类任务的序列表示（即用该token进行后续的分类任务）。句子对形成一个单一的序列。我们用两种方式来区分这些句子：首先，用一个特殊的标记[SEP]将它们分开；其次，我们在每个token中添加一个可学习的embedding，表明它是属于句子$A$还是句子$B$。如上图所示，我们将输入embedding记为$E$，最终隐藏向量的特殊token[CLS]记为$C$，第$i$个输入token的最终隐藏向量表示为$T_{i}$。
对于一个给定的token，它的输入表示是通过将相应的token、段embedding和位置embedding相加来形成的，如下图所示。 Pre-training BERT  我们没有使用传统的从左到右或从右到左的语言模型对BERT进行预训练，我们使用两个无监督的任务对BERT进行预训练，如第一张图的左侧所示。</description>
    </item>
    
    <item>
      <title>YOLO系列</title>
      <link>https://gdy0924.github.io/posts/yolo%E7%B3%BB%E5%88%97/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/yolo%E7%B3%BB%E5%88%97/</guid>
      <description>YOLO V1  原文链接：YOLO V1
参考链接：你真的读懂yolo了吗？
Abstract  提出一种新的目标检测算法——YOLO，之前的目标检测工作都是使用另外的分类器实现检测，我们将目标检测构建成回归问题框架，在空间上分离bounding-box和相关类别概率。一个网络直接从完整图像中预测bounding-box和类别概率，因此速度很快。
Introduction  最近的方法，如R-CNN使用候选区域生成方法，首先在图像中生成候选框，然后在这些候选框上进行分类，分类后，再对bounding-box进行细化，消除重复框。这些复杂的操作导致运行速度缓慢，难以优化，因为每个单独的网络必须单独训练。
我们将目标检测重新定义为单一的回归问题，直接从图像像素到bounding-box坐标和分类概率，&amp;ldquo;you only look once&amp;rdquo;（YOLO）就可以预测有什么物体以及它们在哪里。其架构如下图所示（YOLO包括三部分：将输入图像调整为448×448，在图像上运行单一卷积网络，通过模型的置信度来确定检测结果的阈值。）：单个卷积网络同时预测多个bounding-box和及其分类概率，YOLO训练完整的图像，并直接优化检测性能。
相比于之前的网络，其有如下优点：
（1）YOLO的速度非常快：由于我们将目标检测作为一个回归问题，所以我们不需要一个复杂网络。基本网络以每秒45帧的速度运行，另外，YOLO实现的平均平均精度是其他实时系统的两倍以上；
（2）YOLO在做预测时关注的是图像的整体（全局）：与基于滑动窗口和候选区域生成的方法不同，YOLO在训练和测试期间可以看到整个图像，因此它隐式地编码上下文信息，而对于Fast R-CNN，由于看不到上下文信息，会错误的将背景patch预测为目标类；
（3）YOLO学习目标的可推广表示：当在图像上进行训练和测试时，YOLO的性能远远优于DPM和R-CNN等其他检测方法，当应用在新的领域或者有预期之外的输入时，YOLO也可以很好的运行。
YOLO在精度上仍然落后于最先进的检测模型，虽然它可以快速识别图像中的物体，但它很难精确定位一些物体，特别是小的物体。
Unified Detection  我们将目标检测中的独立网络统一为一个单一的神经网络，使用来自整个图像的特征来预测每个bounding-box，同时预测类别概率，即我们的网络会对整个图像和图像中的所有目标对象进行全局推理。 我们的模型将输入图像分割成一个$S×S$网格，如果一个对象的中心落入某一网格单元格中，则该网格单元格负责检测该对象。
每个网格单元格预测$B$个bounding-box和bounding-box的置信度分数，这些置信度分数反映了该bounding-box包含物体的真实程度，也反映了该bounding-box预测的准确程度。我们将信心定义为$Pr(Object)\ast IOU_{pred}^{truth}$：如果该单元格中不存在对象，则置信度分数应该为零，否则，我们希望置信度得分等于bounding-box和ground truth之间的IOU值。
每个bounding-box由5个预测部分组成：$x,y,w,h,confident$，$(x,y)$坐标表示相对于网格单元格边界的bounding-box的中心，$w$和$h$是相对于整个图像进行预测的bounding-box的宽度和高度，$confident$表示bounding-box和ground truth之间的IOU。
每个网格单元还预测类别$C$的条件概率，$Pr(Class_{i}|Object)$，我们只预测每个网格单元的一组类别概率，而不管bounding-box的数量。
在测试时，我们将类别条件概率和单独的bounding-box置信度预测相乘： $$ Pr(Class_{i}|Object)\ast Pr(Object)\ast IOU_{pred}^{truth}=Pr(Class_{i})\ast IOU_{pred}^{truth} $$ 即每个bounding-box的特定类别的置信分数，这些分数既编码了该类出现在框中的概率，也编码了bounding-box与物体的匹配程度。
我们设置$S=7$，$B=2$，如下图所示：该模型将图像划分为一个$S×S$的网格，并为每个网格单元预测$B$个bounding-box，及其置信度和$C$类的概率，因此，这些预测值被编码为一个$S×S×(B\ast 5+C)$张量。 Network Design  我们将该模型作为一个卷积神经网络实现，网络的初始卷积层从图像中提取特征，而全连接层则预测输出的概率和bounding-box坐标。
我们的网络架构是基于GoogleNet的，共有24个卷积层，2个全连接层。没有使用GoogLeNet中的Inception模块，而是简单地使用1×1降维层和3×3卷积层，如下图所示： 我们的网络的最终输出是预测的$7×7×30$大小的张量。
我们还训练了一个快速版本的YOLO，Fast YOLO使用的神经网络的卷积层更少（9个而不是24个），这些层的卷积核也更少。除了网络的大小之外，YOLO和Fast YOLO之间的所有训练和测试参数都是相同的。
Training  我们在ImageNet数据集上预训练了我们的卷积层；然后，我们转换模型来进行目标检测，检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从$224×224$提高到$448×448$；最后一层预测了分类概率和bounding-box坐标，我们根据图像的宽度和高度将边界框的宽度和高度进行归一化，使它们落在0到1之间，参数化边界框$x$和$y$坐标为特定网格单元位置的偏移，因此它们也被限定在0和1之间；最后一层使用线性激活函数，而所有其他层使用Leaky ReLU： $$ x&amp;gt; 0,f(x)=x $$ $$ otherwise,f(x)=0.1x $$ 我们使用均方误差进行模型优化，但对定位误差和分类误差的权重相同，因此可能并不理想。另外，在每个图像中，许多网格单元格不包含任何物体，其置信度分数就为0，远远超过了包含物体的单元格的梯度。
为了解决上述问题，我们增加bounding-box坐标预测的损失，减少不包含物体的单元格的预测的损失，引入两个参数，$\lambda_{coord}$和$\lambda_{noobj} $，并设置$\lambda_{coord}=5$、$\lambda_{noobj}=0.5$。
均方误差对大bounding-box和小bounding-box的误差具有同样的权重，但是误差度量应该反映：大bounding-box的小偏差比小bounding-box的小偏差小，为了解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。
YOLO为每个网格单元格预测多个bounding-box。在训练时，对于每一个物体，我们希望有一个bounding-box预测器去负责该物体，因此，我们指定一个预测器来“负责”预测一个物体，与ground truth有最高IOU的预测。这使得bounding-box预测器之间的专门化，每个预测器都能更好地预测特定大小、纵横比或物体的类别。
因此，我们的损失函数如下： $$ \lambda_{coord}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{obj}[(x_{i}-\hat{x_{i}})^{2}+(y_{i}-\hat{y_{i}})^{2}] $$ $$ +\lambda_{coord}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{obj}[(\sqrt{w_{i}}-\sqrt{\hat{w_{i}}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h_{i}}})^{2}] $$ $$ +\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{obj}(C_{i}-\hat{C_{i}})^{2} $$ $$ +\lambda_{noobj} \sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{noobj}(C_{i}-\hat{C_{i}})^{2} $$ $$ +\sum_{i=0}^{S^{2}}\mathbb{I}_ {i}^{obj}\sum_{c\in classes}(p_{i}(c)-\hat{p_{i}}(c))^{2} $$ 其中，$\mathbb{I}_ {i}^{obj}$表示单元格$i$是否有物体出现（0或1），$\mathbb{I}_ {ij}^{obj}$表示单元格$i$中第$j$个bounding-box预测器负责预测（0或1）。第一行是边界框中心坐标的误差；第二行是边界框长宽的误差，由于边界框大小本身便会影响到误差的大小，为了降低边界框大小的影响，而采用了开方差来比较；第三行是包含物体的单元格以及准确预测的边界框的置信度误差；第四行是不包含物体的单元格中以及准确预测的边界框的置信度误差；第五行是包含物体的单元格中，预测对象概率的各分项的误差平方之和。</description>
    </item>
    
    <item>
      <title>R-CNN系列</title>
      <link>https://gdy0924.github.io/posts/r-cnn%E7%B3%BB%E5%88%97/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/r-cnn%E7%B3%BB%E5%88%97/</guid>
      <description>R-CNN 原文链接：R-CNN
Abstract  在pascalVOC数据集上针对图像检测任务，其性能在过去几年中已经趋于稳定。性能最好的方法是复杂的集成系统，它通常将多个低级图像特征与高级上下文结合起来。在本文中，我们提出了一种简单且可扩展的检测算法，相对于之前对VOC2012的最佳结果，它将平均精度(mAP)提高了30%以上——实现了53.3%的mAP。
该方法具有两个关键点：（1）可以将卷积神经网络(CNNs)应用于自下而上的区域建议，以定位和分割对象；（2）当标记的训练数据稀缺时，通过对辅助任务的监督预训练，然后进行特定领域的微调，会产生显著的性能提高。
我们将区域建议与CNN结合起来，我们称我们的方法为R-CNN，即具有CNN特征的区域。
Introduction  LeCun等人表明，通过反向传播的随机梯度下降对于训练卷积神经网络(CNNs)是有效的，这是一类新的神经网络的模型。2012年，AlexNet上显示出更高的图像分类精度，重新点燃了人们对cnn的兴趣。因此人们开始讨论ImageNet上的CNN分类结果在多大程度上可以推广到pascalVOC挑战上的目标检测结果？
我们通过缩小图像分类和目标检测之间的差距来回答上述问题。本文首次证明，CNN可以显著提高对pascalVOC的目标检测性能。为了实现这一结果，我们关注两个问题：使用深度网络来定位对象，训练一个只用少量标注数据的高性能模型。
与图像分类不同，检测需要在图像中定位（可能有很多）对象。一种方法将定位框架定义为一个回归问题。另一种选择是建立一个滑动窗口检测器。为了保持高分辨率，这些CNN通常只有两个卷积层和池化层。我们考虑了滑动窗口的方法，但是在比较高层的单元，我们拥有5个卷积层，有很大的感受野和步长。
我们通过“区域识别”操作来解决CNN定位问题，在测试时，我们的方法为输入图像生成大约2000个类别独立的区域建议，使用CNN从每个建议中提取一个固定长度的特征向量，然后用线性支持向量机对每个区域进行分类。下图展示了方法概述及结果，由于我们的模型结合了区域建议和CNN，因此称之为R-CNN。 目标检测中面临的第二个挑战是标记数据的稀缺，目前可用的数量不足以训练大型的CNN。解决这个问题的传统解决方法是使用无监督的预训练，然后进行有监督的微调。因此，我们通过实验证明在大型辅助数据集(ILSVRC)上进行监督预训练，然后在小数据集(pascal)上进行微调，是在数据稀缺时学习高性能CNN的有效方法。我们注意到，由于R-CNN在图片的区域上进行操作，所以很自然地可以将其扩展到语义分割的任务。
Object detection with R-CNN  该目标检测模型由三个模块组成。（1）产生与类别不相关的候选区域，这些候选区域定义了检测模型的候选检测集；（2）一个大型的卷积神经网络，它从每个区域中提取一个固定长度的特征向量；（3）一组特定于类的线性支持向量机。
Module design  Region proposals：最近的各种论文提供了产生类别不相关的候选区域的方法。例如：，我们使用的是selective search方法。
Feature extraction：基于AlexNet网络，从每个候选区域中提取一个4096维的特征向量，网络输入是227×227RGB图像，接着通过5个卷积层和2个全连接层。为了提取候选区域的特征，由于网络的输入大小必须是227×227的，因此需要对候选区域进行缩放扭曲，如下图所示： Test-time detection  在测试时，我们对测试图像进行selective search，提取出大约2000个候选区域，将候选区域进行变换，经过CNN网络以提取特征，针对每个类，使用训练好的SVM进行评分。对于图像中所有的得分区域，使用NMS（对于每个类独立），即如果一个候选区域与另一个候选区域的IoU大于阈值，且另一个候选区域的得分大于该区域，则拒绝该区域。
Run-time analysis：两个操作使检测更有效：所有的CNN参数都在所有类别中共享；与其他常用方法相比，CNN提取到的特征向量是低维的。
Training  Supervised pre-training：我们在一个像素级标注的大型数据集(ILSVRC2012)上对CNN进行了预训练（该数据的边界框标签不可用）。
Domain-specific fine-tuning：为了使我们的CNN适应新的任务和新的领域，我们使用转换后的候选区域对CNN进行随机梯度下降(SGD)训练。CNN的架构没有变，除了最后的分类数变为（N+1），其中N是对象类的数量，+1作为背景。将与真值的IoU大于0.5的区域作为正类，其余为负类。
Object category classifiers：使用二分类来对目标进行检测，使用IoU重叠阈值来决定一个区域是否为正类，阈值选择0.3是在验证集上实验选择的（{0,0.1,&amp;hellip;,0.5}），我们发现，将阈值设置为0.5或者0时，mAP会降低。
针对每个类，都会训练一个线性SVM。
Selective Search 参考链接：Selective Search （以下部分是Selective Search论文中的内容。）
物体识别，在之前的做法主要是基于穷举搜索（Exhaustive Search）：选择一个窗口扫描整张图像（image），改变窗口的大小，继续扫描整张图像。这种做法是比较原始直观，改变窗口大小，扫描整张图像，非常耗时。若能过滤掉一些无用的box将会节省大量时间。这就是本文中Selective Search(选择性搜索)的优点，将穷举搜索（Exhausticve Search)和分割（Segmentation)结合起来。
层次归类算法(Hierarchical Grouping Algorithm)  input：图片
output：集合L
（1）获取图片初始分割区域$R=(r_{1},&amp;hellip;,r_{n})$；
（2）初始化相似度集合$S=\varnothing $；
（3）计算$R$中两两相邻区域$(r_{i},r_{j})$之间的相似度，添加到相似度集合$S$；
（4）从相似度集合$S$中找出，相似度最大的两个区域$r_{i}$和$r_{j}$，将其合并成为一个区域$r_{t}$，从$S$中除去原先与$r_{i}$和$r_{j}$相邻区域之间计算的相似度，计算$r_{t}$与其相邻区域（原先与$r_{i}$或$r_{j}$相邻的区域）的相似度，添加到相似度集合$S$中，将新区域$r_{t}$添加到区域集合$R$中；
（5）迭代步骤（4）直至$S$为空，即可合并区域都已合并完；
（6）获取R中每个区域的Bounding Boxes，这个结果就是图像中物体可能位置的可能结果集合L。
多样性策略  论文给出了两个方面的多样化策略：颜色空间多样化，相似度计算的多样化。</description>
    </item>
    
    <item>
      <title>Transformer</title>
      <link>https://gdy0924.github.io/posts/transformer/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/transformer/</guid>
      <description>原文链接：Transformer
Abstract  目前主要的序列模型是主要是基于复杂的循环或者卷积神经网络的，包括一个编码器（encoder）和解码器（decoder）。性能最好的模型通过一种注意力机制将编码器和解码器连接起来。我们提出了一种新的简单的网络架构，Transformer，完全基于注意机制，完全取消递归和卷积，并通过实验验证了其有效性。
Introduction  循环神经网络，特别是长短期记忆LSTM和门控循环GRU神经网络，已经成为序列建模和transduction问题的最先进的方法，如语言建模和机器翻译。此后，许多努力继续推动循环语言模型和编解码器架构。
循环模型通常会沿着输入和输出序列的位置进行计算，它们生成一系列隐藏状态$h_{t}$，通过前一个隐藏状态$h_{t-1}$和输入$t$的计算得来。这种顺序性质阻碍了训练过程的并行化，这在较长的序列长度时变得至关重要，因为内存限制了样本之间的批处理。
注意力机制已经成为各种任务中引人注目的序列建模和transduction模型的一个组成部分，其允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离，通常与循环神经网络一起使用。
在本文，我们提出了Transformer，一个不需要循环神经网络的模型架构，而是完全依赖于一个注意力机制来提取输入和输出之间的全局依赖关系，允许更多的并行化。
Background  为了减少顺序计算，提出了扩展神经GPU、ByteNe和ConvS2S等，这些方法都使用卷积神经网络作为基本的block，并行计算所有输入和输出位置的隐藏表示。在这些模型中，关联两个任意输入或输出位置的信号所需的操作数量随着距离的增大而增长，这使得学习远距离位置之间的依赖关系更加困难。
自注意力机制，是一种注意机制，将单个序列的不同位置联系起来，以计算序列的表示。自注意力机制已成功地应用于各种任务中，包括阅读理解、抽象摘要、文本隐含和学习任务独立的句子表征。
据我们所知，Transformer是第一个完全依赖于自注意力机制来计算其输入和输出的表示的transduction模型，而不使用序列对齐的循环或卷积神经网络。
Model Architecture  大多数神经序列transduction模型都是编码器（encoder）-解码器（decoder）结构。其中，编码器将一个符号表示的输入序列$(x_{1},&amp;hellip;,x_{n})$映射到一个连续表示的序列$z=(z_{1},&amp;hellip;,z_{n})$。给定$z$，解码器一次生成一个符号的输出序列$(y_{1},&amp;hellip;,y_{m})$，在生成下一个符号时，使用之前生成的符号作为附加的输入。
Transformer遵循encoder-decoder架构，使用堆叠的自注意力机制，编码器和解码器的全连接层，分别如下图的左半部分和右半部分所示： Encoder and Decoder Stacks encoder和decoder的堆叠  Encoder： encoder由6个完全相同的层组成（n=6），每个层都有两个子层，第一个子层是一个multi-head自注意机制，第二个子层是一个简单的、全连接的前馈网络。我们在两个子层中使用残差连接，接着进行层归一化（layer normalization）。也就是说，每个子层的输出是$LayerNorm(x+Sublayer(x))$，其中$Sublayer(x)$是由子层本身实现的函数。为了方便残差连接，模型中的所有子层以及embedding层都会产生$d_{model}=512$的输出。
Decoder： decoder也由6个完全相同的层组成（n=6）。除了每个编码器层中的两个子层外，decoder还增加了第三个子层，该子层对encoder的输出进行multi-head自注意操作。与encoder类似，我们在每个子层使用残差连接，然后进行层归一化。我们还修改了decoder中的自注意子层，以防止位置关注后续的位置。
Attention 注意力机制  注意力机制可以描述为将查询（query）和一组键-值对（key-value）映射到输出，其中query、key、value和输出都是向量。输出是对value的加权求和，分配给每个value的权重由query与相应key通过函数计算得来的。
Scaled Dot-Product Attention  我们将我们的注意力机制称为“Scaled Dot-Product Attention”（下图所示）。输入由query和key（维度为$d_{k}$），以及value（维度为$d_{v}$）组成。我们计算query和所有key的点乘，除以$\sqrt{d_{k}}$，并通过SoftMax获得value的权重。 实际上，我们将一组query、key和value整合成矩阵形式$Q$、$K$和$V$，同时计算一组query注意力分数，计算公式如下： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$ 两个最常用的注意力函数是 additive attention和dot-product attention。dot-product attention与我们的算法相同，除了比例因子$\frac{1}{\sqrt{d_{k}}}$。点积注意力更快，更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。
对于较小的$d_{k}$值，这两种注意力机制的表现相似，在$d_{k}$值较大的情况下， additive attention优于dot-product attention。我们猜测，对于大的$d_{k}$，其得到的点积结果也越来越大，使SoftMax进入一个具有极小梯度的区域，因此，为了避免上述问题，我们用比例因子$\frac{1}{\sqrt{d_{k}}}$来缩放点积。
Multi-Head Attention  我们发现，不使用$d_{model}$维的key、value和query来得到单一的注意力函数，而是对key、value和query进行$h$次的线性投影，分别使用不同的、学习到的$d_{k}$、$d_{v}$和$d_{q}$维的线性投影。在每个query、key和value的投影结果上，我们并行地执行注意力函数，生成$d_{v}$维输出，这些输出被连接起来并再次线性变换，从而得到最终的值，如下图所示： multi-head注意力机制允许模型共同关注来自不同位置的不同表示子空间的信息，公式如下： $$ MultiHead(Q,K,V)=Concat(h_{1},&amp;hellip;,h_{h})W^{O} $$ $$ where; h_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}) $$ 其中的投影是参数矩阵，$W_{i}^{Q}\in\mathbb{R}^{d_{model}\times d_{k}}$，$W_{i}^{K}\in\mathbb{R}^{d_{model}\times d_{k}}$，$W_{i}^{V}\in\mathbb{R}^{d_{model}\times d_{V}}$，$W^{O}\in\mathbb{R}^{hd_{v}\times d_{model}}$。</description>
    </item>
    
    <item>
      <title>ShuffleNet</title>
      <link>https://gdy0924.github.io/posts/shufflenet/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/shufflenet/</guid>
      <description>ShuffleNet 原文链接：ShuffleNet
Abstract  引入了一种计算效率非常高的CNN架构，名为ShuffleNet，它是专门为计算能力非常有限的移动设备而设计的。新的架构采用了两种新的操作，pointwise group convolution和channel shuffle，在保持精度的同时大大降低了计算成本。
Introduction  构建更深更大的卷积神经网络(CNNs)是解决主要视觉识别任务的主要趋势。最精确的CNN通常有数百层和数千个通道，因此需要的计算量非常大。但在非常有限的计算预算下，也需要追求最佳的准确性，如无人机，如无人机、机器人和智能手机等通用移动平台。但现有的许多工作都专注于剪枝、压缩或低位表示一个“基本的”网络架构。在本文，我们的目标是探索一个高效的基本结构。
性能好的网络如Xception和ResNeXt，在规模小的网络中效率较低，因为密集的1×1卷积很昂贵。我们提出使用pointwise group convolution来降低1×1卷积的计算复杂度，为了克服group convolution带来的副作用，提出一种新的操作：channel shuffle，可以使信息在通道中流通。基于上述两种技术，构建一个高效的结构：ShuffleNet。相比于其他架构，对于给定的计算复杂度，可以实现更多的特征图通道，以编码更多信息，这对于很小的网络性能是很重要的。
Related Work Efficient Model Designs  深度神经网络在计算机视觉任务中取得的成功，模型设计发挥了重要的作用。例如，与简单的叠加卷积层相比，GoogLeNet以更低的复杂度增加了网络的深度。SqueezeNet在保持精度的同时显著减少了参数和计算。ResNet利用高效的bottleneck结构来实现令人印象深刻的性能。SENet引入了一个架构单元，可以以轻微的计算成本提高性能。与此同时，最近的一项工作采用了强化学习和模型搜索来探索有效的模型设计，所提出的移动NASNet模型达到了与我们对应的ShuffleNet模型相当的性能，但是NASNet并没有报告在极小的模型上的结果，也没有评估在移动设备上的实际测试时间。
Group Convolution  Group Convolution的概念首次在AlexNet中引入，用于将模型分布在两个GPU上，在ResNeXt中很好地证明了其有效性。在Xception中提出的深度可分离卷积depthwise separable convolution。最近，MobileNet利用深度可分离的卷积，并在轻量级模型中获得了最先进的结果。我们的工作以一种新的形式推广了group convolution和depthwise separable convolution。
Channel Shuffle Operation  channel shuffle操作的想法在之前的高效模型设计中几乎没有被提到过，即使CNN库cuda-convnet支持随机稀疏卷积(random sparse convolution)层，相当于group convolution之后进行channel shuffle。这种random shuffle操作很少被使用。最近，另一个工作也采用了这个想法来进行两阶段卷积，但其并没有专门研究channel shuffle本身的有效性及其在小模型设计中的应用。
Model Acceleration  在加速推理的同时，保证预训练模型的准确性。
Approach Channel Shuffle for Group Convolutions  Xception和ResNeXt，在构建块中引入高效的深度可分离卷积或组卷积，以在表示能力和计算成本之间取得良好的权衡。然而，我们注意到，这两种设计都没有完全考虑到1×1卷积（也称为Pointwise Convolution），需要相当大的复杂性。在小型网络中，昂贵的Pointwise Convolution导致通道数量有限，可能会严重损害精度。
为了解决这个问题，一个简单的解决方法是使用通道稀疏连接，例如组卷积也同样应用在1×1卷积上。通过确保每个卷积只在相应的输入通道组上操作，组卷积大大降低了计算成本。然而，如果多个组卷积堆叠在一起，就有一个副作用：某个通道的输出只与输入通道的一小部分有关。如上图中的（a）显示了两个堆叠的组卷积。很明显，来自某一组的输出只与组内的输入有关，这阻止了通道组之间的信息交流。
如果我们允许组卷积获得来自不同组的输入数据，如上图中的（b）所示，那么输入通道和输出通道将完全相关。具体来说，对于上一组层生成的特征图，我们可以首先将每个组中的通道划分为几个子组，然后向下一层中的每个组提供不同的子组。这可以通过channel shuffle操作实现，如上图中的（c）：假设一个卷积层，有g个组，输出有g×n个通道，即每组的输出通道为n；我们首先将输出通道维度重塑为(g，n)，然后将其转平，作为下一层的输入，如下图所示： ShuffleNet Unit  利用channel shuffle操作，我们提出了ShuffleNet网络。从下图中的（a）模块出发，它是一个残差模块。其中3×3的卷积使用深度可分离卷积实现，将第一个1×1卷积替换成点组卷积紧跟着一个channel shuffle，形成一个ShuffleNet Unit，如下图的（b）所示，第二个点组卷积是为了恢复通道数目，以匹配shortcut连接。对于下采样模块，我们进行了两个修改，如下图（c）所示：（1）在shortcut通路上添加了一个平均池化层；（2）用通道连接替换元素相加，可以扩大通道，无需额外的计算成本。 由于点组卷积与channel shuffle，ShuffleNet Unit相同的设置下具有更小的复杂性，与ResNet和ResNeXt相比，例如：对于输入通道c，输出通道m，分组g的模块来说，ResNet需要$ 2cm+9m^{2}$，ResNeXt需要$ 2cm+\frac{9m^{2}}{g}$，而ShuffleNet 需要$\frac{2cm}{g}+9m$。即在相同的计算量下，ShuffleNet可以拥有更多的通道数。</description>
    </item>
    
    <item>
      <title>ResNet</title>
      <link>https://gdy0924.github.io/posts/resnet/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/resnet/</guid>
      <description>ResNet 原文连接：ResNet
Abstract  更深的网络往往更难被训练，因此提出残差学习模块，在简化网络训练的同时，加深网络的深度，可以达到152层。
Introduction  深度卷积神经网络为图像分类带来了一系列突破。深度神经网络以端到端、多层的方式集成了由低到高的特征表示，并且特征的“级别”可以通过堆叠层的数量（深度）来增加。有研究表明，网络深度是至关重要的。
深度增加的同时，会带来一个问题，即梯度消失/爆炸。然而，这个问题已经被归一化初始化和中间归一化层所解决，这使得具有几十层的网络开始收敛于反向传播的随机梯度下降(SGD)。
当更深层次的网络能够开始收敛时，出现网络退化的问题，即随着网络深度的增加，精度达到饱和，然后迅速下降。但这种网络退化不是由过拟合引起的，是由于在适当的深度模型中添加更多的层会导致更高的训练误差。
训练精度的下降表明，并不是所有的结构都同样容易进行优化。通过构造更深的模型，有一个解决方案：增加的层是恒等映射（ identity mappings），而其他层是从浅层复制过来的。这表明，一个更深的模型不会比它较浅的模型产生更高的训练误差。
在本文中，引入深度残差学习框架解决网络退化问题，我们不是希望每几个堆叠层直接匹配所需的映射$H(x)$，而是匹配$F(x)$的另一个映射，即残差映射$H(x)-x$，那么原始的映射就变成了$F(x)+x$。在极端情况下，如果一个恒等映射是最优的，那么将残差推到零要比用一堆非线性层来拟合一个恒等映射更容易。
公式$F(x)+x$可用Shortcut Connection实现，并且Shortcut Connection只需实现恒等映射，既不增加额外参数也不增加计算量。
我们在ImageNe上进行了全面的实验来证明退化问题，并评估我们的方法，得到：（1）深度残差网很容易优化，但对应的“普通”网络（简单的堆叠层）表现出更高的训练误差；（2）深度残差网可以很容易地从大大增加的深度中获得精度，产生的结果比以前的网络更好得多
Related Work Residual Representations  残差表示作为一种重构或预处理方法是有效的。首先，在图像识别任务中的VLAD （矢量量化），有研究表明编码残差矢量比编码原始矢量更有效；其次，在低级视觉和计算机图形学中，需要求解部分微分方程，也被证明这些求解器比没有考虑残差性质的标准求解器收敛得快得多。
Shortcut Connections  训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出。在一些研究中，某些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸（如GoogleNet）。
同时，Highway Networks（下图）实现了带有门控单元的shortcut connection，需要参数控制，而 ResNet没有，这样就不会增加额外参数。（Highway Networks相当于对输入一部分进行处理（和传统神经网络相同），一部分直接通过）另外Highway Networks并没有精度与网络深度的显著提高（没有超过100层）。 Deep Residual Learning Residual Learning  假设存在映射$H(x)$，我们不是期望堆叠的层映射近似$H(x)$，而是让这些层近似于一个残差函数$F(x):=H(x)-x$。原来的映射就变成了$F(x)+x$，虽然这两种形式都应该能够渐近地近似于所期望的函数，但从之前的研究可以看出，其学习的容易程度可能是不同的，残差收敛速度更快并且效果更好。（其主要思想是将堆叠的非线性层从，拟合原来的最优解映射输出$H(x)$，变成去，拟合输出和输入的差$F(x) = H(x) - x$，此时原最优解映射$H(x)$就可以改写成$F(x) + x$。而作者认为这两种表达的效果相同，但是优化的难度却并不相同，）
正如之前说的那样，如果增加的层可以构造成恒等映射，那么更深的模型的训练误差应该不大于对应较浅的模型。而通过多个非线性层逼近恒等映射是存在困难的，因此，通过残差学习重构，如果恒等映射是最优的，模型就可以简单地将多个非线性层的权值（即$F(x)$）推向零，以接近恒等映射。但在实际情况下，恒等映射不太可能是最优的。
Identity Mapping by Shortcuts  残差块的公式定义和结构图如下所示： $$ y=F(x,\lbrace W_{i} \rbrace)+x $$ 其中，$x$和$y$是输入与输出，函数$F(x,\lbrace W_{i} \rbrace)$是要学习的残差映射。对应图中的两层的例子。$x$和$F(X)$通过Shortcut Connection和元素级相加实现的。公式中的Shortcut Connection不会引入额外的参数也不引入计算复杂度。同时$x$和$F(X)$的维度必须相同，如果不相同，通过$W_{s}$实现维度匹配： $$ y=F(x,\lbrace W_{i} \rbrace)+W_{s}x $$ 残差函数$F$的形式是灵活的。本文实验涉及两层或三层的函数$F$，也可能有更多的层。但如果只有一层，类似于线性层：$y=W_{1}x+x$，，从实验可以看出没有效果。</description>
    </item>
    
    <item>
      <title>GoogleNet</title>
      <link>https://gdy0924.github.io/posts/googlenetpaper/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/googlenetpaper/</guid>
      <description>GoogLeNet获得了2014年ILSVRC比赛分类任务的冠军，其利用Inception模块，在加深网络深度的同时，减少了参数量，从而减少计算资源的利用。
GoogLeNetV1 原文链接：GoogleNetV1
Abstract  提出一种名为“Inception”的深度卷积神经网络架构，可以增加网络的深度和宽度，同时保持计算量不变，从而提高网络精度。
Introduction  随着深度学习的发展，卷积网络在图像领域发展迅速，算法的效率，特别是计算量和内存的使用，变得越来越重要，在该网络中，我们也考虑到了这一方面。我们提出了一种更加高效的深度神经网络架构，名为“Inception module”，直接增加了网络的深度。
Motivation and High Level Considerations  提高深度神经网络性能的最直接的方法是增加网络的规模，包括：网络的深度（层数）和宽度（每一层的单元数量）。但存在两个问题：
（1）更大的尺寸通常意味着大量的参数，这使得扩大后的网络更容易发生过拟合，特别是在训练集有限的情况下；
（2）网络的扩大伴随着计算资源的使用显著增加，如果增加的部分使用效率较低（例如，如果大多数权重最终接近于零），那么就会浪费大量的计算。
解决上述问题的根本方法是：将完全连接转换成稀疏连接，一方面模拟了生物系统，另一方面，有研究证明了稀疏网络的有效性。
之前的研究为了打破网络对称性和提高学习能力，都使用了随机稀疏连接，但是，计算机软硬件对非均匀稀疏数据的计算效率很差，所以在AlexNet中又重新使用了全连接层，目的是为了更好地优化并行运算。
因此，需要一种网络结构，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能，提出“Inception”结构来实现此目的。
Architectural Details 上图是提出的最原始的基本结构：
（1）采用不同大小的卷积核实现不同大小的感受野，在模块最后进行拼接实现不同尺度特征的融合；
（2）卷积核大小采用1、3和5，是为了方便对齐，即在设定卷积步长stride=1后，只要分别设定padding=0、1、2，不同卷积核操作之后得到的特征图大小就相同，方面最后的拼接操作；
（3）由于pooling操作被证明很有效，因此引入了池化分支；
（4）网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例会增加。
上述模块存在一个大问题，在该结构中，即使具有大量卷积核的卷积层中，少量的5×5卷积也会带来巨大的计算量，并且由于卷积结果与池化结果进行合并，因此从上一个模块到下一个模块，输出通道肯定要增加。
因此提出了改进的模块，在计算量大的地方引入降维操作，即在3×3和5×5卷积操作之前，先利用1×1卷积进行通道数的减少，从而减少网络参数，在减少计算量的同时，做一定的正则化，同时还引入了更多的非线性变换。改进的模块如下所示： 该结构的好处是：
（1）允许在每个阶段显著增加单元的数量，而不会导致计算复杂性的不受控制的爆炸；
（2）与直觉相一致，即视觉信息应该在不同的尺度上进行处理，然后进行聚合，以便下一阶段可以同时从不同的尺度上提取特征。
GoogLeNet 上图为完整的GoogLeNet网络结构，由于技术的原因，先进行传统的卷积层，只在较高的层开始使用Inception模块，其特点如下：
（1）该网络有22层深（如果加上池化层，是27层）；
（2）实验发现，用平均池化代替全连接层能够提高精度，因此使用了average pooling，同时使用dropout，但是依旧在网络最后添加了一个额外的全连接层，主要是为了更加方便地调整和微调网络；
（3）由于网络的深度相对较大，因此需要解决梯度回传的问题，在网络中间的层，额外添加了两个辅助的softmax用于向前传导梯度，在训练过程中，它们的损失以折扣权值加到网络的总损失中（辅助分类器的损失加权为0.3），在测试时这两个额外的softmax会被去掉，辅助分类器的结构如下：
（a）5×5大小的平均池化，stride=3；
（b）1×1×128卷积操作，用于降维和非线性激活； （c）1024个神经元的全连接层； （d）p=70%的Dropout； （e）以Softmax损失作为分类器的线性层，分类数为1000。
Conclusions  通过构建密集的块结构来近似最优的稀疏结构，从而达到提高性能而又不大量增加计算量的目的，是一个可行的方法，通过实验也证明了Inception模块的优势。
GoogleNetV2 原文链接：GoogleNetV2
Inception V2学习了VGGNet，用两个3×3的卷积代替5×5的大卷积（用以降低参数量并减轻过拟合），还提出了著名的BatchNormalization方法。BN是一个非常有效的正则化方法，可以让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅提高（详见BatchNormalization）。
GoogleNetV3 原文链接：GoogleNetV2
General Design Principles  提出一些基于使用卷积网络的各种架构选择的设计原则，包含一定的推测，不过经过实验证明基本是有效的：
（1）avoid representational bottlenecks 避免表征瓶颈：（特别是在网络浅层）信息流前向传播过程中显然不能经过高度压缩的层，即表征瓶颈，从输入到输出，特征图的宽和高基本都会逐渐变小，但是不能一下子就变得很小；
（2）higher dimensional representations 高维特征： 高维特征更容易区分和处理，从而会加快训练；
（3）spatial aggregation 空间聚合： 空间聚合可以在低维嵌入上进行，而不会造成太多或任何表征能力的损失，同时还可以提高速度，比如在进行3x3卷积之前，可以对输入先进行降维而不会产生严重的后果；</description>
    </item>
    
    <item>
      <title>Open World Object Detector</title>
      <link>https://gdy0924.github.io/posts/open-world-object-detector/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/open-world-object-detector/</guid>
      <description>原文链接：ORE
Abstract  开放世界对象检测（Open World Object Detector）模型的任务是：识别未知的（unknown）对象， 即没有明确监督标签；当相应的标签逐步被接收到时，可学习到未知对象的类别，并且不忘记之前学习到的标签。以上是该论文提出的问题，并引入了一个新的模型：ORE（Open World Object Detector），该模型基于对比聚类和基于能量的位置对象识别。
Introduction  深度学习中目标检测任务是识别和定位图像中的物体。现有的研究都有一个强有力的假设：要检测的所有类别都在训练阶段给出。当上述假设不存在，那么就有两个具有挑战性的问题：测试图像可能包含未知类别，这些对象应被归为未知类（unknown）；当关于未知类别的对象信息可用时，模型应该逐步学习这些新的类别。该论文提出了一个新的问题：一个模型应该能够将未知对象识别为未知类，在训练数据阶段以统一的方式逐步学习和识别他们。
contributions 1、引入一个新的问题Open World Object Detector；
2、提出一种新方法ORE，基于对比聚类，一个未知提议网络和基于能量的未知类识别；
3、提出一个新的全面的实验设置；
4、所提出的方法在增量目标检测上取得了最优异的成绩。
Related Work Open Set Classification  虽然这种模型可以识别出未知的实例，但它们不能在多个训练过程中以增量的方式动态地更新自己，即无法学习未知类别。
Open World Classification  这种模型可以识别已知类和未知类的对象，并在提供未知的新标签时自适应地改进自己，但是他们没有在图像分类基准上进行测试。
Open Set Detection  一些研究发现传统物体检测模型经常将未知类别的物体识别成已知的某一类别，因此其处理方式通常为：1. 新增一个background类别 2. 去除未知类别的物体。但是上述两个方法都不能在真实动态环境中使用。
Open World Object Detection  在任意时刻$t$，我们已知类别表示为$K^{t}= \lbrace 1,2,&amp;hellip;,C \rbrace \subset N^{+}$（共$C$个类别），其中$N^{+}$代表正整数的集合，同时假设存在一组未知类别$U=\lbrace C+1,&amp;hellip; \rbrace$，在推理过程中可能会遇到。已知类别$K_{t}$在数据集$D^{t}=\lbrace X^{t},Y^{t} \rbrace$中被标记，其中$X$、$Y$分别表示输入图像和标签。输入图像集由$M$张训练图像组成，$X^{t}=\lbrace I_{1},&amp;hellip;,I_{M} \rbrace$以及每张图像对应的标签集$Y^{t}=\lbrace Y_{1},&amp;hellip;,Y_{M} \rbrace$。每个$Y_{i}=\lbrace y_{1},y_{2},&amp;hellip;,y_{K} \rbrace$代表$K$个对象实例的类别标签和定位（即一张图像上有$K$个被标记的对象实例），并且$y_{k}=\left [ l_{k},x_{k},y_{k},w_{k},h_{k} \right ]$，其中$l_{k}$代表类别，$ x_{k}$，$y_{k}$、$w_{k}$、$h_{k}$分别代表 bounding box的中心坐标、宽和高。</description>
    </item>
    
    <item>
      <title>VGG</title>
      <link>https://gdy0924.github.io/posts/vggpaper/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/vggpaper/</guid>
      <description>原文链接：VGG
VGG网络获得了2014年ILSVRC比赛的分类任务的亚军和定位任务的冠军，其主要贡献是通过更小的卷积核堆叠构建更深层的网络。 Abstract  该论文研究了卷积网络深度对精度的影响，通过使用小的卷积核（大小为3×3）加深网络的深度，将深度增加到了16-19层，并取得了不错的效果。
Introduction  卷积神经网络在大规模图像核视频识别方面取得了巨大成功。在该论文中，我们关注卷积网络的另一个方面——网络的深度。通过添加更多的卷积层来增加网络的深度，并且使用的卷积核大小很小。
ConvNet Configurations Architecture  输入为224×224大小的RGB图像，每层都使用3×3大小的卷积核提取特征，固定步长为1，填充为1，以保证卷积前后的特征图大小相同。最大池化的大小为2×2，步长也为2，即特征图缩小一半，并且不是所有的卷积层后边都跟池化层。
卷积层后跟着三个全连接层，前两个有4096个通道，最后一个有1000个通道（因为比赛中的类别数是1000），最后一层是Softmax层。
每一层后都跟着非线性激活层，并且我们没有用到（除了一个架构）AlexNet所提出的LRN层，通过实验发现LRN层并没有提高ILSVRC数据集上的性能，但却导致了内存消耗和计算时间的增加。
Configurations 从图中可以看出，不同网络的架构是相同的，除了深度不同，从11层的网络（8层卷积和3个FC层）到19层网络（16层卷积和3个FC层）。通道数由最开始的64，每次操作都使通道数增加一倍，直到512层。由于卷积核小，因此虽然网络很深，但其网络参数没有很多。
Discussion  不同于之前的网络，我们没有在网络第一层使用较大的感受野，如11×11或7×7，而是利用小的感受野（大小为3×3）贯穿整个网络结构。可以看出，两个3×3大小的感受野堆叠起来的感受野与一个5×5大小的感受野相同，三个3×3大小的感受野堆叠起来的感受野与一个7×7大小的感受野相同。使用三个3×3大小的感受野代替一个7×7大小的感受野的好处有：（1）三层感受野就引入了三次非线性变换，因此增加了网络的非线性表达能力；（2）三个3×3大小的感受野共有27个参数，而一个7×7大小的感受野有49个参数，因此可以减少网络的参数数量，可以看作对网络做了相应的正则化。
在16层的网络中，我们使用得到了1×1的卷积核，在不改变感受野的前提下增加了非线性，同时输入通道核输出通道是相同的。
GoogleNet与我们的相似之处就是它是基于较深的网络（22层）和小卷积核（除了3×3，他们还用到了1×1和5×5）。他们的网络结构比我们更加复杂 ，并且在第一层就将图像分辨率降低了很多，以减少计算量。
Classification Framework Training  使用具有动量的小批量梯度下降算法，Dropout，对输入图像进行随机剪裁、随机水平翻转和随机RGB颜色位移等操作。
考虑了两种训练图像尺寸S的方法：
（1）单尺寸训练：在实验中，使用两种尺寸大小验证模型，S=256和S=384。给定一个卷积网络，我们首先使用S=256训练网络。为了加快S=384网络的训练，我们使用S=256预训练的权值初始化；
（2）多尺寸训练：每个训练图像在一定范围内$\left [ S_{min},S_{max} \right ]$随机采样S，其中，$S_{min}=256,S_{max}=512$。我们通过微调单尺度模型来训练多尺度模型，并使用固定的S=384进行预训练。
Testing  在测试阶段，首先将输入图像缩放至最小的尺寸$Q$，同时$Q$并不一定等于训练中的$S$，在训练阶段，每个$S$对应的网络，使用多个不同的$Q$对其进行测试。接着将网络最后三层的全连接层改为卷积层，第一个FC层改为7×7×4096的卷积层，第二个改为1×1×4096，第三个改为1×1×1000，可以看出，最终的通道数对应的是分类数。通过改为卷积层，可以接受输入大小不同的图片，只是最终得到的特征图的大小不同，但其通道数依旧对应分类数，可以通过对最终的特征图进行空间平均（求和，sum-pooled）得到Softmax输入之前的分数。我们还通过图像的水平翻转来增加测试集，将原始图像和翻转图像的Softmax分数进行平均，以获得图像的最终分数。 Conclusion  提出了比较深的卷积网络（19个权重层）用于大尺度的图像分类，实验结果表明，更深层的网络有利于提高分类精度。</description>
    </item>
    
    <item>
      <title>Relevance-CAM</title>
      <link>https://gdy0924.github.io/posts/relevance-campaper/</link>
      <pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/relevance-campaper/</guid>
      <description>原文链接：Relevance-CAM
Abstract  目前在深度学习的可解释性领域，一种主要的常见的是基于类激活映射(Class Activation Mapping, CAM)的方法，常用在卷积网络的最后一层。在本文中，提出了一种相关加权类激活映射(Relevance-weighted Class Activation Mapping, Relevance-CAM)，以解决基于梯度的CAM在梯度消失方面的问题，并且该方法在网络的任意一层都可以应用。
Introduction  随着深度神经网络的不断发展，可解释网络也被广泛研究，其方法主要分为基于类激活映射(CAM)的和基于分解的。基于CAM的方法通过全局平均池化得到每个通道的权重，接着利用最后一层卷积的特征图进行加权线性求和操作得到可视化的结果。但是该方法只能用在最后一层，且必须用到全局平均池化和全连接层。基于梯度的CAM，Grad-CAM和 Grad-CAM++ 对其进行改进，使用梯度作为权重。层相关性传播(Layer-wise Relevance Propagation, LRP)是基于分解的方法。LRP通过特定的传播规则将最终的输出分数向后传播，重新分配到输入图像中，从而解决了梯度消失的问题。 Contributions 1、提出了Relevance-CAM，解决了梯度消失的问题，同时在中间层也可以工作； 2、通过可视化热图，展示了我们所提出的方法优于其他方法； 3、通过Average Drop、Average Increase和Intersection over Union三种客观数值证明我们所提出的方法优于其他方法； 4、我们通过实验发现，即使在浅层，网络也可以提取到类特定信息。
Background CAM 原文链接：CAM 类激活映射(CAM)通过全局平均池化层获得权重，接着将最后一个卷积层的特征图进行线性加权求和，如下图所示。但是该方法有很大的局限性，只能用于最后一层的可视化，并且要在原网络的基础上增加全局平均池化层。 Grad-CAM 原文链接：Grad-CAM Grad-CAM改变了权重计算的方法，利用最后输出分数对某一层特征图的梯度作为权重，以解决CAM所存在的问题。 其计算公式如下：
$$ L_{Grad-CAM}^{c}=\sum_k\alpha _k^{c}A_k $$
$$ \alpha _k^{c}=GP\left ( \frac{\partial y^{c}}{\partial A_k} \right ) $$
其中，$ A_k $是第$ k $个通道的特征图，$ y^{c} $是类别$ c $对应的得分，$\alpha _{k}^{c}$ 代表权重，GP()表示全局平均函数。
Layer-wise Relevance Propagation(LRP) 原文链接：LRP LRP通过分层地将相关性分数从输出传播到输入图像，从而获得原始图像中每个像素的相关性分数，并且传播过程满足守恒且分数恒不小于零，如下所示： $$ \forall x:f_{c}\left ( x \right )=\sum_{p}R_{p}^{l}\left ( x \right ) $$</description>
    </item>
    
    <item>
      <title>AlexNet</title>
      <link>https://gdy0924.github.io/posts/alexnetpaper/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/alexnetpaper/</guid>
      <description>AlexNet在在2012年的ImageNet竞赛中取得了冠军，作为第一个深度卷积网络在该比赛中获得如此好的成绩，AlexNet证实了深度卷积网络的潜力，并引起了研究者们的极大热情。 AlexNet共包含8层，其中前5层为卷积层，后三层为全连接层，最后一个全连接层的输出是1000维，输入softmax产生最终的输出：1000类的标签分布。
原文链接：AlexNet
网络架构 输入层 输入图片的大小为224×224，并包含R、G、B三个通道。对于原始的数据，AlexNet进行了数据增强操作。
Layer1 输入图片：224×224×3（227×227×3） 卷积核：11×11×96 步长(stride)：4 填充(padding)：1 输出大小：54×54×96（55×55×96） 池化：size=3×3，stride =2, padding=0 第一个卷积层使用96个11×11的卷积核对图片进行特征提取，并且经过ReLU、LRN层和最大池化层得到第一层的输出，大小为27×27×96。
Layer2 输入图片：27×27×96 卷积核：5×5×256 步长(stride)：1 填充(padding)：2 输出大小：27×27×256 池化：size=3×3，stride =2, padding=0 第二个卷积层使用256个5×5的卷积核对图片进行特征提取，并且经过ReLU、LRN层和最大池化层得到第二层的输出，大小为13×13×256。
Layer3 输入图片：13×13×256 卷积核：3×3×384 步长(stride)：1 填充(padding)：1 输出大小：13×13×384 第三个卷积层使用384个3×3的卷积核对图片进行特征提取，并且经过ReLU非线性激活得到第三层的输出，大小为13×13×384。
Layer4 输入图片：13×13×384 卷积核：3×3×384 步长(stride)：1 填充(padding)：1 输出大小：13×13×384 第四个卷积层使用384个3×3的卷积核对图片进行特征提取，并且经过ReLU非线性激活得到第四层的输出，大小为13×13×384。
Layer5 输入图片：13×13×384 卷积核：3×3×256 步长(stride)：1 填充(padding)：1 输出大小：13×13×256 池化：size=3×3，stride =2, padding=0 第五个卷积层使用256个3×3的卷积核对图片进行特征提取，并且经过ReLU非线性激活和池化层得到第五层的输出，大小为6×6×384。
全连接层 第6、7、8层都为全连接层，并且每层的神经元个数都为4096个，最后经过softmax得到最终1000个类别的分类结果。
Innovation创新点 ReLU非线性激活 AlexNet是第一个使用ReLU函数作为激活函数的网络，之前使用最多的激活函数是Sigmiod函数，函数图像如下所示。可以看出，Sigmiod函数在输入x的值很大或者很小的时候，其梯度非常小，几乎接近于0，那么在反向传播过程中，由于梯度的链式法则，就会导致网络的浅层得到的梯度为0，无法正常更新权重，因此AlexNet就提出可以使用ReLU函数来解决梯度消失的问题。ReLU函数在输入x大于0时，其梯度一直为1，解决了梯度消失问题，并且在输入x小于0时，输出为0，就使得网络更加稀疏，从而减少了参数的相互依存关系，缓解了过拟合问题。 多GPU 由于当时GPU内存的限制，AlexNet将网络放在2两GPU上进行训练，从网络架构图中可以看出，每一层都是将通道数一份为2，分别放在不同的GPU上，并且规定GPU只能在特定的层进行通信交流。
LRN 虽然使用ReLU函数不需要再进行标准化，不过实验表明局部响应标准化(Local Response Normalization)有助于泛化。其公式如下： $$ b_{x,y}^{i}=a_{x,y}^{i}/\left ( k+\alpha \sum_{j=max\left ( 0,i-n/2 \right )}^{min\left ( N-1,i+n/2 \right )}\left ( a_{x,y}^{j} \right )^{2}\right )^{\beta } $$ 其中, $ a_{x,y}^{i} $表示经过激活函数ReLU得到的特征图对应位置为(x,y)的输出值，$ b_{x,y}^{i} $ 表示经过LRN后的输出值，$ N $ 为卷积核的个数，$k$、$n$、$\alpha$、$\beta$为超参数，在该论文中设置的分别为：$k=2$，$n=5$，$\alpha=10^{-4}$，$\beta=0.</description>
    </item>
    
    <item>
      <title>LeNet</title>
      <link>https://gdy0924.github.io/posts/lenetpapar/</link>
      <pubDate>Sun, 26 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/lenetpapar/</guid>
      <description>LeNet是很简单的一个经典卷积神经网络，主要用于手写数字识别，所以是一个多分类任务，并且是十个类别。该网络架构如下图所示，每一层分别是卷积层、池化层、卷积层、池化层、卷积层和全连接层，最后连接Softmax实现分类。
原文链接：LeNet
网络架构 输入层 将输入图像的尺寸统一归一化为32×32×1，其中第一个32代表图片的高度，第二个32代表图片的宽度，1是指通道数，由于对应的数据集是黑白的，所以其通道数为1，对于彩色的图片，通道数为3，分别对应R、G、B。
第一层：卷积层 输入图片：32×32×1 卷积核：5×5×6 步长：1 输出大小：28×28×6 神经元数量：28×28×6 参数个数：(5×5+1)×6 第一层为卷积层，使用6个大小为5×5的卷积核，提取图片的feature map，得到6个大小为28×28的feature map。
第二层：池化层 输入：28×28×6 核大小：2×2×6 步长：2 输出大小：14×14×6 神经元数量：14×14×6 参数个数：2×6 通过池化层对图像进行下采样，在该层采用的是最大池化，即选择区域中的最大值作为采样的值，除了最大池化外，还有平均池化等。
第三层：卷积层 输入图片：14×14×6 卷积核：5×5×16 步长：1 输出大小：10×10×16 参数：6×(3×5×5+1)+6×(4×5×5+1)+3×(4×5×5+1)+1×(6××5+1) 该层使用16个大小为5×5的卷积核，对大小为14×14、通道数为6的feature map进行卷积，，最终得到16个大小为10×10的feature map。 对于该层的16个卷积核，其中前六个与上一层的相连三个feature map相对应，接着六个卷积核与上一层的相连四个feature map相对应，接下来的三个与上一层的部分不相连的四个feature map相对应，最后一个卷积核与上一层得到的所有feature map对应。
第四层：池化层 输入：10×10×16 核大小：2×2×16 步长：2 输出大小：5×5×16 神经元数量：5×5×16 参数：2×16 对16个10×10大小的feature map进行最大池化，得到16个大小为5×5的feature map。
第五层：卷积层 输入图片：5×5×16 卷积核：5×5×120 步长：1 输出大小：1×1×120 神经元数量：28×28×6 参数：(16×5×5+1)×120 该层使用120个大小为5×5的卷积核，对图片进行卷积操作，得到120个大小为1×1的feature map。
第六层：全连接层 输入大小：120 输出大小：84 该层为全连接层，共有84个神经元。
输出层 输入大小：84 输出大小：10 该层为全连接层，包含10个神经元，对应最后的十个分类情况。
特点 在S2与C3之间，输入的feature map和输出的feature map之间并不是全连接的，而是局部连接的，如图所示。其中行对应的是C3的feature map，列对应的是S2的feature map。以第0列为例，C3的第一个feature map是由S2的前三个feature map经过卷积核操作得到的，而C3的第七个feature map，也就是第6列，是由S2的前四个feature map经过卷积核操作得到的。 </description>
    </item>
    
  </channel>
</rss>
