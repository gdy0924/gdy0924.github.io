<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.89.4" />
  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/Googlecode.min.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Gao dy" />
  <meta property="og:url" content="https://gdy0924.github.io/posts/reformer/" />
  <link rel="canonical" href="https://gdy0924.github.io/posts/reformer/" /><link rel="alternate" type="application/atom+xml" href="https://gdy0924.github.io/index.xml" title="XY&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gdy0924.github.io\/"
      },
      "articleSection" : "posts",
      "name" : "Reformer",
      "headline" : "Reformer",
      "description" : "REFORMER: THE EFFICIENT TRANSFORMER\n原文链接：DeiT\n上表为内存使用量和时间复杂度的比较，其中$l$为序列的长度，$b$为batch size的大小，$n_{h}$为头数，$n_{c}$为LSH的chunk数量，$n_{r}$为hash重复的次数。\nAbstract  Transformer在许多任务上都实现了最先进的结果，但是其训练代价是很昂贵的，特别是在长序列上。因此，我们提出两种提高transformer的技术：（1）局部敏感哈希（locality-sensitive hashing），将复杂度从$O(L^2)$降低到$O(L log L)$；（2）使用可逆残差层而不是标准残差，使得在训练过程中只存储最后一层的激活值（激活函数的输入值），而不是N次，N为层数。\nIntroduction  基于transformer的大型模型甚至不能在单个GPU上进行微调，因为它们的内存需求需要多加速器硬件设置。transformer架构中的主要内存使用在以下几个方面：\n（1）一个包含N层的模型，其内存比单层模型使用的内存大N倍，因为需要存储激活值以进行反向传播，即每一层的激活值都需要存储下来；\n（2）transformer block中的全连接层FFN中的深度往往比$d_{model}$大得多，如：FFN的输入维度为512，那么中间的维度可能是2048，这也占了内存的很大部分；\n（3）注意力机制在计算序列长度为$L$的序列时，在计算和内存的复杂度上都是$O(L^2)$。\n我们提出以下方法来解决上述问题：\n（1）可逆层：使得整个网络只用存储最后一层的激活值，而不是所有层，之前层的激活值可以通过最后一层推算出来；\n（2）在FFN层中分割激活值，将他们以快的形式进行处理；\n（3）基于局部敏感哈希的近似注意力，将复杂度从$O(L^2)$降低到$O(L log L)$。\nLOCALITY-SENSITIVE HASHING ATTENTION Dot-product attention  transformer中使用点乘计算注意力，输入是维度为$d_{k}$的query和key，以及维度为$d_{v}$的value，计算公式如下： $$ Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V $$\nMulti-head attention  使用可学习的矩阵$W_{Q}$、$W_{K}$和$W_{V}$来处理query、key和value。\nMemory-efficient attention  假设Q、K、V的大小都是$L×d_{model}$，那么$QK^{T}$的计算结果的大小就是$L×L$的，当序列长度很长时，是需要一定的内存去存储的，这就限制了transformer的使用。\nWhere do Q, K, V come from?  Q、K、V是将输入经过三个不同的线性层来得到的。对于LSH注意力模型，我们希望query和keys（Q和K）是相同的，这可以通过一个使用同一个线性层来实现，这种方式称为shared-QK Transformer，并经过实验验证，这种方式并不会影响性能。\nHashing attention  对于LSH注意力模型，从Q=K和V开始，大小为$L×d_{model}$，并根据之前的公式，主要问题在于$QK^{T}$部分的计算。但是，我们最终是只关心softmax的结果，并且softmax是由最大元素来主导的。也就是说，对于每个query$q_{i}$，我们只需要关注在K中最接近$q_{i}$的那些key。比如：假设长度为64K，对于每个$q_{i}$，我们只考虑一个小子集，32或64个最接近的key的集合，这样就可以减少计算量，那么如何在K中找到“最近的邻居”呢？\nLocality sensitive hashing  在高维空间中快速寻找最近邻的问题可以通过局部敏感哈希(LSH)来解决：使用一种哈希函数$H(x)$，使得两个个相似度很高的数据以较高的概率映射成同一个hash值，而令两个相似度很低的数据以极低的概率映射成同一个hash值，该函数称为局部敏感哈希。\n我们通过随即投影来实现，如下图。为了得到，我们先创建一个随机矩阵，大小为$[d_{k},b\/2]$，然后定义$h(x)=argmax([xR;-xR])$，其中$[u;v]$表示两个向量的连接。 在上图中，为角度局部敏感哈希，使用球投影点的随即旋转角度，在上图的二维描述中，上边一行的两个点x和y不太可能在一个中hash buckets，而在下边一行中的x和y的球投影是彼此接近的。\nLSH attention 上图为LSH attention中：hash-bucketing、排序、chunking分块和计算attention结果的简化描述；下图中的(a)-(d)是步骤中的矩阵形式：",
      "inLanguage" : "en-US",
      "author" : "Gao dy",
      "creator" : "Gao dy",
      "publisher": "Gao dy",
      "accountablePerson" : "Gao dy",
      "copyrightHolder" : "Gao dy",
      "copyrightYear" : "2022",
      "datePublished": "2022-05-18 00:00:00 \u002b0000 UTC",
      "dateModified" : "2022-05-18 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gdy0924.github.io\/posts\/reformer\/",
      "keywords" : [  ]
  }
</script>
<title>Reformer</title>
  <meta property="og:title" content="Reformer" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="REFORMER: THE EFFICIENT TRANSFORMER
原文链接：DeiT
上表为内存使用量和时间复杂度的比较，其中$l$为序列的长度，$b$为batch size的大小，$n_{h}$为头数，$n_{c}$为LSH的chunk数量，$n_{r}$为hash重复的次数。
Abstract  Transformer在许多任务上都实现了最先进的结果，但是其训练代价是很昂贵的，特别是在长序列上。因此，我们提出两种提高transformer的技术：（1）局部敏感哈希（locality-sensitive hashing），将复杂度从$O(L^2)$降低到$O(L log L)$；（2）使用可逆残差层而不是标准残差，使得在训练过程中只存储最后一层的激活值（激活函数的输入值），而不是N次，N为层数。
Introduction  基于transformer的大型模型甚至不能在单个GPU上进行微调，因为它们的内存需求需要多加速器硬件设置。transformer架构中的主要内存使用在以下几个方面：
（1）一个包含N层的模型，其内存比单层模型使用的内存大N倍，因为需要存储激活值以进行反向传播，即每一层的激活值都需要存储下来；
（2）transformer block中的全连接层FFN中的深度往往比$d_{model}$大得多，如：FFN的输入维度为512，那么中间的维度可能是2048，这也占了内存的很大部分；
（3）注意力机制在计算序列长度为$L$的序列时，在计算和内存的复杂度上都是$O(L^2)$。
我们提出以下方法来解决上述问题：
（1）可逆层：使得整个网络只用存储最后一层的激活值，而不是所有层，之前层的激活值可以通过最后一层推算出来；
（2）在FFN层中分割激活值，将他们以快的形式进行处理；
（3）基于局部敏感哈希的近似注意力，将复杂度从$O(L^2)$降低到$O(L log L)$。
LOCALITY-SENSITIVE HASHING ATTENTION Dot-product attention  transformer中使用点乘计算注意力，输入是维度为$d_{k}$的query和key，以及维度为$d_{v}$的value，计算公式如下： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$
Multi-head attention  使用可学习的矩阵$W_{Q}$、$W_{K}$和$W_{V}$来处理query、key和value。
Memory-efficient attention  假设Q、K、V的大小都是$L×d_{model}$，那么$QK^{T}$的计算结果的大小就是$L×L$的，当序列长度很长时，是需要一定的内存去存储的，这就限制了transformer的使用。
Where do Q, K, V come from?  Q、K、V是将输入经过三个不同的线性层来得到的。对于LSH注意力模型，我们希望query和keys（Q和K）是相同的，这可以通过一个使用同一个线性层来实现，这种方式称为shared-QK Transformer，并经过实验验证，这种方式并不会影响性能。
Hashing attention  对于LSH注意力模型，从Q=K和V开始，大小为$L×d_{model}$，并根据之前的公式，主要问题在于$QK^{T}$部分的计算。但是，我们最终是只关心softmax的结果，并且softmax是由最大元素来主导的。也就是说，对于每个query$q_{i}$，我们只需要关注在K中最接近$q_{i}$的那些key。比如：假设长度为64K，对于每个$q_{i}$，我们只考虑一个小子集，32或64个最接近的key的集合，这样就可以减少计算量，那么如何在K中找到“最近的邻居”呢？
Locality sensitive hashing  在高维空间中快速寻找最近邻的问题可以通过局部敏感哈希(LSH)来解决：使用一种哈希函数$H(x)$，使得两个个相似度很高的数据以较高的概率映射成同一个hash值，而令两个相似度很低的数据以极低的概率映射成同一个hash值，该函数称为局部敏感哈希。
我们通过随即投影来实现，如下图。为了得到，我们先创建一个随机矩阵，大小为$[d_{k},b/2]$，然后定义$h(x)=argmax([xR;-xR])$，其中$[u;v]$表示两个向量的连接。 在上图中，为角度局部敏感哈希，使用球投影点的随即旋转角度，在上图的二维描述中，上边一行的两个点x和y不太可能在一个中hash buckets，而在下边一行中的x和y的球投影是彼此接近的。
LSH attention 上图为LSH attention中：hash-bucketing、排序、chunking分块和计算attention结果的简化描述；下图中的(a)-(d)是步骤中的矩阵形式：" />
  <meta name="description" content="REFORMER: THE EFFICIENT TRANSFORMER
原文链接：DeiT
上表为内存使用量和时间复杂度的比较，其中$l$为序列的长度，$b$为batch size的大小，$n_{h}$为头数，$n_{c}$为LSH的chunk数量，$n_{r}$为hash重复的次数。
Abstract  Transformer在许多任务上都实现了最先进的结果，但是其训练代价是很昂贵的，特别是在长序列上。因此，我们提出两种提高transformer的技术：（1）局部敏感哈希（locality-sensitive hashing），将复杂度从$O(L^2)$降低到$O(L log L)$；（2）使用可逆残差层而不是标准残差，使得在训练过程中只存储最后一层的激活值（激活函数的输入值），而不是N次，N为层数。
Introduction  基于transformer的大型模型甚至不能在单个GPU上进行微调，因为它们的内存需求需要多加速器硬件设置。transformer架构中的主要内存使用在以下几个方面：
（1）一个包含N层的模型，其内存比单层模型使用的内存大N倍，因为需要存储激活值以进行反向传播，即每一层的激活值都需要存储下来；
（2）transformer block中的全连接层FFN中的深度往往比$d_{model}$大得多，如：FFN的输入维度为512，那么中间的维度可能是2048，这也占了内存的很大部分；
（3）注意力机制在计算序列长度为$L$的序列时，在计算和内存的复杂度上都是$O(L^2)$。
我们提出以下方法来解决上述问题：
（1）可逆层：使得整个网络只用存储最后一层的激活值，而不是所有层，之前层的激活值可以通过最后一层推算出来；
（2）在FFN层中分割激活值，将他们以快的形式进行处理；
（3）基于局部敏感哈希的近似注意力，将复杂度从$O(L^2)$降低到$O(L log L)$。
LOCALITY-SENSITIVE HASHING ATTENTION Dot-product attention  transformer中使用点乘计算注意力，输入是维度为$d_{k}$的query和key，以及维度为$d_{v}$的value，计算公式如下： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$
Multi-head attention  使用可学习的矩阵$W_{Q}$、$W_{K}$和$W_{V}$来处理query、key和value。
Memory-efficient attention  假设Q、K、V的大小都是$L×d_{model}$，那么$QK^{T}$的计算结果的大小就是$L×L$的，当序列长度很长时，是需要一定的内存去存储的，这就限制了transformer的使用。
Where do Q, K, V come from?  Q、K、V是将输入经过三个不同的线性层来得到的。对于LSH注意力模型，我们希望query和keys（Q和K）是相同的，这可以通过一个使用同一个线性层来实现，这种方式称为shared-QK Transformer，并经过实验验证，这种方式并不会影响性能。
Hashing attention  对于LSH注意力模型，从Q=K和V开始，大小为$L×d_{model}$，并根据之前的公式，主要问题在于$QK^{T}$部分的计算。但是，我们最终是只关心softmax的结果，并且softmax是由最大元素来主导的。也就是说，对于每个query$q_{i}$，我们只需要关注在K中最接近$q_{i}$的那些key。比如：假设长度为64K，对于每个$q_{i}$，我们只考虑一个小子集，32或64个最接近的key的集合，这样就可以减少计算量，那么如何在K中找到“最近的邻居”呢？
Locality sensitive hashing  在高维空间中快速寻找最近邻的问题可以通过局部敏感哈希(LSH)来解决：使用一种哈希函数$H(x)$，使得两个个相似度很高的数据以较高的概率映射成同一个hash值，而令两个相似度很低的数据以极低的概率映射成同一个hash值，该函数称为局部敏感哈希。
我们通过随即投影来实现，如下图。为了得到，我们先创建一个随机矩阵，大小为$[d_{k},b/2]$，然后定义$h(x)=argmax([xR;-xR])$，其中$[u;v]$表示两个向量的连接。 在上图中，为角度局部敏感哈希，使用球投影点的随即旋转角度，在上图的二维描述中，上边一行的两个点x和y不太可能在一个中hash buckets，而在下边一行中的x和y的球投影是彼此接近的。
LSH attention 上图为LSH attention中：hash-bucketing、排序、chunking分块和计算attention结果的简化描述；下图中的(a)-(d)是步骤中的矩阵形式：" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{font-family:bree serif,sans-serif;-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  

  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="XY&#39;s Blog">
  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Bungee+Shade" rel="stylesheet">
  
  

  
  
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/"
      >Gao dy</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/gdy0924" target="_blank">About</a>
  </div>
  
</div>
<div class="row end-xs">
  
  <div class="lang-switch col-xs-3 col-xs-offset-9">
    <a href="/en/">English</a>
  </div>
    
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">Reformer</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2022-05-18 00:00:00 UTC">
                18 May 2022
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://gdy0924.github.io/">@Gao dy</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>REFORMER: THE EFFICIENT TRANSFORMER<br>
原文链接：<a href="https://arxiv.org/pdf/2001.04451.pdf">DeiT</a></p>
<p><img src="/img/reformer1.PNG" alt="">
    上表为内存使用量和时间复杂度的比较，其中$l$为序列的长度，$b$为batch size的大小，$n_{h}$为头数，$n_{c}$为LSH的chunk数量，$n_{r}$为hash重复的次数。</p>
<h2 id="abstract">Abstract</h2>
<p>    Transformer在许多任务上都实现了最先进的结果，但是其训练代价是很昂贵的，特别是在长序列上。因此，我们提出两种提高transformer的技术：（1）局部敏感哈希（locality-sensitive hashing），将复杂度从$O(L^2)$降低到$O(L log L)$；（2）使用可逆残差层而不是标准残差，使得在训练过程中只存储最后一层的激活值（激活函数的输入值），而不是N次，N为层数。</p>
<h2 id="introduction">Introduction</h2>
<p>    基于transformer的大型模型甚至不能在单个GPU上进行微调，因为它们的内存需求需要多加速器硬件设置。transformer架构中的主要内存使用在以下几个方面：<br>
    （1）一个包含N层的模型，其内存比单层模型使用的内存大N倍，因为需要存储激活值以进行反向传播，即每一层的激活值都需要存储下来；<br>
    （2）transformer block中的全连接层FFN中的深度往往比$d_{model}$大得多，如：FFN的输入维度为512，那么中间的维度可能是2048，这也占了内存的很大部分；<br>
    （3）注意力机制在计算序列长度为$L$的序列时，在计算和内存的复杂度上都是$O(L^2)$。<br>
    我们提出以下方法来解决上述问题：<br>
    （1）可逆层：使得整个网络只用存储最后一层的激活值，而不是所有层，之前层的激活值可以通过最后一层推算出来；<br>
    （2）在FFN层中分割激活值，将他们以快的形式进行处理；<br>
    （3）基于局部敏感哈希的近似注意力，将复杂度从$O(L^2)$降低到$O(L log L)$。</p>
<h2 id="locality-sensitive-hashing-attention">LOCALITY-SENSITIVE HASHING ATTENTION</h2>
<h3 id="dot-product-attention">Dot-product attention</h3>
<p>    transformer中使用点乘计算注意力，输入是维度为$d_{k}$的query和key，以及维度为$d_{v}$的value，计算公式如下：
$$
Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V
$$</p>
<h3 id="multi-head-attention">Multi-head attention</h3>
<p>    使用可学习的矩阵$W_{Q}$、$W_{K}$和$W_{V}$来处理query、key和value。</p>
<h3 id="memory-efficient-attention">Memory-efficient attention</h3>
<p>    假设Q、K、V的大小都是$L×d_{model}$，那么$QK^{T}$的计算结果的大小就是$L×L$的，当序列长度很长时，是需要一定的内存去存储的，这就限制了transformer的使用。</p>
<h3 id="where-do-q-k-v-come-from">Where do Q, K, V come from?</h3>
<p>    Q、K、V是将输入经过三个不同的线性层来得到的。对于LSH注意力模型，我们希望query和keys（Q和K）是相同的，这可以通过一个使用同一个线性层来实现，这种方式称为shared-QK Transformer，并经过实验验证，这种方式并不会影响性能。</p>
<h3 id="hashing-attention">Hashing attention</h3>
<p>    对于LSH注意力模型，从Q=K和V开始，大小为$L×d_{model}$，并根据之前的公式，主要问题在于$QK^{T}$部分的计算。但是，我们最终是只关心softmax的结果，并且softmax是由最大元素来主导的。也就是说，对于每个query$q_{i}$，我们只需要关注在K中最接近$q_{i}$的那些key。比如：假设长度为64K，对于每个$q_{i}$，我们只考虑一个小子集，32或64个最接近的key的集合，这样就可以减少计算量，那么如何在K中找到“最近的邻居”呢？</p>
<h3 id="locality-sensitive-hashing">Locality sensitive hashing</h3>
<p>    在高维空间中快速寻找最近邻的问题可以通过局部敏感哈希(LSH)来解决：使用一种哈希函数$H(x)$，使得两个个相似度很高的数据以较高的概率映射成同一个hash值，而令两个相似度很低的数据以极低的概率映射成同一个hash值，该函数称为局部敏感哈希。<br>
    我们通过随即投影来实现，如下图。为了得到，我们先创建一个随机矩阵，大小为$[d_{k},b/2]$，然后定义$h(x)=argmax([xR;-xR])$，其中$[u;v]$表示两个向量的连接。
<img src="/img/reformer2.PNG" alt="">
    在上图中，为角度局部敏感哈希，使用球投影点的随即旋转角度，在上图的二维描述中，上边一行的两个点x和y不太可能在一个中hash buckets，而在下边一行中的x和y的球投影是彼此接近的。</p>
<h3 id="lsh-attention">LSH attention</h3>
<p><img src="/img/reformer7.PNG" alt="">
    上图为LSH attention中：hash-bucketing、排序、chunking分块和计算attention结果的简化描述；下图中的(a)-(d)是步骤中的矩阵形式：</p>
<p><img src="/img/reformer8.PNG" alt="">
    (a)为原始Attention，可以看出是稀疏的矩阵；<br>
    (b)为对k、q进行hash操作，投射到不同的bucket中，但从图中可以看出存在问题，即每个bucket中k和q的数量不一致，无法进行attention计算；<br>
    (c)置Q与K相同，也就是shared-QK操作，具体是$k_{j}=\frac{q_{j}}{\left | q_{j} \right |}$，这时候就能保证对角线都是attend to的，而且q和k在bucket中的个数是一样，在图中也可以看出，对角线是空心的，是因为：如果attend to本身，会导致其值特别大，其他的值特别小，经过softmax之后，其他都是0，就自己本身是1，因此不去计算自己与自己的attention，除非是在第一个位置，没有其他位置可用时，才与自己计算；<br>
    (d)chunk分块，可以实现并行计算，同时解决了某些bucket中数量多，某些数量少，设置的chunk的长度为$\frac{2L}{n}$（序列长度为$L$，bucket个数为$n$），即对于每个query，需要与自己chunk中的同一个bucket中的位置进行计算，也要与前一个chunk中和自己在一个bucket中的位置进行计算。</p>
<h3 id="multi-round-lsh-attention">Multi-round LSH attention</h3>
<p>    使用哈希函数，相似的向量落在不同的桶中的概率总是很小的，同时这个概率可以通过使用n轮不同的哈希函数来降低。</p>
<h3 id="causal-masking-for-shared-qk-attention">Causal masking for shared-QK attention</h3>
<p>    在transformer中，使用mask来防止关注到未来的信息，虽然不允许使用未来的信息，但是经典的transformer中是允许关注自己的，但这在shared-QK attention中是不可取的，因为其与自身向量的点积总是大于query向量与另一个位置的向量的点积的（我的理解是：在传统transformer中，因为Q和K是不同的，那么最重要的计算结果会很大可能不是自己与自己，那么就不用担心自己与自己来点乘，但是如果Q和K相同了，那么肯定自己与自己的结果是最大的，但可能不是很好的结果）。因此，需要mask来对自己的内容进行掩码，除非在token没有其他有效注意目标的情况下，例如序列中的第一个token，没有未来信息可以使用，也没有过去的信息，那么就需要自己与自己。</p>
<h2 id="reversible-transformer">Reversible Transformer</h2>
<p>    从第一个表中可以看出，每一项都以$b\cdot n_{h}\cdot l\cdot d_{k}$或者$b\cdot l\cdot d_{model}$开始，这是无法避免的。在每层激活前的大小已经是$b\cdot l\cdot d_{model}$，所有具有$n_{l}$层的模型内存使用至少是$b\cdot l\cdot d_{model} \cdot n_{l}$。更糟糕的是，在前馈层中维度$d$更大，通常为4096，因此使得内存使用更大。<br>
    因此，我们提出使用可逆层来处理$n_{l}$的部分，以降低成本。</p>
<h3 id="revnets">RevNets</h3>
<p>    可逆残差网络（）主要思想是允许任何给定层的激活值可以从下一层的激活值中计算出来，在只使用模型参数的情况下，即不需要存储每一层的激活值，只存储最后一层的激活值。当反向传播从网络的输出传递到输入时，层可以逐个反向反转，而不是必须检查中间激活值以供反向传播使用。<br>
    正常的残差层通过一个函数$x \mapsto y$，该函数有一个输入和一个输出，并且具有$y=x+F(x)$的形式，而对于可逆层来说：$(x_{1},x_{2}) \mapsto (y_{1},y_{2})$，并且遵循：
$$
y_{1}=x_{1}+F(x_{2})			y_{2}=x_{2}+G(y_{1})
$$
    某一层可以通过减去残差来实现反转：
$$
x_{2}=y_{2}-G(y_{1})			x_{1}=y_{1}-F(x_{2})
$$</p>
<h3 id="reversible-transformer-1">Reversible Transformer</h3>
<p>    我们将RevNet的想法融入到transformer中，在revnet block中结合注意力层和前馈层。在上述公式中，$F$作为一个注意力层，$G$作为一个前馈层：
$$
Y_{1}=X_{1}+Attention(X_{2})			Y_{2}=X_{2}+FeedForward(Y_{1})
$$
    可逆transformer中不需要在每一层中存储激活值，因此去掉了$n_{l}$项，减少了计算量与内存使用。</p>
<h3 id="chunking">Chunking</h3>
<p>    虽然可逆性消除了$n_{l}$项，但前馈层中的维数依旧很高，如4096，然而，前馈层中的计算在一个序列中的不同位置上是完全独立的，因此计算可以被分成$c$个块：
$$
Y_{2}=[Y_{2}^{(1)};&hellip;;Y_{2}^{(c)}]=[X_{2}^{(1)}+FeedForward(Y_{1}^{(1)});&hellip;;X_{2}^{(c)}+FeedForward(Y_{1}^{(c)})]
$$
    前馈层通常通过并行地对所有位置执行计算来进行批处理，但是一次在一个块上进行操作可能会减少内存。</p>
<h2 id="实验">实验</h2>
<h3 id="effect-of-sharing-qk">Effect of sharing QK</h3>
<p><img src="/img/reformer3.PNG" alt="">
    上图为普通注意力和sharing QK注意力的困惑曲线（perplexity curves），可以看出sharing QK注意力并不比普通的要差。</p>
<h3 id="effect-of-reversible-layers">Effect of reversible layers</h3>
<p><img src="/img/reformer4.PNG" alt="">
    上图中比较了普通的transformer和可逆transformer的困惑曲线（perplexity curves），可以看出，学习曲线几乎相同，可以得出可逆transformer的内存节省并不以牺牲准确性为代价。</p>
<h3 id="lsh-attention-in-transformer">LSH attention in Transformer</h3>
<p><img src="/img/reformer5.PNG" alt="">
    由上图可以看出，随着散列数量的增加，性能会更加准确，并且可以与纯注意力相比较。</p>
<h3 id="large-reformer-models">Large Reformer models</h3>
<p><img src="/img/reformer6.PNG" alt="">
    上图为不同注意力模型的运行速度与序列长度的关系，可以看出，当普通注意机制在较长的序列长度下变得较慢时，LSH的注意速度可以保持不变。</p>
<h2 id="conclusion">Conclusion</h2>
<p>    Reformer将transformer的建模能力与在长序列上有效执行的结构相结合，这将有助于大型的、参数丰富的transformer模型应用变得更加广泛，处理长序列的能力也为在许多生成任务中使用Reformer开辟了道路。</p>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://joway.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>