<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.89.4" />
  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/Googlecode.min.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Gao dy" />
  <meta property="og:url" content="https://gdy0924.github.io/posts/segnet/" />
  <link rel="canonical" href="https://gdy0924.github.io/posts/segnet/" /><link rel="alternate" type="application/atom+xml" href="https://gdy0924.github.io/index.xml" title="XY&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gdy0924.github.io\/"
      },
      "articleSection" : "posts",
      "name" : "SegNet",
      "headline" : "SegNet",
      "description" : "原文链接：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\nAbstract  提出了一种新的深度全卷积神经网络结构，用于像素级语义分割，称为SegNet：由一个编码器和一个解码器，以及一个分类层组成。其中，编码器的结构与VGG-16中的13个卷积层完全相同；解码器用于将低分辨率特征图映射到与输入分辨率相同的特征图，以进行像素级分类。\nSegNet创新之处是：利用解码器对其较低分辨率的特征图进行上采样。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引来进行非线性上采样。得到的上采样的映射是稀疏的，然后与可训练的卷积核进行卷积，以生成密集的特征映射。\nIntroduction  语义分割有广泛的应用，从场景理解、推断对象之间的支持关系到自动驾驶。最近的一些方法尝试直接采用用于类别预测的深度神经网络结构来进行像素级标记，但结果不够精确。这主要是因为最大池化和下采样降低了特征图的分辨率。因此，SegNet的动机是将低分辨率特征图映射到与输入分辨率相同以进行像素级分类。这种映射必须产生对精确的边界定位有用的特征。\nSegNet中的编码器与VGG16中的卷积层相同，删除了VGG16末端的全连接层。SegNet的关键组成部分是解码器网络，由逐层的编码器组成，每个编码器都对应着一个解码器。其中，解码器使用从对应的编码器传递过来的最大池化索引，对其输入的特征图进行非线性上采样。\n在解码过程中重用最大池化索引有几个优点：（1）改进边界划分；（2）减少参数数量；（3）这种形式的上采样可以兼容任何的编码器-解码器结构，只需要很少的修改。\nRelative Work  深度卷积神经网络在图像分类方面的成功，许多研究者尝试将用于分类的网络应用于分割。更新的深度架构专门为分割而设计，通过卷积编码和映射低分辨率的图像表示到像素级预测，提高了最先进的技术。\n全卷积网络（FCN）中的每个解码器对其输入特征图进行上采样，并将它们与相应的编码器特征图结合起来，以产生到下一个解码器的输入。通过对FCN添加一个递归神经网络（RNN），并在大数据集上对其进行微调，进一步提高了FCN的预测性能：RNN层模拟了CRFs清晰的边界划分能力，同时利用了FCN的特征表示能力。\n多尺度融合的深度网络架构也在不断被提出，有两种类型：（1）使用几个不同尺度的输入图像和相应的深度特征提取网络；（2）组合来自单个深度架构的不同层的特征图。常见的想法是：使用在多个尺度上提取的特征来提供局部和全局上下文信息，并且使用早期编码层的特征图以保留更多的细节信息，从而生成更清晰的预测边界。\n我们的工作受到无监督特征学习架构的启发，其关键的模块是一个编码器-解码器网络。编码器通过卷积、非线性激活、最大池化和下采样来获得特征图，同时在池化过程中，最大位置的索引被存储并传递给解码器。解码器通过使用存储的集合索引来对特征图进行上采样：它使用一个可训练的卷积操作对这些上采样映射进行卷积，以重建输入图像。\nArchitecture SegNet由一个编码器网络和一个相应的解码器网络，以及最后一个像素级分类层组成，如上图所示。编码器网络由13个卷积层组成，分别对应VGG16网络中的前13个卷积层。因此，我们可以从大数据集上训练的权值来初始化训练过程。删除全连接层以便编码器的最终输出保留更高分辨率的特征图。每个编码器层都具有与其相对应的解码器层，因此该解码器网络也具有13层。最终的解码器输出被输入到一个softmax分类器，以预测每个像素的类别概率。\n编码器网络中的每个编码器进行卷积操作，以生成特征图，接着是Batch Normalization、ReLU和2×2大小（步幅为2）的最大池化（非重叠窗口）。随着下采样，图像分辨率降低，不利于分割时的边界预测，因此，在进行下采样之前，需要在编码器特征图中捕获和存储边界信息。如果推理过程中的内存不受约束，那么所有的编码器中的特征图都可以被存储，但通常不会这样实现。因此我们提出了一种方法：即只存储最大池化索引，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，这可以对每个2×2池窗口使用2位来完成。虽然这种较低的内存存储会导致轻微的精度损失，但仍然适用于实际应用。\n解码器使用从相应的编码器层获取到的最大池化索引对其输入特征图进行上采样，以生成稀疏特征图，如下图所示。然后将这些特征图进行卷积，以生成密集的特征图。解码器与其对应的编码器，它们的通道数也是相同的，除了第一个编码器对应的解码器。第一个编码器的输入通道数是3，而其对应的解码器（最后一个）通道数对应的是类别的数量。 与SegNet相比，U-Net（用于医学领域）不重用池化索引，而是将整个特征图（以更多内存为代价）转移到相应的解码器，并将它们连接到上采样（通过反卷积）解码器特征图。另一方面，SegNet使用来自VGG网的所有预训练的卷积层权值作为预训练的权值。\nDecoder Variants  总设计了8中不同的解码器变体架构，分别如下：\n（1）Bilinear-Interpolation：使用固定的双线性插值权值的上采样方法，即不需要学习的上采样；\n（2）SegNet-Basic： a smaller version of SegNet，4个encoder和4个decoder；\n（3）SegNet-Basic-EncoderAddition：在解码器部分添加上相对应的编码器部分的特征图；\n（4）SegNet-Basic-SingleChannelDecoder：在SegNet中，解码器对特征图进行上采样之后，会进行卷积操作，其通道数与对应编码器的通道数相同，对其变形就是设置通道数为单通道；\n（5）FCN-Basic：与SegNet-Basic共享相同的编码器网络，但在解码器中使用反卷积进行上采样；\n（6）FCN-Basic-NoAddition：不将编码器中的特征图添加到解码器中；\n（7）FCN-Basic-NoDimReduction：在编码器中不进行降维操作；\n（8）FCN-Basic-NoAddition-NoDimReduction：既不在编码器中进行降维，也不将编码器的特征图添加到解码器中。\n上述8中变形结构的预测结果如下表所示，利用全局精度G、类平均精度C、平均交并比mIoU和语义轮廓度量BF来评价性能： Analysis  为了比较不同的解码器变体的定量性能，我们使用三个常用的性能度量：全局精度G，类平均精度C和平均交并比mIoU，计算语义轮廓得分BF就是评估f1-score，其中包括计算精度和召回率。\n从上表中，我们可以看到，基于双线性插值的没有任何学习的上采样在所有精度指标上表现最差，而其余所有学习到的上采样（FCN-Basic及其变体）或上采样后的进行卷积（SegNet-Basic及其变体）的方法都表现得明显更好，强调了学习解码器的必要性。\n当比较SegNet-Basic和FCN-Basic时，我们可以看到两者在这个测试的所有准确性测量上都表现得同样好。不同之处在于，SegNet在推理过程中使用了更少的内存，因为它只存储最大池索引；另一方面，FCN-Basic需要存储编码器中的特征图，会消耗更多的内存。同时，相比于使用了降维的FCN-Basic，SegNet-Basic中的特征维度会更多，因此，FCN-Basic的推理速度会更快。从另一个角度来看，SegNet-Basic中的解码器网络使其总体上成为一个比FCN-Basic更大的网络，这赋予了它更多的灵活性，因此在相同的迭代次数下获得了比FCN-Basic更高的训练精度。总的来说，当推理时间内存受到限制，但推理时间可能在一定程度上受损时，SegNet-Basic比FCN-Basic有优势。\nSegNet-Basic在解码器方面与FCN-Basic-NoAddition最相似，两者都学会了生成密集的特征图，要么通过反卷积，要么通过上采样接着卷积。SegNet-Basic的性能更优越，部分原因是它的解码器尺寸更大。FCN-Basic-NoAddition的准确性低于FCN-Basic，这表明，捕获编码器特征图中的信息是至关重要的。\nFCN-Basic-NoAddition-NoDimReduction模型的大小比SegNet-Basic略大，因为最终的编码器特征图没有被降维以匹配类别的数量。该模型的测试性能和训练精度都不如SegNet-Basic，这表明使用一个更大的解码器是不够的，但捕获编码器的特征图信息是很重要的，特别是细粒度的轮廓信息。\n总的来说：\n（1）当编码器特征图可以完全存储时，能够获得最佳性能。这一点最清楚地反映在语义轮廓度量(BF)中；\n（2）当推理过程中的内存受到限制时，就可以存储压缩形式的编码器特征图（降维、最大池索引），并与适当的解码器一起使用，以提高性能；\n（3）更大的解码器可以提高给定的编码器网络的性能。\nConclusion  我们提出了SegNet，一种用于语义分割的深度卷积网络架构。我们对SegNet及其它变体进行了分析，以揭示设计分割架构所涉及的实际权衡，特别是训练时间、内存和准确性。那些完全存储编码器网络特征图的体系结构表现最好，但在推理期间需要消耗更多的内存。另一方面，SegNet更有效，因为它只存储特征图的最大池化索引，并将其用于自己的解码器网络中，以获得良好的性能。",
      "inLanguage" : "en-US",
      "author" : "Gao dy",
      "creator" : "Gao dy",
      "publisher": "Gao dy",
      "accountablePerson" : "Gao dy",
      "copyrightHolder" : "Gao dy",
      "copyrightYear" : "2022",
      "datePublished": "2022-04-29 00:00:00 \u002b0000 UTC",
      "dateModified" : "2022-04-29 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gdy0924.github.io\/posts\/segnet\/",
      "keywords" : [  ]
  }
</script>
<title>SegNet</title>
  <meta property="og:title" content="SegNet" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="原文链接：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
Abstract  提出了一种新的深度全卷积神经网络结构，用于像素级语义分割，称为SegNet：由一个编码器和一个解码器，以及一个分类层组成。其中，编码器的结构与VGG-16中的13个卷积层完全相同；解码器用于将低分辨率特征图映射到与输入分辨率相同的特征图，以进行像素级分类。
SegNet创新之处是：利用解码器对其较低分辨率的特征图进行上采样。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引来进行非线性上采样。得到的上采样的映射是稀疏的，然后与可训练的卷积核进行卷积，以生成密集的特征映射。
Introduction  语义分割有广泛的应用，从场景理解、推断对象之间的支持关系到自动驾驶。最近的一些方法尝试直接采用用于类别预测的深度神经网络结构来进行像素级标记，但结果不够精确。这主要是因为最大池化和下采样降低了特征图的分辨率。因此，SegNet的动机是将低分辨率特征图映射到与输入分辨率相同以进行像素级分类。这种映射必须产生对精确的边界定位有用的特征。
SegNet中的编码器与VGG16中的卷积层相同，删除了VGG16末端的全连接层。SegNet的关键组成部分是解码器网络，由逐层的编码器组成，每个编码器都对应着一个解码器。其中，解码器使用从对应的编码器传递过来的最大池化索引，对其输入的特征图进行非线性上采样。
在解码过程中重用最大池化索引有几个优点：（1）改进边界划分；（2）减少参数数量；（3）这种形式的上采样可以兼容任何的编码器-解码器结构，只需要很少的修改。
Relative Work  深度卷积神经网络在图像分类方面的成功，许多研究者尝试将用于分类的网络应用于分割。更新的深度架构专门为分割而设计，通过卷积编码和映射低分辨率的图像表示到像素级预测，提高了最先进的技术。
全卷积网络（FCN）中的每个解码器对其输入特征图进行上采样，并将它们与相应的编码器特征图结合起来，以产生到下一个解码器的输入。通过对FCN添加一个递归神经网络（RNN），并在大数据集上对其进行微调，进一步提高了FCN的预测性能：RNN层模拟了CRFs清晰的边界划分能力，同时利用了FCN的特征表示能力。
多尺度融合的深度网络架构也在不断被提出，有两种类型：（1）使用几个不同尺度的输入图像和相应的深度特征提取网络；（2）组合来自单个深度架构的不同层的特征图。常见的想法是：使用在多个尺度上提取的特征来提供局部和全局上下文信息，并且使用早期编码层的特征图以保留更多的细节信息，从而生成更清晰的预测边界。
我们的工作受到无监督特征学习架构的启发，其关键的模块是一个编码器-解码器网络。编码器通过卷积、非线性激活、最大池化和下采样来获得特征图，同时在池化过程中，最大位置的索引被存储并传递给解码器。解码器通过使用存储的集合索引来对特征图进行上采样：它使用一个可训练的卷积操作对这些上采样映射进行卷积，以重建输入图像。
Architecture SegNet由一个编码器网络和一个相应的解码器网络，以及最后一个像素级分类层组成，如上图所示。编码器网络由13个卷积层组成，分别对应VGG16网络中的前13个卷积层。因此，我们可以从大数据集上训练的权值来初始化训练过程。删除全连接层以便编码器的最终输出保留更高分辨率的特征图。每个编码器层都具有与其相对应的解码器层，因此该解码器网络也具有13层。最终的解码器输出被输入到一个softmax分类器，以预测每个像素的类别概率。
编码器网络中的每个编码器进行卷积操作，以生成特征图，接着是Batch Normalization、ReLU和2×2大小（步幅为2）的最大池化（非重叠窗口）。随着下采样，图像分辨率降低，不利于分割时的边界预测，因此，在进行下采样之前，需要在编码器特征图中捕获和存储边界信息。如果推理过程中的内存不受约束，那么所有的编码器中的特征图都可以被存储，但通常不会这样实现。因此我们提出了一种方法：即只存储最大池化索引，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，这可以对每个2×2池窗口使用2位来完成。虽然这种较低的内存存储会导致轻微的精度损失，但仍然适用于实际应用。
解码器使用从相应的编码器层获取到的最大池化索引对其输入特征图进行上采样，以生成稀疏特征图，如下图所示。然后将这些特征图进行卷积，以生成密集的特征图。解码器与其对应的编码器，它们的通道数也是相同的，除了第一个编码器对应的解码器。第一个编码器的输入通道数是3，而其对应的解码器（最后一个）通道数对应的是类别的数量。 与SegNet相比，U-Net（用于医学领域）不重用池化索引，而是将整个特征图（以更多内存为代价）转移到相应的解码器，并将它们连接到上采样（通过反卷积）解码器特征图。另一方面，SegNet使用来自VGG网的所有预训练的卷积层权值作为预训练的权值。
Decoder Variants  总设计了8中不同的解码器变体架构，分别如下：
（1）Bilinear-Interpolation：使用固定的双线性插值权值的上采样方法，即不需要学习的上采样；
（2）SegNet-Basic： a smaller version of SegNet，4个encoder和4个decoder；
（3）SegNet-Basic-EncoderAddition：在解码器部分添加上相对应的编码器部分的特征图；
（4）SegNet-Basic-SingleChannelDecoder：在SegNet中，解码器对特征图进行上采样之后，会进行卷积操作，其通道数与对应编码器的通道数相同，对其变形就是设置通道数为单通道；
（5）FCN-Basic：与SegNet-Basic共享相同的编码器网络，但在解码器中使用反卷积进行上采样；
（6）FCN-Basic-NoAddition：不将编码器中的特征图添加到解码器中；
（7）FCN-Basic-NoDimReduction：在编码器中不进行降维操作；
（8）FCN-Basic-NoAddition-NoDimReduction：既不在编码器中进行降维，也不将编码器的特征图添加到解码器中。
上述8中变形结构的预测结果如下表所示，利用全局精度G、类平均精度C、平均交并比mIoU和语义轮廓度量BF来评价性能： Analysis  为了比较不同的解码器变体的定量性能，我们使用三个常用的性能度量：全局精度G，类平均精度C和平均交并比mIoU，计算语义轮廓得分BF就是评估f1-score，其中包括计算精度和召回率。
从上表中，我们可以看到，基于双线性插值的没有任何学习的上采样在所有精度指标上表现最差，而其余所有学习到的上采样（FCN-Basic及其变体）或上采样后的进行卷积（SegNet-Basic及其变体）的方法都表现得明显更好，强调了学习解码器的必要性。
当比较SegNet-Basic和FCN-Basic时，我们可以看到两者在这个测试的所有准确性测量上都表现得同样好。不同之处在于，SegNet在推理过程中使用了更少的内存，因为它只存储最大池索引；另一方面，FCN-Basic需要存储编码器中的特征图，会消耗更多的内存。同时，相比于使用了降维的FCN-Basic，SegNet-Basic中的特征维度会更多，因此，FCN-Basic的推理速度会更快。从另一个角度来看，SegNet-Basic中的解码器网络使其总体上成为一个比FCN-Basic更大的网络，这赋予了它更多的灵活性，因此在相同的迭代次数下获得了比FCN-Basic更高的训练精度。总的来说，当推理时间内存受到限制，但推理时间可能在一定程度上受损时，SegNet-Basic比FCN-Basic有优势。
SegNet-Basic在解码器方面与FCN-Basic-NoAddition最相似，两者都学会了生成密集的特征图，要么通过反卷积，要么通过上采样接着卷积。SegNet-Basic的性能更优越，部分原因是它的解码器尺寸更大。FCN-Basic-NoAddition的准确性低于FCN-Basic，这表明，捕获编码器特征图中的信息是至关重要的。
FCN-Basic-NoAddition-NoDimReduction模型的大小比SegNet-Basic略大，因为最终的编码器特征图没有被降维以匹配类别的数量。该模型的测试性能和训练精度都不如SegNet-Basic，这表明使用一个更大的解码器是不够的，但捕获编码器的特征图信息是很重要的，特别是细粒度的轮廓信息。
总的来说：
（1）当编码器特征图可以完全存储时，能够获得最佳性能。这一点最清楚地反映在语义轮廓度量(BF)中；
（2）当推理过程中的内存受到限制时，就可以存储压缩形式的编码器特征图（降维、最大池索引），并与适当的解码器一起使用，以提高性能；
（3）更大的解码器可以提高给定的编码器网络的性能。
Conclusion  我们提出了SegNet，一种用于语义分割的深度卷积网络架构。我们对SegNet及其它变体进行了分析，以揭示设计分割架构所涉及的实际权衡，特别是训练时间、内存和准确性。那些完全存储编码器网络特征图的体系结构表现最好，但在推理期间需要消耗更多的内存。另一方面，SegNet更有效，因为它只存储特征图的最大池化索引，并将其用于自己的解码器网络中，以获得良好的性能。" />
  <meta name="description" content="原文链接：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
Abstract  提出了一种新的深度全卷积神经网络结构，用于像素级语义分割，称为SegNet：由一个编码器和一个解码器，以及一个分类层组成。其中，编码器的结构与VGG-16中的13个卷积层完全相同；解码器用于将低分辨率特征图映射到与输入分辨率相同的特征图，以进行像素级分类。
SegNet创新之处是：利用解码器对其较低分辨率的特征图进行上采样。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引来进行非线性上采样。得到的上采样的映射是稀疏的，然后与可训练的卷积核进行卷积，以生成密集的特征映射。
Introduction  语义分割有广泛的应用，从场景理解、推断对象之间的支持关系到自动驾驶。最近的一些方法尝试直接采用用于类别预测的深度神经网络结构来进行像素级标记，但结果不够精确。这主要是因为最大池化和下采样降低了特征图的分辨率。因此，SegNet的动机是将低分辨率特征图映射到与输入分辨率相同以进行像素级分类。这种映射必须产生对精确的边界定位有用的特征。
SegNet中的编码器与VGG16中的卷积层相同，删除了VGG16末端的全连接层。SegNet的关键组成部分是解码器网络，由逐层的编码器组成，每个编码器都对应着一个解码器。其中，解码器使用从对应的编码器传递过来的最大池化索引，对其输入的特征图进行非线性上采样。
在解码过程中重用最大池化索引有几个优点：（1）改进边界划分；（2）减少参数数量；（3）这种形式的上采样可以兼容任何的编码器-解码器结构，只需要很少的修改。
Relative Work  深度卷积神经网络在图像分类方面的成功，许多研究者尝试将用于分类的网络应用于分割。更新的深度架构专门为分割而设计，通过卷积编码和映射低分辨率的图像表示到像素级预测，提高了最先进的技术。
全卷积网络（FCN）中的每个解码器对其输入特征图进行上采样，并将它们与相应的编码器特征图结合起来，以产生到下一个解码器的输入。通过对FCN添加一个递归神经网络（RNN），并在大数据集上对其进行微调，进一步提高了FCN的预测性能：RNN层模拟了CRFs清晰的边界划分能力，同时利用了FCN的特征表示能力。
多尺度融合的深度网络架构也在不断被提出，有两种类型：（1）使用几个不同尺度的输入图像和相应的深度特征提取网络；（2）组合来自单个深度架构的不同层的特征图。常见的想法是：使用在多个尺度上提取的特征来提供局部和全局上下文信息，并且使用早期编码层的特征图以保留更多的细节信息，从而生成更清晰的预测边界。
我们的工作受到无监督特征学习架构的启发，其关键的模块是一个编码器-解码器网络。编码器通过卷积、非线性激活、最大池化和下采样来获得特征图，同时在池化过程中，最大位置的索引被存储并传递给解码器。解码器通过使用存储的集合索引来对特征图进行上采样：它使用一个可训练的卷积操作对这些上采样映射进行卷积，以重建输入图像。
Architecture SegNet由一个编码器网络和一个相应的解码器网络，以及最后一个像素级分类层组成，如上图所示。编码器网络由13个卷积层组成，分别对应VGG16网络中的前13个卷积层。因此，我们可以从大数据集上训练的权值来初始化训练过程。删除全连接层以便编码器的最终输出保留更高分辨率的特征图。每个编码器层都具有与其相对应的解码器层，因此该解码器网络也具有13层。最终的解码器输出被输入到一个softmax分类器，以预测每个像素的类别概率。
编码器网络中的每个编码器进行卷积操作，以生成特征图，接着是Batch Normalization、ReLU和2×2大小（步幅为2）的最大池化（非重叠窗口）。随着下采样，图像分辨率降低，不利于分割时的边界预测，因此，在进行下采样之前，需要在编码器特征图中捕获和存储边界信息。如果推理过程中的内存不受约束，那么所有的编码器中的特征图都可以被存储，但通常不会这样实现。因此我们提出了一种方法：即只存储最大池化索引，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，这可以对每个2×2池窗口使用2位来完成。虽然这种较低的内存存储会导致轻微的精度损失，但仍然适用于实际应用。
解码器使用从相应的编码器层获取到的最大池化索引对其输入特征图进行上采样，以生成稀疏特征图，如下图所示。然后将这些特征图进行卷积，以生成密集的特征图。解码器与其对应的编码器，它们的通道数也是相同的，除了第一个编码器对应的解码器。第一个编码器的输入通道数是3，而其对应的解码器（最后一个）通道数对应的是类别的数量。 与SegNet相比，U-Net（用于医学领域）不重用池化索引，而是将整个特征图（以更多内存为代价）转移到相应的解码器，并将它们连接到上采样（通过反卷积）解码器特征图。另一方面，SegNet使用来自VGG网的所有预训练的卷积层权值作为预训练的权值。
Decoder Variants  总设计了8中不同的解码器变体架构，分别如下：
（1）Bilinear-Interpolation：使用固定的双线性插值权值的上采样方法，即不需要学习的上采样；
（2）SegNet-Basic： a smaller version of SegNet，4个encoder和4个decoder；
（3）SegNet-Basic-EncoderAddition：在解码器部分添加上相对应的编码器部分的特征图；
（4）SegNet-Basic-SingleChannelDecoder：在SegNet中，解码器对特征图进行上采样之后，会进行卷积操作，其通道数与对应编码器的通道数相同，对其变形就是设置通道数为单通道；
（5）FCN-Basic：与SegNet-Basic共享相同的编码器网络，但在解码器中使用反卷积进行上采样；
（6）FCN-Basic-NoAddition：不将编码器中的特征图添加到解码器中；
（7）FCN-Basic-NoDimReduction：在编码器中不进行降维操作；
（8）FCN-Basic-NoAddition-NoDimReduction：既不在编码器中进行降维，也不将编码器的特征图添加到解码器中。
上述8中变形结构的预测结果如下表所示，利用全局精度G、类平均精度C、平均交并比mIoU和语义轮廓度量BF来评价性能： Analysis  为了比较不同的解码器变体的定量性能，我们使用三个常用的性能度量：全局精度G，类平均精度C和平均交并比mIoU，计算语义轮廓得分BF就是评估f1-score，其中包括计算精度和召回率。
从上表中，我们可以看到，基于双线性插值的没有任何学习的上采样在所有精度指标上表现最差，而其余所有学习到的上采样（FCN-Basic及其变体）或上采样后的进行卷积（SegNet-Basic及其变体）的方法都表现得明显更好，强调了学习解码器的必要性。
当比较SegNet-Basic和FCN-Basic时，我们可以看到两者在这个测试的所有准确性测量上都表现得同样好。不同之处在于，SegNet在推理过程中使用了更少的内存，因为它只存储最大池索引；另一方面，FCN-Basic需要存储编码器中的特征图，会消耗更多的内存。同时，相比于使用了降维的FCN-Basic，SegNet-Basic中的特征维度会更多，因此，FCN-Basic的推理速度会更快。从另一个角度来看，SegNet-Basic中的解码器网络使其总体上成为一个比FCN-Basic更大的网络，这赋予了它更多的灵活性，因此在相同的迭代次数下获得了比FCN-Basic更高的训练精度。总的来说，当推理时间内存受到限制，但推理时间可能在一定程度上受损时，SegNet-Basic比FCN-Basic有优势。
SegNet-Basic在解码器方面与FCN-Basic-NoAddition最相似，两者都学会了生成密集的特征图，要么通过反卷积，要么通过上采样接着卷积。SegNet-Basic的性能更优越，部分原因是它的解码器尺寸更大。FCN-Basic-NoAddition的准确性低于FCN-Basic，这表明，捕获编码器特征图中的信息是至关重要的。
FCN-Basic-NoAddition-NoDimReduction模型的大小比SegNet-Basic略大，因为最终的编码器特征图没有被降维以匹配类别的数量。该模型的测试性能和训练精度都不如SegNet-Basic，这表明使用一个更大的解码器是不够的，但捕获编码器的特征图信息是很重要的，特别是细粒度的轮廓信息。
总的来说：
（1）当编码器特征图可以完全存储时，能够获得最佳性能。这一点最清楚地反映在语义轮廓度量(BF)中；
（2）当推理过程中的内存受到限制时，就可以存储压缩形式的编码器特征图（降维、最大池索引），并与适当的解码器一起使用，以提高性能；
（3）更大的解码器可以提高给定的编码器网络的性能。
Conclusion  我们提出了SegNet，一种用于语义分割的深度卷积网络架构。我们对SegNet及其它变体进行了分析，以揭示设计分割架构所涉及的实际权衡，特别是训练时间、内存和准确性。那些完全存储编码器网络特征图的体系结构表现最好，但在推理期间需要消耗更多的内存。另一方面，SegNet更有效，因为它只存储特征图的最大池化索引，并将其用于自己的解码器网络中，以获得良好的性能。" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{font-family:bree serif,sans-serif;-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  

  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="XY&#39;s Blog">
  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Bungee+Shade" rel="stylesheet">
  
  

  
  
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/"
      >Gao dy</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/gdy0924" target="_blank">About</a>
  </div>
  
</div>
<div class="row end-xs">
  
  <div class="lang-switch col-xs-3 col-xs-offset-9">
    <a href="/en/">English</a>
  </div>
    
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">SegNet</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2022-04-29 00:00:00 UTC">
                29 Apr 2022
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://gdy0924.github.io/">@Gao dy</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>原文链接：<a href="https://arxiv.org/pdf/1511.00561.pdf">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></p>
<h2 id="abstract">Abstract</h2>
<p>    提出了一种新的深度全卷积神经网络结构，用于像素级语义分割，称为SegNet：由一个编码器和一个解码器，以及一个分类层组成。其中，编码器的结构与VGG-16中的13个卷积层完全相同；解码器用于将低分辨率特征图映射到与输入分辨率相同的特征图，以进行像素级分类。<br>
    SegNet创新之处是：利用解码器对其较低分辨率的特征图进行上采样。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引来进行非线性上采样。得到的上采样的映射是稀疏的，然后与可训练的卷积核进行卷积，以生成密集的特征映射。</p>
<h2 id="introduction">Introduction</h2>
<p>    语义分割有广泛的应用，从场景理解、推断对象之间的支持关系到自动驾驶。最近的一些方法尝试直接采用用于类别预测的深度神经网络结构来进行像素级标记，但结果不够精确。这主要是因为最大池化和下采样降低了特征图的分辨率。因此，SegNet的动机是将低分辨率特征图映射到与输入分辨率相同以进行像素级分类。这种映射必须产生对精确的边界定位有用的特征。<br>
    SegNet中的编码器与VGG16中的卷积层相同，删除了VGG16末端的全连接层。SegNet的关键组成部分是解码器网络，由逐层的编码器组成，每个编码器都对应着一个解码器。其中，解码器使用从对应的编码器传递过来的最大池化索引，对其输入的特征图进行非线性上采样。<br>
    在解码过程中重用最大池化索引有几个优点：（1）改进边界划分；（2）减少参数数量；（3）这种形式的上采样可以兼容任何的编码器-解码器结构，只需要很少的修改。</p>
<h2 id="relative-work">Relative Work</h2>
<p>    深度卷积神经网络在图像分类方面的成功，许多研究者尝试将用于分类的网络应用于分割。更新的深度架构专门为分割而设计，通过卷积编码和映射低分辨率的图像表示到像素级预测，提高了最先进的技术。<br>
    全卷积网络（FCN）中的每个解码器对其输入特征图进行上采样，并将它们与相应的编码器特征图结合起来，以产生到下一个解码器的输入。通过对FCN添加一个递归神经网络（RNN），并在大数据集上对其进行微调，进一步提高了FCN的预测性能：RNN层模拟了CRFs清晰的边界划分能力，同时利用了FCN的特征表示能力。<br>
多尺度融合的深度网络架构也在不断被提出，有两种类型：（1）使用几个不同尺度的输入图像和相应的深度特征提取网络；（2）组合来自单个深度架构的不同层的特征图。常见的想法是：使用在多个尺度上提取的特征来提供局部和全局上下文信息，并且使用早期编码层的特征图以保留更多的细节信息，从而生成更清晰的预测边界。<br>
    我们的工作受到无监督特征学习架构的启发，其关键的模块是一个编码器-解码器网络。编码器通过卷积、非线性激活、最大池化和下采样来获得特征图，同时在池化过程中，最大位置的索引被存储并传递给解码器。解码器通过使用存储的集合索引来对特征图进行上采样：它使用一个可训练的卷积操作对这些上采样映射进行卷积，以重建输入图像。</p>
<h2 id="architecture">Architecture</h2>
<p><img src="/img/segnet1.PNG" alt="">
    SegNet由一个编码器网络和一个相应的解码器网络，以及最后一个像素级分类层组成，如上图所示。编码器网络由13个卷积层组成，分别对应VGG16网络中的前13个卷积层。因此，我们可以从大数据集上训练的权值来初始化训练过程。删除全连接层以便编码器的最终输出保留更高分辨率的特征图。每个编码器层都具有与其相对应的解码器层，因此该解码器网络也具有13层。最终的解码器输出被输入到一个softmax分类器，以预测每个像素的类别概率。<br>
    编码器网络中的每个编码器进行卷积操作，以生成特征图，接着是Batch Normalization、ReLU和2×2大小（步幅为2）的最大池化（非重叠窗口）。随着下采样，图像分辨率降低，不利于分割时的边界预测，因此，在进行下采样之前，需要在编码器特征图中捕获和存储边界信息。如果推理过程中的内存不受约束，那么所有的编码器中的特征图都可以被存储，但通常不会这样实现。因此我们提出了一种方法：即只存储最大池化索引，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，这可以对每个2×2池窗口使用2位来完成。虽然这种较低的内存存储会导致轻微的精度损失，但仍然适用于实际应用。<br>
    解码器使用从相应的编码器层获取到的最大池化索引对其输入特征图进行上采样，以生成稀疏特征图，如下图所示。然后将这些特征图进行卷积，以生成密集的特征图。解码器与其对应的编码器，它们的通道数也是相同的，除了第一个编码器对应的解码器。第一个编码器的输入通道数是3，而其对应的解码器（最后一个）通道数对应的是类别的数量。
<img src="/img/segnet2.PNG" alt="">
    与SegNet相比，U-Net（用于医学领域）不重用池化索引，而是将整个特征图（以更多内存为代价）转移到相应的解码器，并将它们连接到上采样（通过反卷积）解码器特征图。另一方面，SegNet使用来自VGG网的所有预训练的卷积层权值作为预训练的权值。</p>
<h3 id="decoder-variants">Decoder Variants</h3>
<p>    总设计了8中不同的解码器变体架构，分别如下：<br>
    （1）Bilinear-Interpolation：使用固定的双线性插值权值的上采样方法，即不需要学习的上采样；<br>
    （2）SegNet-Basic： a smaller version of SegNet，4个encoder和4个decoder；<br>
    （3）SegNet-Basic-EncoderAddition：在解码器部分添加上相对应的编码器部分的特征图；<br>
    （4）SegNet-Basic-SingleChannelDecoder：在SegNet中，解码器对特征图进行上采样之后，会进行卷积操作，其通道数与对应编码器的通道数相同，对其变形就是设置通道数为单通道；<br>
    （5）FCN-Basic：与SegNet-Basic共享相同的编码器网络，但在解码器中使用反卷积进行上采样；<br>
    （6）FCN-Basic-NoAddition：不将编码器中的特征图添加到解码器中；<br>
    （7）FCN-Basic-NoDimReduction：在编码器中不进行降维操作；<br>
    （8）FCN-Basic-NoAddition-NoDimReduction：既不在编码器中进行降维，也不将编码器的特征图添加到解码器中。<br>
    上述8中变形结构的预测结果如下表所示，利用全局精度G、类平均精度C、平均交并比mIoU和语义轮廓度量BF来评价性能：
<img src="/img/segnet3.PNG" alt=""></p>
<h3 id="analysis">Analysis</h3>
<p>    为了比较不同的解码器变体的定量性能，我们使用三个常用的性能度量：全局精度G，类平均精度C和平均交并比mIoU，计算语义轮廓得分BF就是评估f1-score，其中包括计算精度和召回率。<br>
    从上表中，我们可以看到，基于双线性插值的没有任何学习的上采样在所有精度指标上表现最差，而其余所有学习到的上采样（FCN-Basic及其变体）或上采样后的进行卷积（SegNet-Basic及其变体）的方法都表现得明显更好，强调了学习解码器的必要性。<br>
    当比较SegNet-Basic和FCN-Basic时，我们可以看到两者在这个测试的所有准确性测量上都表现得同样好。不同之处在于，SegNet在推理过程中使用了更少的内存，因为它只存储最大池索引；另一方面，FCN-Basic需要存储编码器中的特征图，会消耗更多的内存。同时，相比于使用了降维的FCN-Basic，SegNet-Basic中的特征维度会更多，因此，FCN-Basic的推理速度会更快。从另一个角度来看，SegNet-Basic中的解码器网络使其总体上成为一个比FCN-Basic更大的网络，这赋予了它更多的灵活性，因此在相同的迭代次数下获得了比FCN-Basic更高的训练精度。总的来说，当推理时间内存受到限制，但推理时间可能在一定程度上受损时，SegNet-Basic比FCN-Basic有优势。<br>
    SegNet-Basic在解码器方面与FCN-Basic-NoAddition最相似，两者都学会了生成密集的特征图，要么通过反卷积，要么通过上采样接着卷积。SegNet-Basic的性能更优越，部分原因是它的解码器尺寸更大。FCN-Basic-NoAddition的准确性低于FCN-Basic，这表明，捕获编码器特征图中的信息是至关重要的。<br>
    FCN-Basic-NoAddition-NoDimReduction模型的大小比SegNet-Basic略大，因为最终的编码器特征图没有被降维以匹配类别的数量。该模型的测试性能和训练精度都不如SegNet-Basic，这表明使用一个更大的解码器是不够的，但捕获编码器的特征图信息是很重要的，特别是细粒度的轮廓信息。<br>
     总的来说：<br>
     （1）当编码器特征图可以完全存储时，能够获得最佳性能。这一点最清楚地反映在语义轮廓度量(BF)中；<br>
     （2）当推理过程中的内存受到限制时，就可以存储压缩形式的编码器特征图（降维、最大池索引），并与适当的解码器一起使用，以提高性能；<br>
    （3）更大的解码器可以提高给定的编码器网络的性能。</p>
<h2 id="conclusion">Conclusion</h2>
<p>    我们提出了SegNet，一种用于语义分割的深度卷积网络架构。我们对SegNet及其它变体进行了分析，以揭示设计分割架构所涉及的实际权衡，特别是训练时间、内存和准确性。那些完全存储编码器网络特征图的体系结构表现最好，但在推理期间需要消耗更多的内存。另一方面，SegNet更有效，因为它只存储特征图的最大池化索引，并将其用于自己的解码器网络中，以获得良好的性能。</p>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://joway.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>