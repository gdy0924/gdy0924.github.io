<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.89.4" />
  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/Googlecode.min.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Gao dy" />
  <meta property="og:url" content="https://gdy0924.github.io/posts/bert/" />
  <link rel="canonical" href="https://gdy0924.github.io/posts/bert/" /><link rel="alternate" type="application/atom+xml" href="https://gdy0924.github.io/index.xml" title="XY&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gdy0924.github.io\/"
      },
      "articleSection" : "posts",
      "name" : "BERT",
      "headline" : "BERT",
      "description" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n原文链接：BERT\nAbstract  引入了一种新的语言表示模型，称为BERT（Bidirectional Encoder Representations from Transformers）。与最近的语言表示模型不同，BERT是从未标记文本中预训练出的双向表示，通过联合所有层中的左右上下文。因此，预训练的BERT模型可以通过一个额外的输出层来进行微调，从而为广泛的任务创建最先进的模型。\nIntroduction  预训练的语言模型已被证明对许多自然语言处理任务都有效。目前有两种策略为下游任务应用预训练模型：基于特征的和微调的。基于特征的方法使用特定的任务框架，包括预训练的表示作为额外的特征。微调的方法引入最少的参数，通过简单的微调所有的预训练参数来训练下游任务。我们认为，目前的技术限制了预训练的表示，特别是对于微调方法，主要的限制是标准语言模型是单向的，这限制了在预训练过程中可以使用的架构的选择。\n在本文，我们改进了基于微调的方法，提出了BERT（Bidirectional Encoder Representations from Transformers），其通过使用预训练的掩码语言模型（masked language model，MLM）减轻了前面提到的单向性约束。掩码语言模型从输入中随机掩码一些token，其目的是仅根据其上下文来预测掩码词的原始词汇表id。与从左向右的预训练的语言模型不同，MLM使表示能够融合左右上下文，这使我们能够对双向Transformer进行预训练。除了掩码语言模型，我们还使用了“预测下一个句子”任务，联合预训练文本对的表示。\nContributions：\n（1）证明了双向预训练对语言表示的重要性；\n（2）证明了预训练表示可以减少对许多精心设计的任务特定架构的需要，BERT是第一个基于微调的表示模型，它在大型句子级和标记级任务上取得了最优的效果。\nRelated Work Unsupervised Feature-based Approaches  学习广泛适用的单词表征一直是一个活跃的研究领域，包括非神经方法和基于神经网络的方法。预先训练好的单词embedding是现代自然语言处理领域的一个组成部分，与从头开始学习的embedding相比，它提供了显著的改进。为了对单词embedding向量进行预训练，我们使用了从左向右的语言建模目标，以及在左右上下文中区分正确和不正确单词的目标。\nUnsupervised Fine-tuning Approaches  最近，产生上下文标记表示的句子或文档编码器可以从未标记的文本中预训练得到，并针对有监督的下游任务进行了微调。这些方法的优点是很少需要从头开始学习参数。从左到右的语言建模和自动编码器已用于预训练这些模型。\nTransfer Learning from Supervised Data  有研究表明，可以从大型数据集的监督任务中进行有效转移，如自然语言推理和机器翻译。计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，其中有效的方法是对使用ImageNet预训练的模型进行微调。\nBERT  我们将在本节中详细的介绍BERT。在我们的框架中，共有两个步骤：预训练和微调。在预训练过程中，该模型在不同的预训练任务上对未标记的数据进行训练；为了进行微调，首先使用预先训练好的参数初始化BERT模型，并使用从下游任务中获得的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。下图是以问答为例的示意图： BERT的一个显著特点是针对不同任务使用统一框架，即预训练的架构和最终的下游任务架构之间的差别很小。\nModel Architecture：BERT的模型架构是一个多层双向Transformer编码器，不再详细介绍，详见transformer（Attention is all you need）。在该项目中，层数（transformer块）由$L$表示，隐藏层大小为$H$，自注意力的头数为$A$，我们的结果主要是在两个模型大小上的：$BERT_{BASE} (L=12,H=768,A=12)$和$BERT_{LARGE}(L=24,H=1024,A=16)$。 Input\/Output Representations：为了使BERT能够处理各种下游任务，我们的输入表示能够明确地在token序列中表示单个句子和一对句子（如：\u0026lt;Question，Answer\u0026gt;）。在该项目中，一个“sentence”可以是一个连续文本的任意跨度，而不是一个实际的语言句子；一个“sequence”指的是BERT的输入token序列，它可以是单个句子或两个句子放在一起的。\n使用WordPiece embedding，其中包含30,000个词汇token，每个序列的第一个token是特殊的分类标记[CLS]。与该token对应的最终隐藏状态被用作分类任务的序列表示（即用该token进行后续的分类任务）。句子对形成一个单一的序列。我们用两种方式来区分这些句子：首先，用一个特殊的标记[SEP]将它们分开；其次，我们在每个token中添加一个可学习的embedding，表明它是属于句子$A$还是句子$B$。如上图所示，我们将输入embedding记为$E$，最终隐藏向量的特殊token[CLS]记为$C$，第$i$个输入token的最终隐藏向量表示为$T_{i}$。\n对于一个给定的token，它的输入表示是通过将相应的token、段embedding和位置embedding相加来形成的，如下图所示。 Pre-training BERT  我们没有使用传统的从左到右或从右到左的语言模型对BERT进行预训练，我们使用两个无监督的任务对BERT进行预训练，如第一张图的左侧所示。",
      "inLanguage" : "en-US",
      "author" : "Gao dy",
      "creator" : "Gao dy",
      "publisher": "Gao dy",
      "accountablePerson" : "Gao dy",
      "copyrightHolder" : "Gao dy",
      "copyrightYear" : "2022",
      "datePublished": "2022-04-16 00:00:00 \u002b0000 UTC",
      "dateModified" : "2022-04-16 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gdy0924.github.io\/posts\/bert\/",
      "keywords" : [  ]
  }
</script>
<title>BERT</title>
  <meta property="og:title" content="BERT" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
原文链接：BERT
Abstract  引入了一种新的语言表示模型，称为BERT（Bidirectional Encoder Representations from Transformers）。与最近的语言表示模型不同，BERT是从未标记文本中预训练出的双向表示，通过联合所有层中的左右上下文。因此，预训练的BERT模型可以通过一个额外的输出层来进行微调，从而为广泛的任务创建最先进的模型。
Introduction  预训练的语言模型已被证明对许多自然语言处理任务都有效。目前有两种策略为下游任务应用预训练模型：基于特征的和微调的。基于特征的方法使用特定的任务框架，包括预训练的表示作为额外的特征。微调的方法引入最少的参数，通过简单的微调所有的预训练参数来训练下游任务。我们认为，目前的技术限制了预训练的表示，特别是对于微调方法，主要的限制是标准语言模型是单向的，这限制了在预训练过程中可以使用的架构的选择。
在本文，我们改进了基于微调的方法，提出了BERT（Bidirectional Encoder Representations from Transformers），其通过使用预训练的掩码语言模型（masked language model，MLM）减轻了前面提到的单向性约束。掩码语言模型从输入中随机掩码一些token，其目的是仅根据其上下文来预测掩码词的原始词汇表id。与从左向右的预训练的语言模型不同，MLM使表示能够融合左右上下文，这使我们能够对双向Transformer进行预训练。除了掩码语言模型，我们还使用了“预测下一个句子”任务，联合预训练文本对的表示。
Contributions：
（1）证明了双向预训练对语言表示的重要性；
（2）证明了预训练表示可以减少对许多精心设计的任务特定架构的需要，BERT是第一个基于微调的表示模型，它在大型句子级和标记级任务上取得了最优的效果。
Related Work Unsupervised Feature-based Approaches  学习广泛适用的单词表征一直是一个活跃的研究领域，包括非神经方法和基于神经网络的方法。预先训练好的单词embedding是现代自然语言处理领域的一个组成部分，与从头开始学习的embedding相比，它提供了显著的改进。为了对单词embedding向量进行预训练，我们使用了从左向右的语言建模目标，以及在左右上下文中区分正确和不正确单词的目标。
Unsupervised Fine-tuning Approaches  最近，产生上下文标记表示的句子或文档编码器可以从未标记的文本中预训练得到，并针对有监督的下游任务进行了微调。这些方法的优点是很少需要从头开始学习参数。从左到右的语言建模和自动编码器已用于预训练这些模型。
Transfer Learning from Supervised Data  有研究表明，可以从大型数据集的监督任务中进行有效转移，如自然语言推理和机器翻译。计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，其中有效的方法是对使用ImageNet预训练的模型进行微调。
BERT  我们将在本节中详细的介绍BERT。在我们的框架中，共有两个步骤：预训练和微调。在预训练过程中，该模型在不同的预训练任务上对未标记的数据进行训练；为了进行微调，首先使用预先训练好的参数初始化BERT模型，并使用从下游任务中获得的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。下图是以问答为例的示意图： BERT的一个显著特点是针对不同任务使用统一框架，即预训练的架构和最终的下游任务架构之间的差别很小。
Model Architecture：BERT的模型架构是一个多层双向Transformer编码器，不再详细介绍，详见transformer（Attention is all you need）。在该项目中，层数（transformer块）由$L$表示，隐藏层大小为$H$，自注意力的头数为$A$，我们的结果主要是在两个模型大小上的：$BERT_{BASE} (L=12,H=768,A=12)$和$BERT_{LARGE}(L=24,H=1024,A=16)$。 Input/Output Representations：为了使BERT能够处理各种下游任务，我们的输入表示能够明确地在token序列中表示单个句子和一对句子（如：&amp;lt;Question，Answer&amp;gt;）。在该项目中，一个“sentence”可以是一个连续文本的任意跨度，而不是一个实际的语言句子；一个“sequence”指的是BERT的输入token序列，它可以是单个句子或两个句子放在一起的。
使用WordPiece embedding，其中包含30,000个词汇token，每个序列的第一个token是特殊的分类标记[CLS]。与该token对应的最终隐藏状态被用作分类任务的序列表示（即用该token进行后续的分类任务）。句子对形成一个单一的序列。我们用两种方式来区分这些句子：首先，用一个特殊的标记[SEP]将它们分开；其次，我们在每个token中添加一个可学习的embedding，表明它是属于句子$A$还是句子$B$。如上图所示，我们将输入embedding记为$E$，最终隐藏向量的特殊token[CLS]记为$C$，第$i$个输入token的最终隐藏向量表示为$T_{i}$。
对于一个给定的token，它的输入表示是通过将相应的token、段embedding和位置embedding相加来形成的，如下图所示。 Pre-training BERT  我们没有使用传统的从左到右或从右到左的语言模型对BERT进行预训练，我们使用两个无监督的任务对BERT进行预训练，如第一张图的左侧所示。" />
  <meta name="description" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
原文链接：BERT
Abstract  引入了一种新的语言表示模型，称为BERT（Bidirectional Encoder Representations from Transformers）。与最近的语言表示模型不同，BERT是从未标记文本中预训练出的双向表示，通过联合所有层中的左右上下文。因此，预训练的BERT模型可以通过一个额外的输出层来进行微调，从而为广泛的任务创建最先进的模型。
Introduction  预训练的语言模型已被证明对许多自然语言处理任务都有效。目前有两种策略为下游任务应用预训练模型：基于特征的和微调的。基于特征的方法使用特定的任务框架，包括预训练的表示作为额外的特征。微调的方法引入最少的参数，通过简单的微调所有的预训练参数来训练下游任务。我们认为，目前的技术限制了预训练的表示，特别是对于微调方法，主要的限制是标准语言模型是单向的，这限制了在预训练过程中可以使用的架构的选择。
在本文，我们改进了基于微调的方法，提出了BERT（Bidirectional Encoder Representations from Transformers），其通过使用预训练的掩码语言模型（masked language model，MLM）减轻了前面提到的单向性约束。掩码语言模型从输入中随机掩码一些token，其目的是仅根据其上下文来预测掩码词的原始词汇表id。与从左向右的预训练的语言模型不同，MLM使表示能够融合左右上下文，这使我们能够对双向Transformer进行预训练。除了掩码语言模型，我们还使用了“预测下一个句子”任务，联合预训练文本对的表示。
Contributions：
（1）证明了双向预训练对语言表示的重要性；
（2）证明了预训练表示可以减少对许多精心设计的任务特定架构的需要，BERT是第一个基于微调的表示模型，它在大型句子级和标记级任务上取得了最优的效果。
Related Work Unsupervised Feature-based Approaches  学习广泛适用的单词表征一直是一个活跃的研究领域，包括非神经方法和基于神经网络的方法。预先训练好的单词embedding是现代自然语言处理领域的一个组成部分，与从头开始学习的embedding相比，它提供了显著的改进。为了对单词embedding向量进行预训练，我们使用了从左向右的语言建模目标，以及在左右上下文中区分正确和不正确单词的目标。
Unsupervised Fine-tuning Approaches  最近，产生上下文标记表示的句子或文档编码器可以从未标记的文本中预训练得到，并针对有监督的下游任务进行了微调。这些方法的优点是很少需要从头开始学习参数。从左到右的语言建模和自动编码器已用于预训练这些模型。
Transfer Learning from Supervised Data  有研究表明，可以从大型数据集的监督任务中进行有效转移，如自然语言推理和机器翻译。计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，其中有效的方法是对使用ImageNet预训练的模型进行微调。
BERT  我们将在本节中详细的介绍BERT。在我们的框架中，共有两个步骤：预训练和微调。在预训练过程中，该模型在不同的预训练任务上对未标记的数据进行训练；为了进行微调，首先使用预先训练好的参数初始化BERT模型，并使用从下游任务中获得的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。下图是以问答为例的示意图： BERT的一个显著特点是针对不同任务使用统一框架，即预训练的架构和最终的下游任务架构之间的差别很小。
Model Architecture：BERT的模型架构是一个多层双向Transformer编码器，不再详细介绍，详见transformer（Attention is all you need）。在该项目中，层数（transformer块）由$L$表示，隐藏层大小为$H$，自注意力的头数为$A$，我们的结果主要是在两个模型大小上的：$BERT_{BASE} (L=12,H=768,A=12)$和$BERT_{LARGE}(L=24,H=1024,A=16)$。 Input/Output Representations：为了使BERT能够处理各种下游任务，我们的输入表示能够明确地在token序列中表示单个句子和一对句子（如：&amp;lt;Question，Answer&amp;gt;）。在该项目中，一个“sentence”可以是一个连续文本的任意跨度，而不是一个实际的语言句子；一个“sequence”指的是BERT的输入token序列，它可以是单个句子或两个句子放在一起的。
使用WordPiece embedding，其中包含30,000个词汇token，每个序列的第一个token是特殊的分类标记[CLS]。与该token对应的最终隐藏状态被用作分类任务的序列表示（即用该token进行后续的分类任务）。句子对形成一个单一的序列。我们用两种方式来区分这些句子：首先，用一个特殊的标记[SEP]将它们分开；其次，我们在每个token中添加一个可学习的embedding，表明它是属于句子$A$还是句子$B$。如上图所示，我们将输入embedding记为$E$，最终隐藏向量的特殊token[CLS]记为$C$，第$i$个输入token的最终隐藏向量表示为$T_{i}$。
对于一个给定的token，它的输入表示是通过将相应的token、段embedding和位置embedding相加来形成的，如下图所示。 Pre-training BERT  我们没有使用传统的从左到右或从右到左的语言模型对BERT进行预训练，我们使用两个无监督的任务对BERT进行预训练，如第一张图的左侧所示。" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{font-family:bree serif,sans-serif;-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  

  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="XY&#39;s Blog">
  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Bungee+Shade" rel="stylesheet">
  
  

  
  
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/"
      >Gao dy</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/gdy0924" target="_blank">About</a>
  </div>
  
</div>
<div class="row end-xs">
  
  <div class="lang-switch col-xs-3 col-xs-offset-9">
    <a href="/en/">English</a>
  </div>
    
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">BERT</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2022-04-16 00:00:00 UTC">
                16 Apr 2022
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://gdy0924.github.io/">@Gao dy</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>
    原文链接：<a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a></p>
<h2 id="abstract">Abstract</h2>
<p>    引入了一种新的语言表示模型，称为BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers）。与最近的语言表示模型不同，BERT是从未标记文本中预训练出的双向表示，通过联合所有层中的左右上下文。因此，预训练的BERT模型可以通过一个额外的输出层来进行微调，从而为广泛的任务创建最先进的模型。</p>
<h2 id="introduction">Introduction</h2>
<p>    预训练的语言模型已被证明对许多自然语言处理任务都有效。目前有两种策略为下游任务应用预训练模型：基于特征的和微调的。基于特征的方法使用特定的任务框架，包括预训练的表示作为额外的特征。微调的方法引入最少的参数，通过简单的微调所有的预训练参数来训练下游任务。我们认为，目前的技术限制了预训练的表示，特别是对于微调方法，主要的限制是标准语言模型是单向的，这限制了在预训练过程中可以使用的架构的选择。<br>
    在本文，我们改进了基于微调的方法，提出了BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers），其通过使用预训练的掩码语言模型（masked language model，MLM）减轻了前面提到的单向性约束。掩码语言模型从输入中随机掩码一些token，其目的是仅根据其上下文来预测掩码词的原始词汇表id。与从左向右的预训练的语言模型不同，MLM使表示能够融合左右上下文，这使我们能够对双向Transformer进行预训练。除了掩码语言模型，我们还使用了“预测下一个句子”任务，联合预训练文本对的表示。<br>
    <em><strong>Contributions</strong></em>：<br>
    （1）证明了双向预训练对语言表示的重要性；<br>
    （2）证明了预训练表示可以减少对许多精心设计的任务特定架构的需要，BERT是第一个基于微调的表示模型，它在大型句子级和标记级任务上取得了最优的效果。</p>
<h2 id="related-work">Related Work</h2>
<h3 id="unsupervised-feature-based-approaches">Unsupervised Feature-based Approaches</h3>
<p>    学习广泛适用的单词表征一直是一个活跃的研究领域，包括非神经方法和基于神经网络的方法。预先训练好的单词embedding是现代自然语言处理领域的一个组成部分，与从头开始学习的embedding相比，它提供了显著的改进。为了对单词embedding向量进行预训练，我们使用了从左向右的语言建模目标，以及在左右上下文中区分正确和不正确单词的目标。</p>
<h3 id="unsupervised-fine-tuning-approaches">Unsupervised Fine-tuning Approaches</h3>
<p>    最近，产生上下文标记表示的句子或文档编码器可以从未标记的文本中预训练得到，并针对有监督的下游任务进行了微调。这些方法的优点是很少需要从头开始学习参数。从左到右的语言建模和自动编码器已用于预训练这些模型。</p>
<h3 id="transfer-learning-from-supervised-data">Transfer Learning from Supervised Data</h3>
<p>    有研究表明，可以从大型数据集的监督任务中进行有效转移，如自然语言推理和机器翻译。计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，其中有效的方法是对使用ImageNet预训练的模型进行微调。</p>
<h2 id="bert">BERT</h2>
<p>    我们将在本节中详细的介绍BERT。在我们的框架中，共有两个步骤：预训练和微调。在预训练过程中，该模型在不同的预训练任务上对未标记的数据进行训练；为了进行微调，首先使用预先训练好的参数初始化BERT模型，并使用从下游任务中获得的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。下图是以问答为例的示意图：
<img src="/img/bert1.PNG" alt="">
    BERT的一个显著特点是针对不同任务使用统一框架，即预训练的架构和最终的下游任务架构之间的差别很小。<br>
<strong>Model Architecture</strong>：BERT的模型架构是一个多层双向Transformer编码器，不再详细介绍，详见transformer（Attention is all you need）。在该项目中，层数（transformer块）由$L$表示，隐藏层大小为$H$，自注意力的头数为$A$，我们的结果主要是在两个模型大小上的：$BERT_{BASE} (L=12,H=768,A=12)$和$BERT_{LARGE}(L=24,H=1024,A=16)$。
<strong>Input/Output Representations</strong>：为了使BERT能够处理各种下游任务，我们的输入表示能够明确地在token序列中表示单个句子和一对句子（如：&lt;Question，Answer&gt;）。在该项目中，一个“sentence”可以是一个连续文本的任意跨度，而不是一个实际的语言句子；一个“sequence”指的是BERT的输入token序列，它可以是单个句子或两个句子放在一起的。<br>
    使用WordPiece embedding，其中包含30,000个词汇token，每个序列的第一个token是特殊的分类标记[CLS]。与该token对应的最终隐藏状态被用作分类任务的序列表示（即用该token进行后续的分类任务）。句子对形成一个单一的序列。我们用两种方式来区分这些句子：首先，用一个特殊的标记[SEP]将它们分开；其次，我们在每个token中添加一个可学习的embedding，表明它是属于句子$A$还是句子$B$。如上图所示，我们将输入embedding记为$E$，最终隐藏向量的特殊token[CLS]记为$C$，第$i$个输入token的最终隐藏向量表示为$T_{i}$。<br>
    对于一个给定的token，它的输入表示是通过将相应的token、段embedding和位置embedding相加来形成的，如下图所示。
<img src="/img/bert2.PNG" alt=""></p>
<h3 id="pre-training-bert">Pre-training BERT</h3>
<p>    我们没有使用传统的从左到右或从右到左的语言模型对BERT进行预训练，我们使用两个无监督的任务对BERT进行预训练，如第一张图的左侧所示。<br>
<strong>Task 1: Masked LM</strong><br>
    直观地说，深度双向模型会比左到右模型或从右到左的浅连接模型更强大。但是，标准的语言模型只能从左到右或从右到左进行训练，因为双向条件反射将允许每个词间接地“看到自己”，并且该模型可以简单地在多层上下文中简单地预测目标词。<br>
    为了训练一个双向表示，我们简单地随机掩码一些输入token，预测这些掩码token，我们将这个过程称为“掩码语言模型”（masked language model，MLM）。在这种情况下，与掩码token对应的最终隐藏向量被输入到一个softmax中，再进行输出。在我们的实验中，随机掩码每个序列中15%的token。<br>
    虽然这使我们获得一个双向的预训练模型，但会导致预训练和微调阶段出现不匹配的情况，因为[MASK]token在微调过程中并不会出现。为了缓解这种情况，我们并不总是用[MASK]token其替换被掩码的单词。训练数据生成器随机选择15%的token位置进行预测，如果选择了第$i$个token，则将第$i$个token替换为：（1）80%使用[MASK]token；（2）10%使用随机token；（3）10%不改变第$i$个token。然后，$T_{i}$用来通过交叉熵损失来预测原始token。<br>
<strong>Task 2: Next Sentence Prediction (NSP)</strong><br>
    许多重要的下游任务，如问答和自然语言推理，都是基于对两个句子之间关系的理解，这不是由语言模型直接捕获的。为了训练一个能够理解句子关系的模型，我们对“预测下一个句子”的任务进行了预训练，该任务可以从任何单语语料库中简单地生成。具体来说，当为每个预训练样本选择句子$A$和$B$时，50%为$A$后跟着的实际句子是$B$（标签为<em>IsNext</em>），50%为从语料库中的随机选择的一个句子（标签为<em>NotNext</em>）；如第一张图所示，$C$用于下一个句子预测(NSP)。尽管它很简单，但通过实验证明其非常有效。<br>
<strong>Pre-training data</strong><br>
    预训练模型在很大程度上遵循现有的语言模型，对于预训练语料库，我们使用BooksCorpus（800M个单词）和英语维基百科（2500M个单词）。</p>
<h3 id="fine-tuning-bert">Fine-tuning BERT</h3>
<p>    微调很简单，因为transformer中的自注意机制允许BERT通过改变输入和输出来建模许多下游任务——无论是单个文本还是文本对。对于文本对，一个常见的方式是在应用双向交叉注意力之前独立地对文本对进行编码，但BERT使用自注意力机制来统一这两个阶段，因为编码一个具有自注意力机制的文本对包含了两个句子之间的双向交叉注意力。<br>
    对于每个任务，我们只需将特定于任务的输入和输出传递给BERT，并端到端的调整所有参数即可。在输入中，预训练中的句子$A$和句子$B$类似于：（1）注释中的句子对；（2）假设-前提对；（3）问答中的问题-段落对；（4）文本分类或序列标记中的文本-$\phi $对。在输出中，token表征被输入进一个token-level任务的输出层，例如序列标记或问题回答；[CLS]表征被输入进分类的输出层，例如情感分析。</p>
<h2 id="conclusion">Conclusion</h2>
<p>    最近语言模型中的迁移学习的改进表明，丰富的、无监督的预训练是许多语言理解模型的一个重要组成部分，我们将预训练进一步推广到深度双向架构，允许预训练模型处理一组广泛的NLP任务。</p>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://joway.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>