<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on XY&#39;s Blog</title>
    <link>https://gdy0924.github.io/posts/</link>
    <description>Recent content in Posts on XY&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gdy0924.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MAE</title>
      <link>https://gdy0924.github.io/posts/reformer-%E5%89%AF%E6%9C%AC/</link>
      <pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/reformer-%E5%89%AF%E6%9C%AC/</guid>
      <description>Masked Autoencoders Are Scalable Vision Learners
Abstract  本文证明了掩码自动编码器(Masked Autoencoders, MAE)是一种可扩展的自监督学习模型，其方法很简单：随机mask输入图像的patch，并重建缺失的像素。首先，开发了一个非对称的encoder-decoder架构，encoder只对可见的patch进行处理（即没有mask的patch），一个轻量级的decoder，从潜在的表示和mask token种重构原始图像。其次，我们发现mask更高比例的输入图像（如75%），能够产生更有意义的自监督任务。
Introduction  基于GPT中的自回归语言建模和BERT中的mask自编码的解决方案在概念上很简单：它们mask部分数据，并学习预测被mask的内容。mask自编码的思想，是一种更一般的去噪自编码器的形式，也适用于计算机视觉。这会有个问题：是什么使mask的自编码在视觉和语言领域之间有所不同？
我们试图从以下角度来回答这个问题：
（1）到目前为止，两者的架构都是不同的：在视觉中，卷积在很长时间都占了主导地位。卷积通常在规则的网格上运行，将mask token和位置embedding等添加到卷积网络中并不简单，但是随着ViT的提出，该问题已经得到了解决；
（2）语言于视觉之间的信息密度是不同的：语言具有高度语义性和信息密集性，当训练一个模型预测句子中缺失的一些单词时，该任务可以诱导复杂的语言理解；相反，图像具有大量的空间冗余信息，如一个缺失的patch可以从相邻的patch中恢复，而很少有高度理解的部分；为了克服上述这种差异，我们提出mask非常多的patch，这种方式在很大程度上减少了冗余，创造了一个具有挑战性的自监督任务；
（3）自编码器中的decoder，将潜在的表示映射回输入端，在重建文本和图像之间起的作用不同：在视觉中，decoder重建像素，其输出的语义级别比较低；但是在语言中，预测包含丰富语义信息的单词，因此，对于图像，decoder的设计在确定学习到的语义水平中起着关键作用。
基于上述分析，我们提出了一种简单的、有效的、可扩展的mask自编码器（Masked Autoencoders,MAE），用于学习视觉表示。
MAE从输入图像中随机mask patch，并重建像素空间中mask掉的patch。非对称的encoder-decoder架构，其中，encoder只对未mask的patch进行操作，轻量级的decoder从潜在表示和mask token中重建输入，如下图所示： Related Work Masked language modeling  掩码语言模型和自回归模型是自然语言处理领域非常成功的方法，如：Bert、GPT。该方法mask输入中的某些内容，并训练模型来预测mask掉的内容，这种训练可以很好的推广到各种下游任务中。
Autoencoding  自编码中包含一个encoder和一个decoder，encoder将输入映射到潜在表示，decoder重构输入。MAE是去噪自编码的一种形式，但在许多方面与经典的DAE不同。
Masked image encoding  掩码图像编码从被掩码的图像中学习表示。
Self-supervised learning Approach  MAE是一种自编码模型，可以重建原始输入的部分内容。包含一个encoder，将输入映射到一个潜在的表示，一个decoder，从潜在的表示中重构原始输入；与原始的自编码器不同，我们设计的是非对称的结果，即encoder只对未mask的输入进行操作，轻量级的decoder从潜在表示和mask token中重建完整的输入。
Masking  与ViT相同，我们将输入图像分割成规则的patch，接着对patch进行随机采样，对其进行mask。该随机采样的比例很高，即被mask掉的patch比例很高，在很大程度上消除了冗余，从而创建了一个不容易通过从可见的相邻patch进行推断来解决的任务。
MAE encoder  encoder是一个ViT架构，但只应用于未mask的patch。与ViT相同，将patch embedding和位置embedding相加作为输入，然后通过一系列的transformer block来处理输入，但是，该encoder只处理整个输入图像的一小个子集。mask patch被移除了，这使得只是用一小部分的计算量和内存来训练一个大的encoder。完整的输入图像由轻量级的decoder来处理。
MAE decoder  MAE中decoder的输入是完整的token集合，包括encoder处理的token和mask token。每个mask token是一个共享的、可学习的向量，用来表示该patch是否需要预测。同时向完整的token集合中加入位置embedding，输入到另外的transformer block中。
MAE的decoder仅在预训练阶段用于实现像素重建，因此是独立于encoder的；并且decoder比encoder更窄更浅，计算量更小。通过这种不对称的设计，完整的token集合只由轻量级的decoder来处理，大大减少了预训练的时间。
Reconstruction target  MAE通过预测每个掩码patch的像素值类重建输入图像，encoder中输出的每个元素代表一个patch的像素向量，decoder的最后一层是一个线性层，其输出通道的数量等于一个patch中像素的数量。decoder的输出经过reshape，重建成原始图像。损失函数使用MSE，并且只计算mask patch上的损失。
我们还提出了一个变体，其重建目标是每个mask patch的归一化像素值，在实验中，使用归一化像素值作为重建目标，提高了数据表示的质量。</description>
    </item>
    
    <item>
      <title>Reformer</title>
      <link>https://gdy0924.github.io/posts/reformer/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/reformer/</guid>
      <description>REFORMER: THE EFFICIENT TRANSFORMER
原文链接：DeiT
上表为内存使用量和时间复杂度的比较，其中$l$为序列的长度，$b$为batch size的大小，$n_{h}$为头数，$n_{c}$为LSH的chunk数量，$n_{r}$为hash重复的次数。
Abstract  Transformer在许多任务上都实现了最先进的结果，但是其训练代价是很昂贵的，特别是在长序列上。因此，我们提出两种提高transformer的技术：（1）局部敏感哈希（locality-sensitive hashing），将复杂度从$O(L^2)$降低到$O(L log L)$；（2）使用可逆残差层而不是标准残差，使得在训练过程中只存储最后一层的激活值（激活函数的输入值），而不是N次，N为层数。
Introduction  基于transformer的大型模型甚至不能在单个GPU上进行微调，因为它们的内存需求需要多加速器硬件设置。transformer架构中的主要内存使用在以下几个方面：
（1）一个包含N层的模型，其内存比单层模型使用的内存大N倍，因为需要存储激活值以进行反向传播，即每一层的激活值都需要存储下来；
（2）transformer block中的全连接层FFN中的深度往往比$d_{model}$大得多，如：FFN的输入维度为512，那么中间的维度可能是2048，这也占了内存的很大部分；
（3）注意力机制在计算序列长度为$L$的序列时，在计算和内存的复杂度上都是$O(L^2)$。
我们提出以下方法来解决上述问题：
（1）可逆层：使得整个网络只用存储最后一层的激活值，而不是所有层，之前层的激活值可以通过最后一层推算出来；
（2）在FFN层中分割激活值，将他们以快的形式进行处理；
（3）基于局部敏感哈希的近似注意力，将复杂度从$O(L^2)$降低到$O(L log L)$。
LOCALITY-SENSITIVE HASHING ATTENTION Dot-product attention  transformer中使用点乘计算注意力，输入是维度为$d_{k}$的query和key，以及维度为$d_{v}$的value，计算公式如下： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$
Multi-head attention  使用可学习的矩阵$W_{Q}$、$W_{K}$和$W_{V}$来处理query、key和value。
Memory-efficient attention  假设Q、K、V的大小都是$L×d_{model}$，那么$QK^{T}$的计算结果的大小就是$L×L$的，当序列长度很长时，是需要一定的内存去存储的，这就限制了transformer的使用。
Where do Q, K, V come from?  Q、K、V是将输入经过三个不同的线性层来得到的。对于LSH注意力模型，我们希望query和keys（Q和K）是相同的，这可以通过一个使用同一个线性层来实现，这种方式称为shared-QK Transformer，并经过实验验证，这种方式并不会影响性能。
Hashing attention  对于LSH注意力模型，从Q=K和V开始，大小为$L×d_{model}$，并根据之前的公式，主要问题在于$QK^{T}$部分的计算。但是，我们最终是只关心softmax的结果，并且softmax是由最大元素来主导的。也就是说，对于每个query$q_{i}$，我们只需要关注在K中最接近$q_{i}$的那些key。比如：假设长度为64K，对于每个$q_{i}$，我们只考虑一个小子集，32或64个最接近的key的集合，这样就可以减少计算量，那么如何在K中找到“最近的邻居”呢？
Locality sensitive hashing  在高维空间中快速寻找最近邻的问题可以通过局部敏感哈希(LSH)来解决：使用一种哈希函数$H(x)$，使得两个个相似度很高的数据以较高的概率映射成同一个hash值，而令两个相似度很低的数据以极低的概率映射成同一个hash值，该函数称为局部敏感哈希。
我们通过随即投影来实现，如下图。为了得到，我们先创建一个随机矩阵，大小为$[d_{k},b/2]$，然后定义$h(x)=argmax([xR;-xR])$，其中$[u;v]$表示两个向量的连接。 在上图中，为角度局部敏感哈希，使用球投影点的随即旋转角度，在上图的二维描述中，上边一行的两个点x和y不太可能在一个中hash buckets，而在下边一行中的x和y的球投影是彼此接近的。
LSH attention 上图为LSH attention中：hash-bucketing、排序、chunking分块和计算attention结果的简化描述；下图中的(a)-(d)是步骤中的矩阵形式：</description>
    </item>
    
    <item>
      <title>DeiT</title>
      <link>https://gdy0924.github.io/posts/deit/</link>
      <pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/deit/</guid>
      <description>Training data-efficient image transformers &amp;amp; distillation through attention
原文链接：DeiT
Abstract  最近，基于注意力机制的神经网络用于计算机视觉领域，但是这些网络需要大量数据进行预训练，从而限制了其使用。
在该论文中，我们参考ViT网络，引入一种蒸馏方式，使得student网络可以通过注意力机制从teacher网络中进行学习。
Introduction  Vision Transformer（ViT）使用自注意力机制用于图像分类，以图像补丁作为输入，但是transformer架构在数据量不足的训练下不能很好地泛化，而这些模型的训练涉及大量的计算资源。
在本文中，我们提出DeiT网络，引入了一个一种基于token的策略，contributions：
（1）我们的方法证明使用不包含卷积层的神经网络可以在没有外部数据的情况下，可以与在ImageNet数据集上最好结果的模型相比较，并且我们提出的模型DeiT-S和DeiT-Ti有很少的参数；
（2）引入了一种新的基于蒸馏标记的蒸馏方式：复制由teacher网络得到的label，与类标记作用相同，这两种label相互作用以提高精度。
Related work Image Classification  虽然有许多尝试使用transformer来实现图像分类，但其性能不如卷积网络。最近，Vision Transformer（ViT）没有使用任何卷积，达到了卷积的水平，然而，ViT需要大量的数据进行预训练，才能使transformer架构有效。在本文中，不需要大型训练数据集，也实现了强大的性能，即只使用Imagenet1k数据集。
Transformer architecture Knowledge Distillation  知识蒸馏，由Hinton提出，是指student网络利用来自强大的teacher网络中得到的“soft”标签进行训练。soft标签（如：0.2，0.3，0.5）是softmax函数的输出，而不是hard标签（如：0，0，1），这种训练方式提高了student网络的性能（或者它可以看作是将teacher网络压缩成一个更小的模型——student网络）。一方面，teacher的soft标签将有类似于标签平滑的效果；另一方面，teacher的监督考虑了数据增强的影响：例如，一张很大的场景下有一只小猫缩在角落，其标签为猫，如果对其进行crop操作，这只猫就不会在crop后的图中了，这也改变了图片的标签。利用teacher网络中的soft标签可以用一种soft的方式将偏差转移到student网络中，例如：使用卷积网络作为teacher，在transformer中引入由卷积得到的偏差，是可以提升网络性能的。
Vision transformer: overview Multi-head Self Attention layers (MSA) $$ Attention(Q,K,V)=Softmax(\frac{QK^{T}}{\sqrt{d}})V $$ 其中，$Q=XW_{Q}$，$K=XW_{K}$，$V=XW_{V}$
Transformer block for images  Transformer中，MSA后跟着前馈网络（FFN），由两个线性层组成，中间有GeLU非线性激活函数，第一个线性层将维数从$D$扩展到$4D$，第二层将维数从$4D$减少为$D$。
ViT将图片当作序列一样处理，将图像分成大小为$16×16$的patch，作为transformer的输入，并结合位置信息（position embedding）。
The class token  可训练的向量，在第一层附加在patch tokens中，该向量通过transformer，最后用一个线性层来预测类别。这种架构迫使自注意力机制在patch tokens和class token之间传播信息：在训练时，监督信号只来自class token，patch tokens是模型唯一的变量输入。
Fixing the positional encoding across resolutions  使用较低分辨的图像对网络进行预训练，使用较高分辨率的图像对网络进行微调，可以加快训练。当增加输入图像的分辨率时，我们保持patch大小不变，那么输入patch的数量就会发生变化。采用插值法对位置embedding进行调整。</description>
    </item>
    
    <item>
      <title>总结（一）</title>
      <link>https://gdy0924.github.io/posts/%E6%80%BB%E7%BB%93%E4%B8%80/</link>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/%E6%80%BB%E7%BB%93%E4%B8%80/</guid>
      <description>简单点总结 图像分类 AlexNet：ReLU；Dropout。
VGG：堆叠的小卷积增大感受野，代替大卷积核（加深网络）；全卷积网络（输入任意尺寸的图像）。
GoogleNet：Inception模块（融合不同尺度的特征）；辅助分类器（解决更深网络中的梯度回传问题）。
ResNet：残差网络；shortcut连接。（极大的增加网络深度）
ResNeXt：split-transform-merge思想；将分组卷积引入ResNet（减少计算量）。
ShuffleNet：分组卷积（减少计算量与内存使用量）；channel shuffle操作。
目标检测 R-CNN：two-stage；Selective Search；多次卷积；SVM+regression。
Fast R-CNN：two-stage；Selective Search；一次卷积；softmax+regression。
Faster R-CNN：two-stage；RNP网络（解决Selective Search算法时间长问题）；RPN与特征提取共享卷积层；引入anchor（一个中心点9个锚框）；softmax+regression。
YOLO：one-stage；全卷积网络；图像分为S×S的网格，一个网格预测两个bounding-box和一个对象类别；最终的通道数对应每个网格要预测的所有数值。
SSD：one-stage；全卷积网络；default box（anchor思想，一个中心点5个锚框，解决YOLO一个网格只能预测一个对象的问题）；多尺度融合，即每种分辨率大小的特征图都经过一个detector后融合。
图像分割 FCN：全卷积网络；转置卷积提高分辨率；融合不同分辨率的特征（add）。
SegNet：全卷积网络；反池化+卷积操作提高分辨率（相比转置卷积减少内存使用与计算量）。
U-Net：encoder-decoder架构；转置卷积提高分辨率；skip连接融合特征（concat）。
Attention U-Net：在U-Net的基础上，引入attention gate（突出重点区域）。
DeepLab：空洞卷积（扩大感受野但不降低分辨率）；空间金字塔池架构（融合不同尺度的特征）；在v3+中加入一个decoder，融合浅层特征，形成encoder-decoder架构。
SETR：encoder-decoder结构；用transformer中的encoder提取特征，patch embedding+位置 embedding（相比CNN，感受野更大，捕获更全局的信息）；提出三种不同的decoder提高分辨率（线性插值）。
SegFormer：encoder-decoder结构；修改SETR结构；Overlapped Patch Merging（overlap保留局部特征，patch merging在每个stage生成分辨率不同的特征）；Efficient Self-Attention（减少自注意力模块中的计算量与参数量）；Mix-FFN添加一个卷积层（引入相对位置信息，取消位置embedding）；完全基于MLP的decoder（计算简单）。
图像分类 LeNet AlexNet 1、使用ReLU非线性激活：缓解梯度消失问题；
2、多GPU训练：放在2个GPU上进行训练，在特定的层进行通信交流；
3、LRN：局部响应标准化(Local Response Normalization)；
4、Overlapping Pooling：池化窗口大小和步长不同，缓解过拟合问题；
5、Dropout：随机的丢弃一些神经元，以防止过拟合。
VGG 1、堆叠的小卷积代替大卷积：两个3×3大小的感受野堆叠起来的感受野与一个5×5大小的感受野相同，三个3×3大小的感受野堆叠起来的感受野与一个7×7大小的感受野相同。用小卷积多次引入非线性变换，并且减少网络参数。（使网络更深）
2、全卷积网络：将最后的全连接层换成卷积层，可以输入任意尺寸大小的图片。
GoogleNet 提出Inception结构：保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。通过不同分支融合不同尺度的特征，1×1卷积实现通道数的减少。
辅助分类器：解决梯度回传的问题，额外添加两个辅助的softmax用于向前传导梯度。
后续改进：堆叠小卷积代替大卷积，加入BN，残差连接。
ResNet  当更深层次的网络能够开始收敛时，出现网络退化的问题，即随着网络深度的增加，精度达到饱和，然后迅速下降。
提出Residual Block：（如果增加的层可以构造成恒等映射，那么更深的模型的训练误差应该不大于对应较浅的模型）引入Shortcut连接，即$y=F(x)+x$，左边是网络层数较少时使用（18，34），右边用于更深的网络（50，101，152）；1×1卷积实现通道数的改变，降维减少计算量；add需要保证特征维度相同，使用1×1卷积实现Shortcut路径上特征尺寸和通道数的改变。 ResNeXt  在Residual Block中引入Inception Block的思想（split-transform-merge），即将输入的特征split，然后进行卷积操作，最后在merge合并（分组卷积的思想）： ShuffleNet  轻量级网络，通过改进深度可分离卷积，降低计算量与参数量。
ResNeXt中，在分组卷积之后会跟着1×1的卷积，来融合不同通道的信息（深度可分离卷积也是同样），此时的1×1卷积相对来说计算量比较大，因此也考虑使用分组卷积。但是连续使用分组卷积会导致各组之间没有信息融合，因此引入channel shuffle操作。下图中a是连续使用几次分组卷积的结果，b表示将上一层中每个组中的通道划分为几个子组，然后向下一层中的每个组提供不同的子组，c为channel shuffle操作。 ShuffleNet Block：引入残差连接和深度可分离卷积思想（a）；将1×1卷积替换为分组卷积并引入channel shuffle操作（b）；通道拼接代替元素相加，进一步减少计算量（c）。 目标检测  目标检测方法主要分为两类：two-stage和one-stage。two-stage是先由算法生成一系列作为样本的候选框，再通过卷积神经网络进行样本分类，在检测准确率和定位精度上占优，如R-CNN系列；ne-stage则不用产生候选框，直接将目标边框定位的问题转化为回归问题处理，在算法速度上占优，如YOLO，SSD等。</description>
    </item>
    
    <item>
      <title>SegFormer</title>
      <link>https://gdy0924.github.io/posts/segformer/</link>
      <pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/segformer/</guid>
      <description>原文链接：SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers
Abstract  我们提出了一个SegFormer，将transformer与多层感知器(MLP)解码器统一起来。SegFormer有两个特点：（1）其包括一个层次架构transformer编码器，输出多尺度特征，不需要位置编码，从而避免当测试分辨率与训练不同时，位置编码插值会导致性能下降；（2）避免了复杂的解码器，使用的MLP解码器融合来自不同层的信息，从而结合了局部和全局的注意力机制。将该方法进行扩展，获得了从SegFormer-B0到SegFormer-B5的一系列模型，达到了比以前的模型更好的性能和效率。
Introduction  ViT将transformer用于图像分类，将一个图像分割成多个线性embedding的patch，并将其输入到一个带有位置embedding的transformer中，在ImageNet上取得了令人印象深刻的性能。在语义分割中，SETR的提出证明了transformer的可行性。
SETR采用ViT作为backbone，并合并了几个CNN解码器来扩大特征分辨率。尽管ViT性能良好，但也有一些局限性：
（1）ViT输出单尺度低分辨率特征，而不是多尺度特征；
（2）它在大图像上的计算成本较高。
为了解决这些限制，提出了一种金字塔视觉transformer(pyramid vision Transformer，PVT)，这是一种具有金字塔结构的ViT的扩展，用于密集预测。PVT在目标检测和语义分割方面比ResNet有相当大的改进。
本文介绍了SegFormer，一个前沿的语义分割transformer框架，同时考虑了效率、准确性和鲁棒性。与以前的方法相比，我们的框架重新设计了编码器和解码器。
Contribution：
（1）一种新的无位置编码的分层transformer编码器；
（2）一种轻量级的仅用MLP作为解码器的设计，没有复杂的计算；
（3）在三个公开的语义分割数据集上的效率、准确性和鲁棒性方面都达到了最优。
首先，该编码器在对分辨率与训练图像不同的图像进行预测时，避免了位置编码插值，因此，该编码器可以很容易地适应任意分辨率的测试图像，而不影响性能；其次，分层结构使编码器能够同时生成高分辨率的细粒度特征和低分辨率的粗粒度特征，这与ViT只能生成固定分辨率的特征图形成了对比；最后，我们提出了一个轻量级的MLP解码器，其关键思想是利用transformer的内在特点，即较低层的注意力倾向于局部信息，而最高层的注意力是全局的的。通过融合来自不同层的信息，MLP解码器同时结合了局部和全局的信息。
Related Work Semantic Segmentation  语义分割可以看作是图像分类从图像级到像素级的一种扩展，最早的语义分割模型为FCN，一个全卷积的网络，以端到端的方式进行像素到像素的分类。之后，一些修改：扩大感受野；细化上下文信息；引入边界信息；引入注意力模块等。最近的方法（SETR）已经证明了基于transformer的架构对语义分割任务的有效性，但是，这些方法仍然需要大量计算。
Transformer backbones  ViT是第一个完全利用transformer在图像分类中达到最优性能的工作；随后，DeiT进一步探索了一种数据高效的训练策略和一种蒸馏方法；PVT是第一个在transformer中引入金字塔结构的工作；Swin、CvT、CoaT、LeViT和Twins等方法增强了特征的局部连续性，去掉了固定大小的位置embedding，提高了transformer在密集预测任务中的性能。
Transformers for specific tasks  DETR是第一个使用transformer构建一个无NMS的端到端的目标检测框架。在语义分割方面，SETR采用ViT作为backbone来提取特征，取得了令人印象深刻的性能。然而，这些基于transformer的方法效率非常低，因此很难在实时应用程序中部署。
Method 如上图所示，主要包括两部分：
（1）分层transformer编码器：生成高分辨率粗粒度特征和低分辨率细粒度特征；
（2）轻量级的MLP解码器：融合多层特性，生成最终的语义分割结果。
首先给定一个大小为$H×W×3$的图像，我们首先将其分成大小为$4×4$的patch，与ViT使用大小为$16×16$的patch不同，使用较小的patch更有利于密集预测任务；然后，将这些patch作为分层transformer编码器的输入，以获得原始图像分辨率的{1/4、1/8、1/16、1/32}的多层次特征；将这些多层次特征传递给MLP解码器，以预测$\frac{H}{4}\times \frac{W}{4}\times N_{cls}$分辨率下的分割结果，其中$N_{cls}$是类别的数量。
Hierarchical Transformer Encoder  我们设计了一系列的Mix Transformer编码器(MiT)，MiT-B0到MiT-B5，具有相同的架构，但大小不同：MiT-B0用于快速预测的轻量级模型，而MiT-B5是获得最佳性能的最大模型。MiT的设计受ViT的启发，但为语义分割任务定制与优化。
Hierarchical Feature Representation  与ViT只能生成单分辨率的特征图不同，该模块的目标是，给定一个输入图像，生成类似CNN的多层次特征图。这些特性图同时提供了高分辨率的粗粒度特征和低分辨率的细粒度特征。具体来说，给定一个分辨率为$H×W×3$的输入图像，通过patch 合并（patch merging）获得层次化的特征图$F_{i}$，每个特征图的分辨率为$\frac{H}{2^{i+1}}\times \frac{W}{2^{i+1}}\times C_{i}$，其中$i\in \lbrace 1,2,3,4 \rbrace$，$C_{i+1}$ 比$C_{i}$大。</description>
    </item>
    
    <item>
      <title>SETR</title>
      <link>https://gdy0924.github.io/posts/setrcode/</link>
      <pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/setrcode/</guid>
      <description>Decoder naive 1class SETR_Naive(SegmentationTransformer): 2 def __init__( 3 self, 4 img_dim, 5 patch_dim, 6 num_channels, 7 num_classes, 8 embedding_dim, 9 num_heads, 10 num_layers, 11 hidden_dim, 12 dropout_rate=0.0, 13 attn_dropout_rate=0.0, 14 conv_patch_representation=False, 15 positional_encoding_type=&amp;#34;learned&amp;#34;, 16 ): 17 super(SETR_Naive, self).__init__( 18 img_dim=img_dim, 19 patch_dim=patch_dim, 20 num_channels=num_channels, 21 embedding_dim=embedding_dim, 22 num_heads=num_heads, 23 num_layers=num_layers, 24 hidden_dim=hidden_dim, 25 dropout_rate=dropout_rate, 26 attn_dropout_rate=attn_dropout_rate, 27 conv_patch_representation=conv_patch_representation, 28 positional_encoding_type=positional_encoding_type, 29 ) 30 31 self.num_classes = num_classes 32 self._init_decode() 33 34 #两个（1×1卷积+BN+ReLU）+上采样 35 def _init_decode(self): 36 self.</description>
    </item>
    
    <item>
      <title>SETR</title>
      <link>https://gdy0924.github.io/posts/setr/</link>
      <pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/setr/</guid>
      <description>原文链接：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers
Abstract  最近的语义分割方法采用encoder-decoder结构的全卷积网络：编码器逐步降低分辨率，并捕捉更抽象的特征和更大的感受野。但是由于上下文信息对于分割任务很重要，最近的研究集中在通过空洞卷积或引入注意力模块来增加感受野，不过都是在encoder-decoder结构中进行修改。
在本文中，我们将语义分割作为一个序列到序列的预测任务。具体来说：提出一个纯Transformer，即没有卷积和降低分辨率，将图像编码为一系列patch，在Transformer的每一层都使用全局上下文信息。该encoder可以与一个简单的decoder结合，提供一个强大的分割模型，称为SEgmentation TRansformer（SETR）。
Introduction  标准的全卷积网络具有encoder-decoder架构：encoder用于捕捉特征，decoder对由编码器产生的特征进行像素级分类。其中，encoder由堆叠的卷积层组成，特征图的分辨率逐渐降低，encoder能够学习更多的抽象特征，并且逐渐增大感受野。但是存在一些限制，即不能学习远程的依赖信息，感受野有限对于语义分割任务是不好的。
为了克服上述的限制，一种方法是直接修改卷积操作：大的卷积核大小，空洞卷积和图像/特征金字塔结构；另一种方法是将注意力模块引入FCN架构中：旨在建模特征图中所有像素的全局交互信息。当应用于语义分割任务时，一种常见的设计是将注意力模块与FCN体系结构相结合，并将注意层放置在顶部。针对上述两种方法，标准的encoder-decoder架构是不变的。最近，开始尝试摆脱卷积，关注纯注意力模型，但是，即使没有卷积，也不会改变模型结构的本质：encoder（下采样）-decoder（上采样）架构。
在本文中，我们提出用纯Transformer代替基于堆叠卷积层的编码器，从而产生一种新的分割模型，称为SEgmentation TRansformer（SETR）。该Transformer编码器将输入图像视为图像patch序列，并使用全局自注意力机制对该序列进行转换，用于特征学习。具体地说：首先将一个图像分解成一个固定大小的patch，形成patch序列；然后，对每个patch的像素向量应用线性变换层，得到一系列特征embedding向量作为Transformer的输入；得到Transformer输出的特征，使用decoder来恢复原始图像的分辨率。图像分辨率没有下采样，在Transformer的每一层都进行全局上下文建模。（灵感来源于ViT）
Contributions：
（1）将图像分割问题作为一个序列到序列的预测任务，以替换全卷积网络；
（2）利用Transformer框架，实现纯注意力机制的特征表示encoder；
（3）进一步提出了三种不同复杂性的decoder架构。
Related work Semantic segmentation  最近的许多研究都集中在解决有限的感受野/上下文信息建模问题：DeepLab引入空洞卷积，PSANet提出用于动态捕捉远程上下文的空间注意力模块；DANet提出空间注意力和通道注意力。所有这些方法都基于FCNs，特征编码和提取部分都是基于经典卷积网络，如VGG和ResNet。在该工作中，我们从不同的角度重新思考语义分割任务。
Transformer  AANet将卷积和自注意力机制集中训练，LRNet探索局部自注意力，以避免全局自注意力带来的大量计算，DETR使用Transformer进行目标检测，其中Transformer被附加在检测头中，STTR和LSTR分别采用Transformer进行视差估计和车道形状预测。最近，ViT是第一个纯Transformer图像分类模型，达到了最优的结果，为语义分割任务中使用纯Transformer的编码器设计提供了灵感。
Method FCN-based semantic segmentation  在基于FCN的语义分割网络中，encoder由堆叠的卷积层组成：第一层以图像作为输入，大小为$H×W×3$；后续层的输入是一个大小为$h×w×d$的三维张量，由于卷积操作的局域性，感受野随层数的增加而增加，取决于卷积核核的大小（通常是3×3）。因此，只有具有大感受野的更高层次才能在这个FCN架构中捕捉更多的上下文信息。然而，一旦达到一定深度，增加更多层的优势会迅速减少。因此，对上下文建模的感受野有限是经典FCN结构的限制。
最近，一些研究发现，将FCN与注意机制相结合是学习上下文信息的一种有效的策略。这些方法为例减少计算量，将注意力机制限制在较小尺寸的更高层次的特征图中，导致缺乏对较低层次特征图的学习，为了克服这一限制，我们提出了一种完全基于自注意力机制的编码器，称为SEgmentation TRansformers（SETR）。
Segmentation transformers (SETR) Image to sequence  SETR遵循NLP的输入-输出形式，具体地，Transformer，如上图的（a）所示，以一维的特征embedding序列$Z\in \mathbb{R} ^{L\times C}$作为输入，$L$为序列的长度，$C$为隐含通道的大小。因此，图像序列化需要将输入图像$x\in \mathbb{R} ^{H\times W\times 3}$转换为$Z$。
图像序列化的一种简单方法是将图像像素值flatten为一个大小为$3HW$的一维向量，对于大小为$480×480×3$的典型图像，得到的向量长度将为$691200$。考虑到Transformer的二次复杂度，这种高维向量不可能在空间和时间上同时被处理。因此，将每个像素作为转换器的输入是不可能的。
最终，决定设置Transformer的输入序列长度$L$为$\frac{H}{16}\times \frac{W}{16}=\frac{HW}{256}$。为了获得$\frac{HW}{256}$长的输入序列，我们将一个图像$x\in \mathbb{R} ^{H\times W\times 3}$均匀地划分成为$\frac{H}{16}\times \frac{W}{16}$个patch，每个patch的大小为$16×16$，然后将patch flatten为一个序列。利用线性变换函数$f:p\rightarrow e\in \mathbb{R} ^{C}$，将每个向量化的patch进一步映射到潜在的高维embedding空间，得到图像$x$的一维patch embedding序列。为了对patch的空间信息进行编码，每个位置$i$学习一个特定的embedding $p_{i}$，以形成最终的输入序列：$E=\lbrace e_{1}+p_{1},e_{2}+p_{2},&amp;hellip;,e_{L}+p_{L} \rbrace$。</description>
    </item>
    
    <item>
      <title>Attention U-Net</title>
      <link>https://gdy0924.github.io/posts/attention-u-net/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/attention-u-net/</guid>
      <description>Attention U-Net：Learning Where to Look for the Pancreas
原文链接：Attention U-Net
Abstract  我们提出了一种用于医学图像的attention gate（AG）模型，该模型可以自动学习如何关注不同形状和大小的目标对象。使用AGs训练的模型可以隐式的学习在输入图像中抑制不相关的区域，同时突出显示对特定任务有用的显著特征。AGs可以很容易地集成到CNN架构中，如U-Net，以最小的计算开销，同时提高模型的灵敏度和预测精度。
Introduction  由于人工、密集地标记大量医学图像是一项繁琐且容易出错的任务，自动医学图像分割在图像分析领域得到了广泛的研究。
随着卷积神经网络(CNNs)的出现，全卷积网络(FCNs)和U-Net是两种常用的架构。尽管这些结构具有良好的代表性，但当目标器官在形状和大小上表现出较大的差异时，这些结构依赖于多阶段的级联CNN架构。级联框架提取一个感兴趣的区域(ROI)，并对该特定的ROI做出密集的预测。
然而，这种方法导致了计算资源和模型参数的过度和冗余使用，例如：级联内的所有模型都重复提取类似的低级特征。为了解决这个普遍的问题，我们提出了一个简单而有效的解决方案，即attention gates(AGs)。
带有AGs的CNN模型可以以类似于FCN模型的训练标准方式从头开始训练，并且AGs自动学习在没有额外监督的情况下关注目标对象。此外，它们不引入显著的计算开销，也不需要像在多模型框架的情况下那样需要大量的模型参数。所提出的AGs通过抑制无关区域的特征，提高了模型对密集标签预测的灵敏度和准确性。这样，就可以在保持高预测精度的同时，消除使用额外定位模型的必要性。
在本文中，我们提出了一种新的自注意门控模块，可用于语义分割中的密集标签预测。此外，我们还探讨了AGs对医学图像分析的好处，特别是在图像分割的背景下。
Contributions：
（1）我们提出了基于网格的门控模块，可以使注意力系数更特定于局部区域，与基于全局特征向量的门控模块相比，提高了性能；
（2）首次在医学图像的CNN中使用Soft Attention，该模块可以替代分类任务中的Hard attention和器官定位任务中的定位模块；
（3）将U-Net改进为Attention U-Net，增加了模型对前景像素的敏感度，并设计实验证明了这种改进是通用的。
Related Work Attention Gates  一般AG会被用在自然图像分析、知识图谱、图像描述、机器翻译和分类任务上。Attention机制最开始是通过解释输出类别分数相对于输入图像的梯度来探索注意力图。可训练的Attention机制根据设计可分为Hard attention和Soft attention。Hard attention主要是由迭代区域候选和区域裁剪，通常是不可微的，因此会使模型很难训练，通常依赖强化学习的其他参数；Soft attention是基于概率的，可以正常反向传播梯度。逐通道的attention可以突出某些维度的特征，效果极好(如SENet)；注意力机制可以消除对外部信息（external gating information）的依赖。
Methodology Fully Convolutional Network (FCN)  全卷积网络已经成为医学图像的基准模型，如FCN、U-Net、DeepMedic等，其效果要优于传统的方法，主要是由于：SGD优化，卷积核的参数共享、卷积操作很好地捕捉到了医学图像中的结构信息。卷积层通过逐层捕捉局部信息，逐步提取高维图像表示$x^{l}$，最终，最终在高维空间的离散像素会具有语义信息和巨大的感受野。在第$l$层的输出处，依次通过一个线性变换和一个非线性激活函数，得到了特征图$x^{l}$。
在本文中，我们在一个标准的U-Net架构之上加入注意力模型。U-Net网络的优势主要是与在多个图像尺度上提取图像特征有关。粗粒度的特征图能够捕获上下文信息，并突出显示前景对象的类别和位置。在多个尺度上提取的特征图随后将通过skip连接进行合并，以结合粗粒度级别和细粒度级别的密集预测，如下图所示： Attention Gates for Image Analysis  为了捕获足够大的感受野，从而获取上下文信息，在标准的卷积网络中，特征图的尺寸经过下采样逐渐变小。这样，在粗粒度特征图中的特征才能以全局的尺寸建模位置和关系。但是对于小物体，形变程度大，很难降低false-positive预测。为了提高精度，当前的分割框架依赖于额外的目标定位模型，将任务简化为单独的定位和分割两步骤。其实，通过在一个标准的CNN模型中集成注意门(AGs)来也可以实现。这不需要训练多个模型和大量额外的模型参数，AG会抑制无关背景区域中的特征响应，而无需在网络之间裁剪ROI。
注意力系数（attention coefficients），$\alpha_{i}\in [0,1]$，是为了突出显著的图像区域和抑制任务无关的特征响应，以只保留与特定任务相关的激活，如下图所示。 AGs的输出是输入特征图和注意力系数的元素级相乘（对应元素逐个相乘）：$\hat{x}_ {i,c}^{l}=x_ {i,c}^{l}\cdot \alpha_{i}^{l}$。通常，为每个像素向量$x_{i}^{l}\in \mathbb{R}^{F_{l}}$计算一个注意力标量，其中，$F_{l}$对应于第$l$层中特征图的通道数。在多语义标签类中，我们提出使用多维注意力系数。因此，每个AG都学习专注于目标对象的一个子集，如下图所示： 每个像素$i$使用门控向量$g_{i}\in \mathbb{R}^{F_{g}}$来决定聚焦区域。门控向量包含上下文信息，以修剪低级特征响应。在本文中，选择使用加法注意力机制，相比于乘法注意力，其效果更好，公式如下： $$ q_{att}^{l}=\psi^{T}(\sigma_{1}(W_{x}^{T}x_{i}^{l}+W_{g}^{T}g_{i}+b_{g}))+b_{\psi} $$ $$ \alpha_{i}^{l}=\sigma_{2}(q_{att}^{l}(x_{i}^{l},g_{i};\Theta_{att})) $$ 其中，$\sigma_{2}$是sigmoid激活函数，AG由一组参数$\Theta_{att}$表示，包括：线性变换$W_{x} \in \mathbb{R}^{F_{l}\times F_{int}}$，$W_{g} \in \mathbb{R}^{F_{g}\times F_{int}}$，$\psi \in \mathbb{R}^{F_{int}\times 1}$和偏置项$b_{\psi} \in \mathbb{R}$，$b_{g} \in \mathbb{R}^{F_{int}}$。线性变换是通过1x1卷积来实现的。</description>
    </item>
    
    <item>
      <title>DeepLab系列</title>
      <link>https://gdy0924.github.io/posts/deeplab%E7%B3%BB%E5%88%97code/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/deeplab%E7%B3%BB%E5%88%97code/</guid>
      <description>V1 1import torch 2import torch.nn as nn 3import torch.nn.functional as F 4 5# 使用torch的nn模块中BatchNorm/在encoding文件中定义的BatchNorm 6try: 7 from encoding.nn import SyncBatchNorm 8 _BATCH_NORM = SyncBatchNorm 9except: 10 _BATCH_NORM = nn.BatchNorm2d 11_BOTTLENECK_EXPANSION = 4 12 13#卷积+BN+非线性激活 14class _ConvBnReLU(nn.Sequential): 15 BATCH_NORM = _BATCH_NORM 16 def __init__(self, in_ch, out_ch, kernel_size, stride, padding, dilation, relu=True): 17 super(_ConvBnReLU, self).__init__() 18 self.add_module( 19 &amp;#34;conv&amp;#34;,nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, dilation, bias=False), 20 ) 21 self.add_module(&amp;#34;bn&amp;#34;, _BATCH_NORM(out_ch, eps=1e-5, momentum=0.999)) 22 23 if relu: 24 self.</description>
    </item>
    
    <item>
      <title>DeepLab系列</title>
      <link>https://gdy0924.github.io/posts/deeplab%E7%B3%BB%E5%88%97/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/deeplab%E7%B3%BB%E5%88%97/</guid>
      <description>V1 SEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRFS 原文链接：DeepLab V1
Abstract  深度卷积神经网络(DCNNs)最近在视觉任务中显示出了最先进的性能，如图像分类和目标检测。该工作结合了DCNNs和概率图形模型的方法，用于解决像素级分类的任务，即语义分割。我们通过将DCNN的最后一层与一个全连接的条件随机场(CRF)相结合，克服了深度网络的定位能力弱的特性。
Introduction  将DCNN应用在语义分割任务中有两个挑战：降采样，空间“不敏感”（不变性）。降采样是指在网络每层重复进行最大池和降采样（步长）导致图像分辨率降低；而通过“空洞”卷积算法，能够有效地密集计算DCNN响应。
第二个问题是指从分类器获得以对象为中心的预测需要对空间转换的不变性，这限制了DCNN模型的空间准确性。我们通过使用全连接条件随机场(CRF)，提高了模型捕捉细节的能力。在我们的工作中，我们证明了当与基于DCNN的像素级分类器相结合时，它会有最先进的结果。
DeepLab三个主要优势是：
（1）速度：利用“空洞卷积”算法；
（2）精度：我们获得了最先进的结果；
（3）简单：我们的系统是由两个相当完善的模块组成，DCNNs和CRFs。
Convolutional Neural Networks For Dense Image Labeling Efficient Dense Sliding Window Feature Extraction With Hole Algorithm 利用空洞卷积算法有效地提取密集滑动窗口特征
密集的空间结果有助于特征提取器的成功，作为实现这一目标的第一步，我们将VGG-16的全连接层转换为卷积层，并在原始分辨率的图像上以卷积方式运行网络。然而，这是不够的，因为它产生非常稀疏计算的检测结果（步幅为32像素，即下采样32倍）。为了使下采样变为8倍，修改为：删除最后两个max pooling层（或者stride修改为1），由于max pooling的修改，影响了其后的卷积层，使其视野分别下降了 2x 和 4x，为了保持其原来的视野，便将其改成空洞卷积，dilation 分别为 2 和 4，这种方法普遍适用，并允许我们在任何下采样中有效地计算密集的CNN特征映射，而不引入任何其他的参数。
训练时将预训练的VGG16的权重做fine-tune，损失函数取是输出的特征图与ground truth下采样8倍做交叉熵和；测试时取输出图双线性上采样8倍得到结果。我们使用简单的双线性插值来将最终的特征图的分辨率提高8倍，而计算成本可以忽略不计。FCN网络中没有使用空洞卷积算法，并且在CNN输出处产生非常粗糙的特征图（下采样率为32倍），这迫使他们使用学习的上采样层（转置卷积），显著增加了他们系统的复杂性和训练时间。
CONTROLLING THE RECEPTIVE FIELD SIZE AND ACCELERATING DENSE COMPUTATION WITH CONVOLUTIONAL NETS 利用空洞卷积控制感受野的大小，加速密集计算
最近的基于DCNN的图像识别方法依赖于在图像集大规模分类任务上预先训练的网络。这些网络通常具有较大的感受野大小：对于我们考虑的VGG-16网络，它的感受野是224×224（零填充），如果网络被卷积应用，则为404×404像素。在将网络转换为完全卷积层后，第一个全连接层大小为7×7×4096，成为我们密集预测结果的计算瓶颈。
我们通过对第一个FC层（通过简单的抽取）进行空间下采样到4×4（或3×3）的空间大小来解决这个实际问题。这将网络的感受野减少到128×128（零填充）或308×308（卷积模式），并将第一FC层的计算时间减少了2−3倍。具体来说：给定一个306×306输入图像，它在测试期间以大约8帧/秒的速度在网络顶部产生39×39密集的原始特征分数。训练期间的速度是3帧/秒。我们还成功地尝试了将全连接层的通道数从4096个减少到1024个，在不牺牲性能的情况下，大大减少了计算时间和内存占用。
Detailed Boundary Recovery：Fully-Connected Conditional Random Fields and Multi-Scale Prediction 详细的边界恢复：全连接条件随机场和多尺度预测</description>
    </item>
    
    <item>
      <title>空洞卷积</title>
      <link>https://gdy0924.github.io/posts/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF/</guid>
      <description>原文链接：MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS
参考链接：如何理解空洞卷积（dilated convolution）？
吃透空洞卷积(Dilated Convolutions)
Dilated Convolution  空洞卷积（膨胀卷积/扩张卷积），最初的提出是为了解决图像分割的问题，常见的图像分割算法通常使用池化层和卷积层来增加感受野(Receptive Filed)，对图像进行特征提取，同时也缩小了特征图尺寸，然后再利用上采样还原图像尺寸，特征图缩小再放大的过程造成了精度上的损失。因此需要一种操作可以在增加感受野的同时保持特征图的尺寸不变，从而代替下采样和上采样操作。
在VGG网络中，提出了关键一点：1个 7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加，而利用卷积的叠加可以大幅度的减少参数，有正则化的效果，参数少了就没那么容易发生过拟合。这也是现在绝大部分基于卷积的深层网络都在用小卷积核的原因，常常使用3 x 3的卷积核。
神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征。在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积，即在卷积核中添加间距。
空洞卷积引入了一个称为 “扩张率(dilation rate)”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。
如下图所示，其中：（a）为3x3大小的1-dilated conv，与普通的卷积操作一样；（b）为3x3的2-dilated conv，实际的卷积核大小还是3x3，但是空洞为1，也就是对应一个7x7的图像patch，只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过，也可以理解为卷积核的大小为7x7，但是只有图中的9个点的权重不为0，其余都为0；（c）为4-dilated conv操作。 空洞卷积的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv。
相比于上采样和池化层，主要缺点有：内部数据结构丢失；空间层级化信息丢失；小物体信息无法重建（假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建），而空洞卷积都可以避免上述问题。
潜在的问题 The Gridding Effect  如果仅仅多次叠加 dilation rate为2 的 3 x 3 kernel 的话，则会出现这个问题： 从上图可以看出 kernel 并不连续，也就是并不是所有的 pixel 都用来计算了，因此这里将信息看做棋盘格（checker-board）的方式会损失信息的连续性，这对 pixel-level dense prediction 的任务来说是致命的。</description>
    </item>
    
    <item>
      <title>SegNet</title>
      <link>https://gdy0924.github.io/posts/segnet/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/segnet/</guid>
      <description>原文链接：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
Abstract  提出了一种新的深度全卷积神经网络结构，用于像素级语义分割，称为SegNet：由一个编码器和一个解码器，以及一个分类层组成。其中，编码器的结构与VGG-16中的13个卷积层完全相同；解码器用于将低分辨率特征图映射到与输入分辨率相同的特征图，以进行像素级分类。
SegNet创新之处是：利用解码器对其较低分辨率的特征图进行上采样。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引来进行非线性上采样。得到的上采样的映射是稀疏的，然后与可训练的卷积核进行卷积，以生成密集的特征映射。
Introduction  语义分割有广泛的应用，从场景理解、推断对象之间的支持关系到自动驾驶。最近的一些方法尝试直接采用用于类别预测的深度神经网络结构来进行像素级标记，但结果不够精确。这主要是因为最大池化和下采样降低了特征图的分辨率。因此，SegNet的动机是将低分辨率特征图映射到与输入分辨率相同以进行像素级分类。这种映射必须产生对精确的边界定位有用的特征。
SegNet中的编码器与VGG16中的卷积层相同，删除了VGG16末端的全连接层。SegNet的关键组成部分是解码器网络，由逐层的编码器组成，每个编码器都对应着一个解码器。其中，解码器使用从对应的编码器传递过来的最大池化索引，对其输入的特征图进行非线性上采样。
在解码过程中重用最大池化索引有几个优点：（1）改进边界划分；（2）减少参数数量；（3）这种形式的上采样可以兼容任何的编码器-解码器结构，只需要很少的修改。
Relative Work  深度卷积神经网络在图像分类方面的成功，许多研究者尝试将用于分类的网络应用于分割。更新的深度架构专门为分割而设计，通过卷积编码和映射低分辨率的图像表示到像素级预测，提高了最先进的技术。
全卷积网络（FCN）中的每个解码器对其输入特征图进行上采样，并将它们与相应的编码器特征图结合起来，以产生到下一个解码器的输入。通过对FCN添加一个递归神经网络（RNN），并在大数据集上对其进行微调，进一步提高了FCN的预测性能：RNN层模拟了CRFs清晰的边界划分能力，同时利用了FCN的特征表示能力。
多尺度融合的深度网络架构也在不断被提出，有两种类型：（1）使用几个不同尺度的输入图像和相应的深度特征提取网络；（2）组合来自单个深度架构的不同层的特征图。常见的想法是：使用在多个尺度上提取的特征来提供局部和全局上下文信息，并且使用早期编码层的特征图以保留更多的细节信息，从而生成更清晰的预测边界。
我们的工作受到无监督特征学习架构的启发，其关键的模块是一个编码器-解码器网络。编码器通过卷积、非线性激活、最大池化和下采样来获得特征图，同时在池化过程中，最大位置的索引被存储并传递给解码器。解码器通过使用存储的集合索引来对特征图进行上采样：它使用一个可训练的卷积操作对这些上采样映射进行卷积，以重建输入图像。
Architecture SegNet由一个编码器网络和一个相应的解码器网络，以及最后一个像素级分类层组成，如上图所示。编码器网络由13个卷积层组成，分别对应VGG16网络中的前13个卷积层。因此，我们可以从大数据集上训练的权值来初始化训练过程。删除全连接层以便编码器的最终输出保留更高分辨率的特征图。每个编码器层都具有与其相对应的解码器层，因此该解码器网络也具有13层。最终的解码器输出被输入到一个softmax分类器，以预测每个像素的类别概率。
编码器网络中的每个编码器进行卷积操作，以生成特征图，接着是Batch Normalization、ReLU和2×2大小（步幅为2）的最大池化（非重叠窗口）。随着下采样，图像分辨率降低，不利于分割时的边界预测，因此，在进行下采样之前，需要在编码器特征图中捕获和存储边界信息。如果推理过程中的内存不受约束，那么所有的编码器中的特征图都可以被存储，但通常不会这样实现。因此我们提出了一种方法：即只存储最大池化索引，即为每个编码器特征图记忆每个池化窗口中最大特征值的位置。原则上，这可以对每个2×2池窗口使用2位来完成。虽然这种较低的内存存储会导致轻微的精度损失，但仍然适用于实际应用。
解码器使用从相应的编码器层获取到的最大池化索引对其输入特征图进行上采样，以生成稀疏特征图，如下图所示。然后将这些特征图进行卷积，以生成密集的特征图。解码器与其对应的编码器，它们的通道数也是相同的，除了第一个编码器对应的解码器。第一个编码器的输入通道数是3，而其对应的解码器（最后一个）通道数对应的是类别的数量。 与SegNet相比，U-Net（用于医学领域）不重用池化索引，而是将整个特征图（以更多内存为代价）转移到相应的解码器，并将它们连接到上采样（通过反卷积）解码器特征图。另一方面，SegNet使用来自VGG网的所有预训练的卷积层权值作为预训练的权值。
Decoder Variants  总设计了8中不同的解码器变体架构，分别如下：
（1）Bilinear-Interpolation：使用固定的双线性插值权值的上采样方法，即不需要学习的上采样；
（2）SegNet-Basic： a smaller version of SegNet，4个encoder和4个decoder；
（3）SegNet-Basic-EncoderAddition：在解码器部分添加上相对应的编码器部分的特征图；
（4）SegNet-Basic-SingleChannelDecoder：在SegNet中，解码器对特征图进行上采样之后，会进行卷积操作，其通道数与对应编码器的通道数相同，对其变形就是设置通道数为单通道；
（5）FCN-Basic：与SegNet-Basic共享相同的编码器网络，但在解码器中使用反卷积进行上采样；
（6）FCN-Basic-NoAddition：不将编码器中的特征图添加到解码器中；
（7）FCN-Basic-NoDimReduction：在编码器中不进行降维操作；
（8）FCN-Basic-NoAddition-NoDimReduction：既不在编码器中进行降维，也不将编码器的特征图添加到解码器中。
上述8中变形结构的预测结果如下表所示，利用全局精度G、类平均精度C、平均交并比mIoU和语义轮廓度量BF来评价性能： Analysis  为了比较不同的解码器变体的定量性能，我们使用三个常用的性能度量：全局精度G，类平均精度C和平均交并比mIoU，计算语义轮廓得分BF就是评估f1-score，其中包括计算精度和召回率。
从上表中，我们可以看到，基于双线性插值的没有任何学习的上采样在所有精度指标上表现最差，而其余所有学习到的上采样（FCN-Basic及其变体）或上采样后的进行卷积（SegNet-Basic及其变体）的方法都表现得明显更好，强调了学习解码器的必要性。
当比较SegNet-Basic和FCN-Basic时，我们可以看到两者在这个测试的所有准确性测量上都表现得同样好。不同之处在于，SegNet在推理过程中使用了更少的内存，因为它只存储最大池索引；另一方面，FCN-Basic需要存储编码器中的特征图，会消耗更多的内存。同时，相比于使用了降维的FCN-Basic，SegNet-Basic中的特征维度会更多，因此，FCN-Basic的推理速度会更快。从另一个角度来看，SegNet-Basic中的解码器网络使其总体上成为一个比FCN-Basic更大的网络，这赋予了它更多的灵活性，因此在相同的迭代次数下获得了比FCN-Basic更高的训练精度。总的来说，当推理时间内存受到限制，但推理时间可能在一定程度上受损时，SegNet-Basic比FCN-Basic有优势。
SegNet-Basic在解码器方面与FCN-Basic-NoAddition最相似，两者都学会了生成密集的特征图，要么通过反卷积，要么通过上采样接着卷积。SegNet-Basic的性能更优越，部分原因是它的解码器尺寸更大。FCN-Basic-NoAddition的准确性低于FCN-Basic，这表明，捕获编码器特征图中的信息是至关重要的。
FCN-Basic-NoAddition-NoDimReduction模型的大小比SegNet-Basic略大，因为最终的编码器特征图没有被降维以匹配类别的数量。该模型的测试性能和训练精度都不如SegNet-Basic，这表明使用一个更大的解码器是不够的，但捕获编码器的特征图信息是很重要的，特别是细粒度的轮廓信息。
总的来说：
（1）当编码器特征图可以完全存储时，能够获得最佳性能。这一点最清楚地反映在语义轮廓度量(BF)中；
（2）当推理过程中的内存受到限制时，就可以存储压缩形式的编码器特征图（降维、最大池索引），并与适当的解码器一起使用，以提高性能；
（3）更大的解码器可以提高给定的编码器网络的性能。
Conclusion  我们提出了SegNet，一种用于语义分割的深度卷积网络架构。我们对SegNet及其它变体进行了分析，以揭示设计分割架构所涉及的实际权衡，特别是训练时间、内存和准确性。那些完全存储编码器网络特征图的体系结构表现最好，但在推理期间需要消耗更多的内存。另一方面，SegNet更有效，因为它只存储特征图的最大池化索引，并将其用于自己的解码器网络中，以获得良好的性能。</description>
    </item>
    
    <item>
      <title>U-Net</title>
      <link>https://gdy0924.github.io/posts/u-netcode/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/u-netcode/</guid>
      <description>1import torch 2import torch.nn as nn 3import torch.nn.functional as F 4 5#下采样卷积block（3×3卷积+3×3卷积） 6class UNetConvBlock(nn.Module): 7 def __init__(self, in_chans, out_chans, padding, batch_norm): 8 super(UNetConvBlock, self).__init__() 9 block=[] 10 11 block.append(nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=int(padding)) 12 block.append(nn.ReLU()) 13 14 if batch_norm : 15 block.append(nn.BatchNorm2d(out_chans)) 16 17 block.append(nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=int(padding)) 18 block.append(nn.ReLU()) 19 20 if batch_norm: 21 block.append(nn.BatchNorm2d(out_chans)) 22 23 self.block = nn.Sequential(*block) 24 25 def forward(self, x): 26 out = self.block(x) 27 return out 28 29#上采样block 30class UNetUpBlock(nn.</description>
    </item>
    
    <item>
      <title>U-Net&amp;U-Net&#43;&#43;</title>
      <link>https://gdy0924.github.io/posts/u-netu-net&#43;&#43;/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/u-netu-net&#43;&#43;/</guid>
      <description>U-Net: Convolutional Networks for Biomedical Image Segmentation
原文链接：U-Net
Abstract  深度网络的成功训练通常需要数千个带标签的训练样本。在本文中，我们提出了一种网络和训练策略，通过数据增强来更有效地使用可用的带标签样本。该网络结构由两条路径组成，一个是捕捉上下文的contracting path，另一个是能够实现精确定位的对称的expanding path组成。通过实验证明，这种网络可以从很少的图像中实现端到端的训练。
Introduction  在过去的两年里，深度卷积网络在许多视觉识别任务中都表现出色。但由于可用的训练集的大小和网络的大小，卷积网络的成功受到了限制。卷积网络的典型应用是在分类任务中，对图像的输出是一个单一的类别标签。然而，在许多视觉任务中，特别是在生物医学图像处理中，所需的输出应该包括定位，即为每个像素分配一个类标签。同时，大规模的训练图像在生物医学任务中通常是无法获得的。因此，有工作在滑动窗口设置中训练一个网络，通过在像素周围的局部区域（patch）作为输入来预测每个像素的类标签。首先，这个网络可以定位。其次，在patch方面的训练数据远远大于训练图像的数量。
显然，上述方法有两个缺点。首先，非常慢，因为网络必须为每个patch单独运行，而且由于重叠的patch，会有很多冗余。其次，在定位精度和上下文的使用之间存在一种权衡。较大的补丁需要更多的最大池化层，从而降低了定位精度，而较小的补丁只允许网络看到很少的上下文。
在本文中，我们提出了一个新的网络结构，即所谓的“全卷积网络”。我们对FCN结构进行修改与扩展，使其可以在很少的训练图像下运行，并产生更精确的分割，如下图所示，其主要思想是通过连续的层来添加一个contracting network，其中池化操作被上采样操作所取代。因此，这些层可以提高输出的分辨率。为了实现定位，将从contracting path获得的高分辨率特征图与上采样的输出相结合。 在该结构中，一个重要的修改是，在上采样部分，有大量的特征通道，这样可以将上下文信息传播到更高分辨率的层。因此，expanding path与contracting path是对称的，并形成一个u形结构。该网络没有全连接层，并且卷积层。为了预测图像边界区域中的像素，通过镜像输入图像来引入缺失的上下文。
由于可用的训练数据很少，我们通过对可用的训练图像进行数据增强。许多细胞分割任务中的另一个挑战是分离同一类别的接触对象（即细胞间的连接处），因此使用加权损失，即接触细胞单元之间的分离背景标签在损失函数中的权重很大。
Network Architecture  网络架构如上图所示所示。它由contracting path（左侧）和expanding path（右侧）组成。contracting path遵循卷积网络的典型架构：包括连续两个3x3的卷积，接着是通过ReLU和2x2的最大池化层（下采样），每经过一次下采样，特征通道数将增加一倍。expanding path中的每个block都先对对特征图的进行2倍的上采样，同时将特征通道数量减半，再与contracting path中相应裁剪的特征图进行concat拼接操作，接着是两个3x3卷积（跟着ReLU），这次卷积不改变图像大小。在最后一层，使用1x1的卷积将每个64个分量的特征向量映射到所需的类的数量，该网络总共有23个卷积层。（在上采样的过程中会丢失部分语义特征，通过拼接的方式，可以恢复部分的语义信息，从而保证分割的精度）
网络的输入是一张$572×572$的边缘经过镜像操作的图片，contracting path的输出是$32×32$大小的特征图，expanding path通过反卷积将特征图的尺寸扩大两倍，同时将其通道数数减半，和左侧对称的contracting path的特征图进行拼接，由于左侧contracting path和右侧expanding path的特征图的尺寸不一样，U-Net通过将contracting path的特征图裁剪到和expanding path相同尺寸的特征图。
Overlap-Tile  该策略的思想是：对图像的某一块像素点（黄框内部分）进行预测时，将原始的大图像分割成patch，patch的边缘在原图中可能不是边缘，需要该图像块周围的像素点（蓝色框内）提供上下文信息（context），以获得更准确的预测，如下图所示，预测部分为黄框，输入区域为蓝框。
这样的策略会带来一个问题，图像边界的图像块没有周围像素，因此周围像素采用了镜像扩充，如下图所示，白色框边缘外边部分为镜像填充的部分。
这样的操作会带来图像重叠问题，即第一块图像周围的部分会和第二块图像重叠。因此作者在卷积时只使用有效部分（valid part of each convolution），不使用padding，虽然卷积的时候会用到周围的像素点（蓝色框内），但最终传到下一层的只有中间原先图像块（黄色框内）的部分，并且使最终的输出图像对应要真是预测的部分（也可理解为连续的零填充会带来一些误差）。
Training  利用输入的图像及其对应的分割图，通过随机梯度下降训练网络。为了最小化开销并最大限度地利用GPU内存，我们支持大型输入块而不是大批处理大小，因此将批处理减少为单个图像。因此，我们使用了一个高动量（0.99）。
损失函数通过Softmax和交叉熵损失函数相结合， $$ E=\sum_{x\in \Omega }w(x)log(p_{l(x)}(x)) $$ 其中，$l:\Omega \rightarrow \lbrace 1,&amp;hellip;,K \rbrace$为每个像素的真实标签，$w:\Omega \rightarrow \mathbb{R}$是引入的权重系数，在训练过程中赋予边界像素更大的重要性。
我们预先计算出针对每个ground truth分割的权重图，以使训练数据集中某一类像素的权重更大，并迫使网络学习我们在接触细胞单元之间引入的小分离边界，如下图是的c和d： （上图：用显微镜记录玻璃上的HeLa细胞。（a）原始图像；（b）ground truth：不同的颜色表示HeLa细胞的不同实例；（c）生成的分割掩码：白色为前景，黑色为背景；（d）像素级的损失权重图，以迫使网络学习边界像素，越靠近红色，权重越大。）</description>
    </item>
    
    <item>
      <title>Upsampling</title>
      <link>https://gdy0924.github.io/posts/upsampling/</link>
      <pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/upsampling/</guid>
      <description>参考链接：上采样的几种方法
图像语义分割中，编码器通过卷积层得到图像的一些特征，但是解码器需要将该特征还原到原图像的尺寸大小，才可以对原图像的每个像素点进行分类。从一个较小尺寸的矩阵进行变换，得到较大尺寸的矩阵，在这个过程就是上采样。
插值法  插值法不需要学习任何的参数，只是根据已知的像素点对未知的点进行预测估计，从而可以扩大图像的尺寸，达到上采样的效果。
转置卷积（反卷积） 参考链接：47 转置卷积【动手学深度学习v2】 抽丝剥茧，带你理解转置卷积（反卷积） 原始卷积  直观的理解就是固定大小的卷积核在原始的输入图像进行滑动，通过矩阵加权计算得到输出特征，如下图所示： 但是实际在计算机中计算的时候，并不是像这样一个位置一个位置的进行滑动计算，因为这样的效率太低了。计算机会将卷积核转换成等效的矩阵，将输入转换为向量。通过输入向量和卷积核矩阵的相乘获得输出向量。输出的向量经过整形便可得到我们的二维输出特征。具体的操作如下图所示，由于我们的3x3卷积核要在输入上不同的位置卷积4次，所以通过补零的方法将卷积核分别置于一个4x4矩阵的四个角落。这样我们的输入可以直接和这四个4x4的矩阵进行卷积，而舍去了滑动这一操作步骤。 进一步，将输入拉成长向量，四个4x4卷积核也拉成长向量并进行拼接，如下图所示： 记向量化的图像为$I$，向量化的卷积矩阵为$C$，输出特征向量为$O$，则有： $$ I^{T}×C=O^{T} $$ 上述过程将一个$1×16$的行向量乘以$16×4$的矩阵，得到了$1×4$的行向量。那么反过来将一个$1×4$的向量乘以一个$4×16$的矩阵应该就能得到一个$1×16$的行向量，这就是转置卷积的思想。
转置卷积  对应上面公式，我们有转置卷积的公式： $$ O^{T}×C^{T}=I^{T} $$ 需要注意的是这两个操作并不是可逆的，对于同一个卷积核，经过转置卷积操作之后并不能恢复到原始的数值，保留的只有原始的形状。
代码 1import tensorflow as tf 2 3x = tf.reshape(tf.constant([[1,2], 4 [4,5]],dtype=tf.float32), [1, 2, 2, 1]) 5kernel = tf.reshape(tf.constant([[1,2,3], 6 [4,5,6], 7 [7,8,9]],dtype=tf.float32), [3, 3, 1, 1]) 8transpose_conv = tf.nn.conv2d_transpose(x, kernel, output_shape=[1, 4, 4, 1], strides=[1,1,1,1], padding=&amp;#39;VALID&amp;#39;) 9sess = tf.Session() 10print(sess.run(x)) 11print(sess.run(kernel)) 12print(sess.run(transpose_conv)) 示例  当padding=0，stride=1时，具体例子计算如下：  在原始的卷积操作中，加入padding，会使输出比无填充时尺寸更大，相反，在转置卷积中，加入padding会使输出更小，如下图所示（具体来说，就是在计算完成得到输出后，padding为多少，就把输出结果的几圈去掉）：  在原始的卷积操作中，加入stride，会使输出比无填充时尺寸更小，相反，在转置卷积中，加入stride会使输出更大，如下图所示（stride对应的为补0操作，并且是在内部行列之间进行补0）： 反池化（UnPooling）  max pooling的逆操作，即在max pool的时候记录一个池化的索引，就是记录取得的这个值是在哪个位置。在反池化的时候，输入的值根据索引放回原来的位置，其余位置填0，如下图所示： </description>
    </item>
    
    <item>
      <title>FCN</title>
      <link>https://gdy0924.github.io/posts/fcncode/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/fcncode/</guid>
      <description>1class FCNs(nn.Module): 2 def __init__(self, num_classes, backbone=&amp;#34;vgg&amp;#34;): 3 super(FCNs, self).__init__() 4 self.num_classes = num_classes 5 #VGG为提取特征的网络 6 if backbone == &amp;#34;vgg&amp;#34;: 7 self.features = VGG() 8 #转置卷积(输入通道数，输出通道数，卷积核大小，步长，零填充将添加到输入中每个维度的两侧，在输出形状的每个尺寸的一侧添加的附加大小) 9 #每次上采样2倍，共上采样5次 10 # deconv1 1/16 11 self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1) 12 self.bn1 = nn.BatchNorm2d(512) 13 self.relu1 = nn.ReLU() 14 15 # deconv1 1/8 16 self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1) 17 self.bn2 = nn.BatchNorm2d(256) 18 self.relu2 = nn.</description>
    </item>
    
    <item>
      <title>FCN</title>
      <link>https://gdy0924.github.io/posts/fcn/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/fcn/</guid>
      <description>FCN：Fully Convolutional Networks for Semantic Segmentation
原文链接：FCN
Abstract  卷积网络是一种能够产生特征层次结构的视觉模型，我们证明了通过卷积网络，经过端到端的训练，其性能超过了语义分割领域最先进的方法。建立一个“全卷积”网络，该网络可以接受任意大小的输入，并通过有效的推理和学习产生相应大小的输出。我们定义并详细描述了全卷积网络的结构，并与之前的模型建立了联系。我们将目前的分类网络（AlexNet，VGGnet和GoogLeNe）修改为全卷积网络，并通过微调将其学习到的表示转移到分割任务中。然后，形成了一种新的架构，它将深层、粗层的语义信息与浅层的信息结合起来，以产生准确和详细的分割。
Introduction  卷积网络正在推动图像识别技术的进步，起不仅在图像分类方面得到了改进，而且在具有结构化输出的任务上也取得了进展，包括bounding-box目标检测、部分和关键点的预测等。在从粗略推理到精细推理的过程中，很自然的下一步是对每个像素进行预测。先前的方法使用卷积网络进行语义分割，其中每个像素都标记其封闭对象或区域的类别，但这项工作有缺点。
我们提出了一个全卷积网络(FCN)，端到端进行训练，实现像素到像素的语义分割。据我们所知，这是第一个端到端的训练FCNs：（1）进行像素级预测，（2）来自监督预训练。现有网络的全卷积版本可以预测来自任意大小的输入的密集输出。通过前馈计算和反向传播对全图像进行学习和推理，网络内的上采样层能够在具有下采样池的网络中进行像素级预测和学习。
语义分割面临着语义和位置之间的内在紧张关系：全局信息解决了什么这一问题，而局部信息解决了在哪里这一问题。深度特征层次结构在一个局部-全局的金字塔结构中联合编码位置和语义信息，我们定义了一种新的“skip”结构，将深、粗的语义信息和浅、细的位置信息结合起来（）。
Related work  我们的方法利用了深度图像分类网络和迁移学习。首先是在各种视觉识别任务上进行转移，然后在目标检测领域，以及在混合分类器模型的实例和语义分割上。我们现在重新设计并微调分类网络，以实现语义分割。
Fully convolutional networks Dense prediction with convnets  最近的一些工作已经将卷积网络应用于密集的预测问题，这些方法的共同之处在于：限制和感受野的小模型，逐patch训练，通过超像素投影、随机场正则化、过滤或局部分类，由OverFeat引入用于密集输出的输入移位和输出交错，多尺度金字塔处理，tanh非线性激活。与这些现有的方法不同，我们采用和扩展了深度分类网络架构，使用图像分类作为有监督的预训练，并进行全卷积的微调，以简单有效地从整个图像输入和整个图像ground truth中学习。
Fully convolutional networks  卷积网络中的每一层都是一个大小为$h×w×d$的三维数组，其中$w$和$h$是空间维度，$d$是特征维度（通道数）。卷积神经是建立在平移不变性之上的，它们的基本组件（卷积、池化和激活函数）作用于局部输入区域，并且只依赖于相对的空间坐标。如果将某一层中位置$(x,y)$的数据向量表示为$x_{ij}$，$y_{ij}$为下一层的数据，其计算公式为： $$ y_{ij}=f_{ks}(\lbrace x_{si+\delta i,sj+\delta j} \rbrace _ {0\leq \delta i,0\leq \delta j}) $$ 其中，$k$为卷积核大小，$s$为步长，$f_{ks}$定义了层的类型，卷积或平均池化的矩阵乘法，最大池化，或激活函数等等。
接下来，我们将解释如何将分类网转换为生成粗略特征图的全卷积网络。对于像素级预测，我们需要将这些粗略输出连接回像素。
Adapting classifiers for dense prediction  典型的识别网络，包括LeNet、AlexNet及其后续的网络，接受固定大小的输入并产生输出。这些网络的全连接层具有固定的尺寸，放弃空间坐标。然而，这些全连接层也可以被视为：与覆盖其整个输入区域的内核进行的卷积操作（即卷积大小为整个输入大小）。这样做可以将它们转换成完全卷积的网络，以接受任何大小的输入和输出，如下图所示，得到的最终特征图就相当于对特定输入patch的预测 卷积模型的空间输出图使其可以实现语义分割等密集预测的任务，在每个输出单元上都有ground truth，前向和后向传播都是简单的，并且两者都利用了卷积固有的计算效率。
虽然我们将分类网络重新修改为全卷积网络，从而可以将任何大小的输入生成输出特征图，但输出维度通常通过下采样实现减少。分类网络的下采样是为了保持卷积核小和计算量的合理性，这使得分类网络的全卷积版本的输出非常粗糙。
Upsampling is backwards strided convolution  另一种连接粗略输出到密集像素的方法是插值。例如，简单的双线性插值通过计算距离最近的四个输入的线性映射得到每一个输出$y_{ij}$，该映射只依赖于输入和输出单元的相对位置。在某种意义上，采样因子为$f$的上采样与步长为$\frac{1}{f}$的卷积相同，只要$f$是整数，那么上采样方法就是输出步长为$f$的backwards convolution（有时称为deconvolution）。因此，可以在网络内进行反向传播，以实现端到端学习。deconvolution层的卷积核是不需要固定的（例如，双线性上采样），是可以学习的。在我们的实验中，发现网络中的上采样对于学习密集预测是快速和有效的。
Segmentation Architecture  我们将ILSVRC分类器转换成FCNs，并通过上采样和像素级损失进行密集预测。我们通过微调来训练分割。接下来，我们引入了一个新的”skip“架构，它结合了粗略的、语义的信息和局部的信息来重新细化预测。</description>
    </item>
    
    <item>
      <title>Transformer中的位置编码</title>
      <link>https://gdy0924.github.io/posts/transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</guid>
      <description>位置编码 参考链接：如何理解Transformer论文中的positional encoding，和三角函数有什么关系？
首先，给定一个长为$T$的文本，最简单的位置编码就是计数，即使用$PE=pos=0,1,2,&amp;hellip;,T-1$作为文本中每个字的位置编码，但是这个方法缺点很明显，这个序列是没有上界的。当有一个很长的文本，那么最后一个字的位置编码会比第一个字大太多，肯定会对模型有干扰。
其次，我们希望编码有一个范围，那么就可以想到使用文本长度对每个位置作归一化，得到$PE=\frac{pos}{T-1}$。这样固然使得所有位置编码都落入$[0,1]$区间，但是问题也是显著的：不同长度文本的位置编码步长是不同的，在较短的文本中紧紧相邻的两个字的位置编码差异，会和长文本中相邻数个字的两个字的位置编码差异一致。
最后，对于位置编码应该满足：体现同一个单词在不同位置的区别；体现一定的先后次序关系（相对位置），并且在一定范围内的编码差异不应该依赖于文本长度，具有一定不变性。一种思路是使用有界的周期性函数。在前面的两种做法里面，我们为了体现某个字在句子中的绝对位置，使用了一个单调的函数，使得任意后续的字符的位置编码都大于前面的字，如果我们放弃对绝对位置的追求，转而要求位置编码仅仅关注一定范围内的相对次序关系，那么有界的周期性函数就是很好的选择。
为什么选择三角函数 参考链接：浅谈 Transformer-based 模型中的位置表示
什么？是Transformer位置编码
位置编码可以分为绝对位置编码和相对位置编码，绝对位置对句子语义的影响不大，而相对位置比较重要。
Transformer 中使用 Positional Encoding 生成固定的位置表示： $$ PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) $$ $$ PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) $$ 其中，$pos$为token的位置，$pos=0,1,2,&amp;hellip;,L$，$i$为向量的某一维度，当$d_{model}=512$时，$i=0,1,2,&amp;hellip;,255$。
选择sin函数与cos函数，是因为其有以下特点： $$ sin(\alpha +\beta )=sin\alpha cos\beta +cos\alpha sin\beta $$ $$ cos(\alpha +\beta )=cos\alpha cos\beta -sin\alpha sin\beta $$ 由于上述公式，可以将$PE_{pos+k}$表示为$PE_{pos}$的线性组合，即： $$ PE(pos+k,2i)=sin(\frac{pos+k}{10000^{\frac{2i}{d_{model}}}}) $$ $$ =sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})cos(\frac{k}{10000^{\frac{2i}{d_{model}}}})+cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})sin(\frac{k}{10000^{\frac{2i}{d_{model}}}}) $$ $$ =PE(pos,2i)\times PE(k,2i+1)+PE(pos,2i+1)\times PE(k,2i) $$ $$ PE(pos+k,2i+1)=cos(\frac{pos+k}{10000^{\frac{2i}{d_{model}}}}) $$ $$ =cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})cos(\frac{k}{10000^{\frac{2i}{d_{model}}}})-sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})sin(\frac{k}{10000^{\frac{2i}{d_{model}}}}) $$ $$ =PE(pos,2i+1)\times PE(k,2i+1)-PE(pos,2i)\times PE(k,2i) $$ 借助上述绝对位置的编码公式，可以模型能够学习到相对位置信息。也就是说，如果将$PE_{pos}$和$PE_{pos+k}$相乘，可以得到： $$ PE_{pos}\cdot PE_{pos+k}=\sum_{i=0}^{\frac{d_{model}}{2}-1}sin(w_{i}pos)sin(w_{i}(pos+k))+cos(w_{i}pos)cos(w_{i}(pos+k)) $$ $$ =\sum_{i=0}^{\frac{d_{model}}{2}-1}cos(w_{i}(pos-(pos+k))) $$ $$ =\sum_{i=0}^{\frac{d_{model}}{2}-1}cos(w_{i}k) $$ 其中，$w_{i}=\frac{1}{10000^{\frac{2i}{d_{model}}}}$，从最后结果可以看出，唯一影响结果的就是$k$，并且，如果两个token的距离越大，也就是$k$越大，根据余弦函数的性质可以知道，两个位置的相乘结果越小。这样可以得到，如果两个token距离越远则乘积的结果越小。</description>
    </item>
    
    <item>
      <title>图像分割经典论文总结</title>
      <link>https://gdy0924.github.io/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</guid>
      <description>参考链接：超像素、语义分割、实例分割、全景分割 傻傻分不清？
分类 superpixels（超像素）  超像素是一系列像素的集合，这些像素具有类似的颜色、纹理等特征，距离也比较近。用超像素对一张图片进行分割的结果见下图，其中每个白色线条区域内的像素集合就是一个超像素。需要注意的是，超像素很可能把同一个物体的不同部分分成多个超像素。 Semantic Segmentation（语义分割）  语义分割还就是把图像中每个像素赋予一个类别标签（比如汽车、建筑、地面、天空等），如下图所示，把图像分为了草地（浅绿）、人（红色）、树木（深绿）、天空（蓝色）等标签，用不同的颜色来表示。但是该方式存在一些问题，比如如果一个像素被标记为红色，那就代表这个像素所在的位置是一个人，但是如果有两个都是红色的像素，这种方式无法判断它们是属于同一个人还是不同的人，即语义分割只能判断类别，无法区分个体。 Instance Segmentation（实例分割）  实例分割有点类似于物体检测，不过物体检测一般输出的是 bounding box，而实例分割输出的是一个mask。实例分割和语义分割也不同，它不需要对每个像素进行标记，它只需要找到感兴趣物体的边缘轮廓就行，比如下图中的人就是感兴趣的物体。该图的分割方法采用了一种称为Mask R-CNN的方法。我们可以看到每个人都是不同的颜色的轮廓，因此我们可以区分出单个个体。 Panoptic Segmentation（全景分割）  全景分割是语义分割和实例分割的结合。如下图所示，每个像素都被分为一类，如果一种类别里有多个实例，会用不同的颜色进行区分，我们可以知道哪个像素属于哪个类中的哪个实例。比如下图中黄色和红色都属于人这一个类别里，但是分别属于不同的实例（人），因此我们可以通过mask的颜色很容易分辨出不同的实例。 经典网络 语义分割： FCN：Fully Convolution Networks
U-NET：U-Net: Convolutional Networks for Biomedical Image Segmentation
SegNet：A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
PSPNet：PSPNet: Pyramid Scene Parsing Network
DeepLab：
v1：Semantic Image Segmentation with deep convolutional nets and fully connected CRFs
v2：DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</description>
    </item>
    
    <item>
      <title>Vision Transformer</title>
      <link>https://gdy0924.github.io/posts/vit/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/vit/</guid>
      <description>AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
原文链接：ViT
Abstract  虽然Transformer架构已经成为自然语言处理任务的实际标准，但它在计算机视觉中的应用仍然有限。在视觉领域中，注意力机制要么与卷积网络结合，要么用于替换卷积网络的某些部分，同时保持其整体结构。在本文中，证明了依赖CNN是不必要的，一个直接应用于图像patch序列的transformer可以很好完成图像分类任务。使用大量数据进行预训练，并转移到多个中型或小型图像识别进行基准测试，与最先进的卷积网络相比，Vision Transformer(ViT)获得了良好的结果，同时需要更少的计算资源来训练。
Introduction  自注意力机制，特别是transformer，已经成为自然语言处理的首选模型。主要的方法是在大型文本语料库上进行预训练，然后在较小的特定任务数据集上进行微调。然而，在计算机视觉中，CNN架构仍然占主导地位。受NLP成功的启发，许多工作尝试将自注意力机制与卷积架构相结合，但是在大规模图像识别中，经典的ResNet类架构仍然是最先进的技术。
受NLP中transformer成功的启发，我们将transformer架构直接应用于图像，并实现最少的修改。为此，我们将一个图像分割成一些patch，并将patch的线性embedding序列作为transformer的输入。patch的处理方式与NLP中token的处理方式相同。我们以有监督的方式训练该模型进行图像分类。
当在中等大小的数据集，如ImageNet上进行训练时，模型产生的精度比相当大小的ResNets低几个百分点。但是，通过在更大的数据集上进行训练，我们发现，ViT在足够规模的预训练和转移到数据点较少的任务时获得了优秀的结果。
Relative Work  Transformer已成为许多NLP任务中的最新方法，通常利用transformer在大型语料库上进行预训练，然后对特定的任务进行微调。
与该工作最相关的是：论文《 On the relationship between self-attention and convolutional layers》中提出的模型，该模型从输入图像中提取大小为2×2的patch，并在顶部应用自注意力机制。这个模型与ViT非常相似，但我们的工作进一步证明，大规模的预训练使transformer能够与最先进的卷积网络竞争（甚至更好）。此外，该模型使用了2×2像素的小patch尺寸，这使得该模型仅适用于小分辨率图像，而我们也处理中等分辨率图像。
Method Vision Transformer (ViT) 上图为该模型的整体架构：transformer接收一个一维的token embedding序列，为了处理二维图像，我们将图像$x\in \mathbb{R}^{H\times W\times C}$重塑为展平的2维patch序列$x_{p}\in \mathbb{R}^{N\times (P^{2}\cdot C)}$，其中$(H,W)$是原始图像的分辨率，$C$为通道数量，$(P,P)$是图像patch的分辨率，$N=\frac{HW}{P^{2}}$是patch的数量，也是transformer的有效输入序列长度。transformer在所有的层中使用恒定的潜在向量大小$D$，因此，我们将补丁展平，并通过可训练的线性投影映射到D维，如下面公式所示，我们将这个投影的输出称为patch embedding。 $$ z_{0}=[x_{class};x_{p}^{1}E;x_{p}^{2}E;&amp;hellip;;x_{p}^{N}E]+E_{pos},E\in \mathbb{R}^{(P^{2}\cdot C)\times D},E_{pos}\in \mathbb{R}^{(N+1)\times D} $$ 与BERT的$[class]$token类似，我们在embedding的patch序列中加入了一个可学习的embedding：$z_{0}^{0}=x_{class}$，其为transformer的encoder的输出层的状态$z_{L}^{0}$，作为图像的表示$y$，如下面公式所示。在预训练和微调过程中，一个分类头都被附加在$z_{L}^{0}$上：在预训练时，分类头由一个隐藏层的MLP实现，在微调时由一个单一的线性层实现。（即用最终的输出状态$z_{L}^{0}$来分类） $$ y=LN(z_{L}^{0}) $$ 位置embedding被添加到patch embedding中，以保留位置信息。我们使用标准的可学习的一维位置embedding，因为我们没有观察到使用更先进的二维位置embedding能显著的提高性能，所得到的embedding向量序列作为编码器的输入。
transformer的encoder由Multi-Head自注意（MSA）和MLP块（MLP）组成。在每个块之前应用层归一化（Layernorm，LN），在每个块之后应用残差连接。 $$ {z_{t}}&#39;=MSA(LN(z_{t-1}))+z_{t-1} $$ $$ z_{t}=MLP(LN({z_{t}}&#39;))+{z_{t}}&#39; $$ Hybrid Architecture</description>
    </item>
    
    <item>
      <title>SSD</title>
      <link>https://gdy0924.github.io/posts/ssd/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/ssd/</guid>
      <description>SSD: Single Shot MultiBox Detector
原文链接：SSD
Abstract  提出了一种利用一个深度神经网络来实现目标检测的方法：SSD，根据不同的高宽比和每个特征图定位的尺寸大小，将bounding-box的输出空间离散为一组默认框。在预测时，该网络为每个默认框中每个对象类别的存在生成分数，并对该框进行调整，以更好地匹配对象形状。此外，该网络结合了不同分辨率的多个特征图进行预测，以能够处理不同大小的对象。相比于需要候选框生成的方法，SSD很简单：完全取消了候选区域生成和后续的像素/特征重采样阶段，将所有的计算过程封装在一个网络中。
Introduction  目前最先进的对象检测方法流程是：生成候选框，为每个框重新采样像素或特征，将其输入到一个分类器中。以上方法速度很慢。
本文提出了第一个基于深度网络的对象检测器，不需要为候选框重新采样像素或特征，并且可以达到一样的精度（精度要比YOLO高）。速度的提升来自于消除候选框生成和后续的像素或特征重采样阶段。
我们的改进包括使用一个小的卷积核来预测bounding-box的对象类别和偏移量，为不同的高宽比检测使用单独的预测器（卷积核），并将这些卷积核应用在网络后期的多个特征图上，以便在多个尺度上检测。通过这些修改，特别是在不同尺度上使用多层进行预测，可以使用相对较低分辨率的输入来实现高精度，进一步提高检测速度。
Contributions：
（1）引入SSD，多个类别的单阶段检测器，比之前的单阶段检测器YOLO更快更精确，与更慢的Faster R-CNN精度相同；
（2）SSD的核心是为一组固定的默认边界框预测类别和偏移量，通过使用较小的应用在特征图上的卷积核；
（3）为了获得较高的检测精度，我们从不同尺度的特征图中生成不同尺度的预测，并通过长宽比明确地分离预测；
（4）以上的改进形成了简单的端到端训练和高精度，甚至在低分辨率的输入图像上，进一步提高了速度和准确性。
The Single Shot Detector (SSD) Model  SSD方法是基于前馈卷积网络的，该网络生成固定大小的bounding-box集合，并获得这些框中对象类实例的分数，然后通过NMS产生最终的结果。早期的网络是基于高质量图像分类的标准结构（去掉最终的分类层），我们将称之为base network，我们向网络中添加辅助结构，以生成具有以下特征的检测器：
Multi-scale feature maps for detection
在base network的末端添加卷积特征层，通过这些层，特征图的大小逐步减小，并在多个尺度的特征图上进行检测，对于每一个特征层，用于预测检测的卷积模型是不同的。（Overfeat和YOLO都在单尺度特征图上检测的）。
Convolutional predictors for detection
每个添加的特征层（或是来自base network的现有特征层）都可以使用一组卷积核生成一组固定的检测预测，如下图所示。对于大小为$m×n$、通道数为$p$的特征层，使用$3×3×p$的卷积核，生成一个类别的分数或相对于默认框的偏移量，在$m×n$的每个位置应用卷积核，产生一个输出值。边界框偏移输出值是相对于相对于每个特征图位置的默认框位置来测量的（参见YOLO的架构，它在此步骤中使用的是全连接层而不是卷积）。 Default boxes and aspect ratios
我们将一组默认的边界框与每个特征图的网格关联起来（类似于Faster R-CNN中的anchor）。默认框以卷积的方式滑过特征图，这样每个框相对于其对应网格的位置就是固定的。在每个特征图的网格中，我们预测相对于网格中默认框的偏移量，以及每个框中存在类实例的类别分数。
在每个网格生成$k$个默认框，计算$c$个类分数和相对于原始默认框的4个偏移量，特征图中的每个网格生成$(c+4)×k$个输出，即一个$m×n$大小的特征图产生$(c+4)×k×m×n$个输出，我们的默认框与anchor相同，不过我们将其应用在不同分辨率的特征图上。
default box
如下图所示： （a）表示SSD在训练期间只需要一个输入图像和每个object的ground truth。以卷积的方式，在特征图的每个位置生成一组不同高宽比的默认框（如图中一组为4个），并且应用在不同尺度的特征图上（（b）为$8×8$，（c）为$4×4$）。对于每个默认框，预测所有对象类别和偏移量。
Training  训练SSD与训练两阶段的检测模型的区别在于，ground truth的信息需要被分配给固定的检测器的输出集合中的特定输出。
Matching strategy
在训练过程中，我们需要确定哪些默认框与ground truth相对应，对于每个ground truth框，我们从不同位置、高宽比和尺寸的默认框中进行选择。首先将每个ground truth框与具有 the best jaccard overlap的默认框进行匹配。与MultiBox不同的是，我们将与ground truth的jaccard重叠高于阈值（0.</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://gdy0924.github.io/posts/bert/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/bert/</guid>
      <description>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
原文链接：BERT
Abstract  引入了一种新的语言表示模型，称为BERT（Bidirectional Encoder Representations from Transformers）。与最近的语言表示模型不同，BERT是从未标记文本中预训练出的双向表示，通过联合所有层中的左右上下文。因此，预训练的BERT模型可以通过一个额外的输出层来进行微调，从而为广泛的任务创建最先进的模型。
Introduction  预训练的语言模型已被证明对许多自然语言处理任务都有效。目前有两种策略为下游任务应用预训练模型：基于特征的和微调的。基于特征的方法使用特定的任务框架，包括预训练的表示作为额外的特征。微调的方法引入最少的参数，通过简单的微调所有的预训练参数来训练下游任务。我们认为，目前的技术限制了预训练的表示，特别是对于微调方法，主要的限制是标准语言模型是单向的，这限制了在预训练过程中可以使用的架构的选择。
在本文，我们改进了基于微调的方法，提出了BERT（Bidirectional Encoder Representations from Transformers），其通过使用预训练的掩码语言模型（masked language model，MLM）减轻了前面提到的单向性约束。掩码语言模型从输入中随机掩码一些token，其目的是仅根据其上下文来预测掩码词的原始词汇表id。与从左向右的预训练的语言模型不同，MLM使表示能够融合左右上下文，这使我们能够对双向Transformer进行预训练。除了掩码语言模型，我们还使用了“预测下一个句子”任务，联合预训练文本对的表示。
Contributions：
（1）证明了双向预训练对语言表示的重要性；
（2）证明了预训练表示可以减少对许多精心设计的任务特定架构的需要，BERT是第一个基于微调的表示模型，它在大型句子级和标记级任务上取得了最优的效果。
Related Work Unsupervised Feature-based Approaches  学习广泛适用的单词表征一直是一个活跃的研究领域，包括非神经方法和基于神经网络的方法。预先训练好的单词embedding是现代自然语言处理领域的一个组成部分，与从头开始学习的embedding相比，它提供了显著的改进。为了对单词embedding向量进行预训练，我们使用了从左向右的语言建模目标，以及在左右上下文中区分正确和不正确单词的目标。
Unsupervised Fine-tuning Approaches  最近，产生上下文标记表示的句子或文档编码器可以从未标记的文本中预训练得到，并针对有监督的下游任务进行了微调。这些方法的优点是很少需要从头开始学习参数。从左到右的语言建模和自动编码器已用于预训练这些模型。
Transfer Learning from Supervised Data  有研究表明，可以从大型数据集的监督任务中进行有效转移，如自然语言推理和机器翻译。计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，其中有效的方法是对使用ImageNet预训练的模型进行微调。
BERT  我们将在本节中详细的介绍BERT。在我们的框架中，共有两个步骤：预训练和微调。在预训练过程中，该模型在不同的预训练任务上对未标记的数据进行训练；为了进行微调，首先使用预先训练好的参数初始化BERT模型，并使用从下游任务中获得的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。下图是以问答为例的示意图： BERT的一个显著特点是针对不同任务使用统一框架，即预训练的架构和最终的下游任务架构之间的差别很小。
Model Architecture：BERT的模型架构是一个多层双向Transformer编码器，不再详细介绍，详见transformer（Attention is all you need）。在该项目中，层数（transformer块）由$L$表示，隐藏层大小为$H$，自注意力的头数为$A$，我们的结果主要是在两个模型大小上的：$BERT_{BASE} (L=12,H=768,A=12)$和$BERT_{LARGE}(L=24,H=1024,A=16)$。 Input/Output Representations：为了使BERT能够处理各种下游任务，我们的输入表示能够明确地在token序列中表示单个句子和一对句子（如：&amp;lt;Question，Answer&amp;gt;）。在该项目中，一个“sentence”可以是一个连续文本的任意跨度，而不是一个实际的语言句子；一个“sequence”指的是BERT的输入token序列，它可以是单个句子或两个句子放在一起的。
使用WordPiece embedding，其中包含30,000个词汇token，每个序列的第一个token是特殊的分类标记[CLS]。与该token对应的最终隐藏状态被用作分类任务的序列表示（即用该token进行后续的分类任务）。句子对形成一个单一的序列。我们用两种方式来区分这些句子：首先，用一个特殊的标记[SEP]将它们分开；其次，我们在每个token中添加一个可学习的embedding，表明它是属于句子$A$还是句子$B$。如上图所示，我们将输入embedding记为$E$，最终隐藏向量的特殊token[CLS]记为$C$，第$i$个输入token的最终隐藏向量表示为$T_{i}$。
对于一个给定的token，它的输入表示是通过将相应的token、段embedding和位置embedding相加来形成的，如下图所示。 Pre-training BERT  我们没有使用传统的从左到右或从右到左的语言模型对BERT进行预训练，我们使用两个无监督的任务对BERT进行预训练，如第一张图的左侧所示。</description>
    </item>
    
    <item>
      <title>YOLO系列</title>
      <link>https://gdy0924.github.io/posts/yolo%E7%B3%BB%E5%88%97/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/yolo%E7%B3%BB%E5%88%97/</guid>
      <description>YOLO V1  原文链接：YOLO V1
参考链接：你真的读懂yolo了吗？
Abstract  提出一种新的目标检测算法——YOLO，之前的目标检测工作都是使用另外的分类器实现检测，我们将目标检测构建成回归问题框架，在空间上分离bounding-box和相关类别概率。一个网络直接从完整图像中预测bounding-box和类别概率，因此速度很快。
Introduction  最近的方法，如R-CNN使用候选区域生成方法，首先在图像中生成候选框，然后在这些候选框上进行分类，分类后，再对bounding-box进行细化，消除重复框。这些复杂的操作导致运行速度缓慢，难以优化，因为每个单独的网络必须单独训练。
我们将目标检测重新定义为单一的回归问题，直接从图像像素到bounding-box坐标和分类概率，&amp;ldquo;you only look once&amp;rdquo;（YOLO）就可以预测有什么物体以及它们在哪里。其架构如下图所示（YOLO包括三部分：将输入图像调整为448×448，在图像上运行单一卷积网络，通过模型的置信度来确定检测结果的阈值。）：单个卷积网络同时预测多个bounding-box和及其分类概率，YOLO训练完整的图像，并直接优化检测性能。
相比于之前的网络，其有如下优点：
（1）YOLO的速度非常快：由于我们将目标检测作为一个回归问题，所以我们不需要一个复杂网络。基本网络以每秒45帧的速度运行，另外，YOLO实现的平均平均精度是其他实时系统的两倍以上；
（2）YOLO在做预测时关注的是图像的整体（全局）：与基于滑动窗口和候选区域生成的方法不同，YOLO在训练和测试期间可以看到整个图像，因此它隐式地编码上下文信息，而对于Fast R-CNN，由于看不到上下文信息，会错误的将背景patch预测为目标类；
（3）YOLO学习目标的可推广表示：当在图像上进行训练和测试时，YOLO的性能远远优于DPM和R-CNN等其他检测方法，当应用在新的领域或者有预期之外的输入时，YOLO也可以很好的运行。
YOLO在精度上仍然落后于最先进的检测模型，虽然它可以快速识别图像中的物体，但它很难精确定位一些物体，特别是小的物体。
Unified Detection  我们将目标检测中的独立网络统一为一个单一的神经网络，使用来自整个图像的特征来预测每个bounding-box，同时预测类别概率，即我们的网络会对整个图像和图像中的所有目标对象进行全局推理。 我们的模型将输入图像分割成一个$S×S$网格，如果一个对象的中心落入某一网格单元格中，则该网格单元格负责检测该对象。
每个网格单元格预测$B$个bounding-box和bounding-box的置信度分数，这些置信度分数反映了该bounding-box包含物体的真实程度，也反映了该bounding-box预测的准确程度。我们将信心定义为$Pr(Object)\ast IOU_{pred}^{truth}$：如果该单元格中不存在对象，则置信度分数应该为零，否则，我们希望置信度得分等于bounding-box和ground truth之间的IOU值。
每个bounding-box由5个预测部分组成：$x,y,w,h,confident$，$(x,y)$坐标表示相对于网格单元格边界的bounding-box的中心，$w$和$h$是相对于整个图像进行预测的bounding-box的宽度和高度，$confident$表示bounding-box和ground truth之间的IOU。
每个网格单元还预测类别$C$的条件概率，$Pr(Class_{i}|Object)$，我们只预测每个网格单元的一组类别概率，而不管bounding-box的数量。
在测试时，我们将类别条件概率和单独的bounding-box置信度预测相乘： $$ Pr(Class_{i}|Object)\ast Pr(Object)\ast IOU_{pred}^{truth}=Pr(Class_{i})\ast IOU_{pred}^{truth} $$ 即每个bounding-box的特定类别的置信分数，这些分数既编码了该类出现在框中的概率，也编码了bounding-box与物体的匹配程度。
我们设置$S=7$，$B=2$，如下图所示：该模型将图像划分为一个$S×S$的网格，并为每个网格单元预测$B$个bounding-box，及其置信度和$C$类的概率，因此，这些预测值被编码为一个$S×S×(B\ast 5+C)$张量。 Network Design  我们将该模型作为一个卷积神经网络实现，网络的初始卷积层从图像中提取特征，而全连接层则预测输出的概率和bounding-box坐标。
我们的网络架构是基于GoogleNet的，共有24个卷积层，2个全连接层。没有使用GoogLeNet中的Inception模块，而是简单地使用1×1降维层和3×3卷积层，如下图所示： 我们的网络的最终输出是预测的$7×7×30$大小的张量。
我们还训练了一个快速版本的YOLO，Fast YOLO使用的神经网络的卷积层更少（9个而不是24个），这些层的卷积核也更少。除了网络的大小之外，YOLO和Fast YOLO之间的所有训练和测试参数都是相同的。
Training  我们在ImageNet数据集上预训练了我们的卷积层；然后，我们转换模型来进行目标检测，检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从$224×224$提高到$448×448$；最后一层预测了分类概率和bounding-box坐标，我们根据图像的宽度和高度将边界框的宽度和高度进行归一化，使它们落在0到1之间，参数化边界框$x$和$y$坐标为特定网格单元位置的偏移，因此它们也被限定在0和1之间；最后一层使用线性激活函数，而所有其他层使用Leaky ReLU： $$ x&amp;gt; 0,f(x)=x $$ $$ otherwise,f(x)=0.1x $$ 我们使用均方误差进行模型优化，但对定位误差和分类误差的权重相同，因此可能并不理想。另外，在每个图像中，许多网格单元格不包含任何物体，其置信度分数就为0，远远超过了包含物体的单元格的梯度。
为了解决上述问题，我们增加bounding-box坐标预测的损失，减少不包含物体的单元格的预测的损失，引入两个参数，$\lambda_{coord}$和$\lambda_{noobj} $，并设置$\lambda_{coord}=5$、$\lambda_{noobj}=0.5$。
均方误差对大bounding-box和小bounding-box的误差具有同样的权重，但是误差度量应该反映：大bounding-box的小偏差比小bounding-box的小偏差小，为了解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。
YOLO为每个网格单元格预测多个bounding-box。在训练时，对于每一个物体，我们希望有一个bounding-box预测器去负责该物体，因此，我们指定一个预测器来“负责”预测一个物体，与ground truth有最高IOU的预测。这使得bounding-box预测器之间的专门化，每个预测器都能更好地预测特定大小、纵横比或物体的类别。
因此，我们的损失函数如下： $$ \lambda_{coord}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{obj}[(x_{i}-\hat{x_{i}})^{2}+(y_{i}-\hat{y_{i}})^{2}] $$ $$ +\lambda_{coord}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{obj}[(\sqrt{w_{i}}-\sqrt{\hat{w_{i}}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h_{i}}})^{2}] $$ $$ +\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{obj}(C_{i}-\hat{C_{i}})^{2} $$ $$ +\lambda_{noobj} \sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_ {ij}^{noobj}(C_{i}-\hat{C_{i}})^{2} $$ $$ +\sum_{i=0}^{S^{2}}\mathbb{I}_ {i}^{obj}\sum_{c\in classes}(p_{i}(c)-\hat{p_{i}}(c))^{2} $$ 其中，$\mathbb{I}_ {i}^{obj}$表示单元格$i$是否有物体出现（0或1），$\mathbb{I}_ {ij}^{obj}$表示单元格$i$中第$j$个bounding-box预测器负责预测（0或1）。第一行是边界框中心坐标的误差；第二行是边界框长宽的误差，由于边界框大小本身便会影响到误差的大小，为了降低边界框大小的影响，而采用了开方差来比较；第三行是包含物体的单元格以及准确预测的边界框的置信度误差；第四行是不包含物体的单元格中以及准确预测的边界框的置信度误差；第五行是包含物体的单元格中，预测对象概率的各分项的误差平方之和。</description>
    </item>
    
    <item>
      <title>R-CNN系列</title>
      <link>https://gdy0924.github.io/posts/r-cnn%E7%B3%BB%E5%88%97/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/r-cnn%E7%B3%BB%E5%88%97/</guid>
      <description>R-CNN 原文链接：R-CNN
Abstract  在pascalVOC数据集上针对图像检测任务，其性能在过去几年中已经趋于稳定。性能最好的方法是复杂的集成系统，它通常将多个低级图像特征与高级上下文结合起来。在本文中，我们提出了一种简单且可扩展的检测算法，相对于之前对VOC2012的最佳结果，它将平均精度(mAP)提高了30%以上——实现了53.3%的mAP。
该方法具有两个关键点：（1）可以将卷积神经网络(CNNs)应用于自下而上的区域建议，以定位和分割对象；（2）当标记的训练数据稀缺时，通过对辅助任务的监督预训练，然后进行特定领域的微调，会产生显著的性能提高。
我们将区域建议与CNN结合起来，我们称我们的方法为R-CNN，即具有CNN特征的区域。
Introduction  LeCun等人表明，通过反向传播的随机梯度下降对于训练卷积神经网络(CNNs)是有效的，这是一类新的神经网络的模型。2012年，AlexNet上显示出更高的图像分类精度，重新点燃了人们对cnn的兴趣。因此人们开始讨论ImageNet上的CNN分类结果在多大程度上可以推广到pascalVOC挑战上的目标检测结果？
我们通过缩小图像分类和目标检测之间的差距来回答上述问题。本文首次证明，CNN可以显著提高对pascalVOC的目标检测性能。为了实现这一结果，我们关注两个问题：使用深度网络来定位对象，训练一个只用少量标注数据的高性能模型。
与图像分类不同，检测需要在图像中定位（可能有很多）对象。一种方法将定位框架定义为一个回归问题。另一种选择是建立一个滑动窗口检测器。为了保持高分辨率，这些CNN通常只有两个卷积层和池化层。我们考虑了滑动窗口的方法，但是在比较高层的单元，我们拥有5个卷积层，有很大的感受野和步长。
我们通过“区域识别”操作来解决CNN定位问题，在测试时，我们的方法为输入图像生成大约2000个类别独立的区域建议，使用CNN从每个建议中提取一个固定长度的特征向量，然后用线性支持向量机对每个区域进行分类。下图展示了方法概述及结果，由于我们的模型结合了区域建议和CNN，因此称之为R-CNN。 目标检测中面临的第二个挑战是标记数据的稀缺，目前可用的数量不足以训练大型的CNN。解决这个问题的传统解决方法是使用无监督的预训练，然后进行有监督的微调。因此，我们通过实验证明在大型辅助数据集(ILSVRC)上进行监督预训练，然后在小数据集(pascal)上进行微调，是在数据稀缺时学习高性能CNN的有效方法。我们注意到，由于R-CNN在图片的区域上进行操作，所以很自然地可以将其扩展到语义分割的任务。
Object detection with R-CNN  该目标检测模型由三个模块组成。（1）产生与类别不相关的候选区域，这些候选区域定义了检测模型的候选检测集；（2）一个大型的卷积神经网络，它从每个区域中提取一个固定长度的特征向量；（3）一组特定于类的线性支持向量机。
Module design  Region proposals：最近的各种论文提供了产生类别不相关的候选区域的方法。例如：，我们使用的是selective search方法。
Feature extraction：基于AlexNet网络，从每个候选区域中提取一个4096维的特征向量，网络输入是227×227RGB图像，接着通过5个卷积层和2个全连接层。为了提取候选区域的特征，由于网络的输入大小必须是227×227的，因此需要对候选区域进行缩放扭曲，如下图所示： Test-time detection  在测试时，我们对测试图像进行selective search，提取出大约2000个候选区域，将候选区域进行变换，经过CNN网络以提取特征，针对每个类，使用训练好的SVM进行评分。对于图像中所有的得分区域，使用NMS（对于每个类独立），即如果一个候选区域与另一个候选区域的IoU大于阈值，且另一个候选区域的得分大于该区域，则拒绝该区域。
Run-time analysis：两个操作使检测更有效：所有的CNN参数都在所有类别中共享；与其他常用方法相比，CNN提取到的特征向量是低维的。
Training  Supervised pre-training：我们在一个像素级标注的大型数据集(ILSVRC2012)上对CNN进行了预训练（该数据的边界框标签不可用）。
Domain-specific fine-tuning：为了使我们的CNN适应新的任务和新的领域，我们使用转换后的候选区域对CNN进行随机梯度下降(SGD)训练。CNN的架构没有变，除了最后的分类数变为（N+1），其中N是对象类的数量，+1作为背景。将与真值的IoU大于0.5的区域作为正类，其余为负类。
Object category classifiers：使用二分类来对目标进行检测，使用IoU重叠阈值来决定一个区域是否为正类，阈值选择0.3是在验证集上实验选择的（{0,0.1,&amp;hellip;,0.5}），我们发现，将阈值设置为0.5或者0时，mAP会降低。
针对每个类，都会训练一个线性SVM。
Selective Search 参考链接：Selective Search （以下部分是Selective Search论文中的内容。）
物体识别，在之前的做法主要是基于穷举搜索（Exhaustive Search）：选择一个窗口扫描整张图像（image），改变窗口的大小，继续扫描整张图像。这种做法是比较原始直观，改变窗口大小，扫描整张图像，非常耗时。若能过滤掉一些无用的box将会节省大量时间。这就是本文中Selective Search(选择性搜索)的优点，将穷举搜索（Exhausticve Search)和分割（Segmentation)结合起来。
层次归类算法(Hierarchical Grouping Algorithm)  input：图片
output：集合L
（1）获取图片初始分割区域$R=(r_{1},&amp;hellip;,r_{n})$；
（2）初始化相似度集合$S=\varnothing $；
（3）计算$R$中两两相邻区域$(r_{i},r_{j})$之间的相似度，添加到相似度集合$S$；
（4）从相似度集合$S$中找出，相似度最大的两个区域$r_{i}$和$r_{j}$，将其合并成为一个区域$r_{t}$，从$S$中除去原先与$r_{i}$和$r_{j}$相邻区域之间计算的相似度，计算$r_{t}$与其相邻区域（原先与$r_{i}$或$r_{j}$相邻的区域）的相似度，添加到相似度集合$S$中，将新区域$r_{t}$添加到区域集合$R$中；
（5）迭代步骤（4）直至$S$为空，即可合并区域都已合并完；
（6）获取R中每个区域的Bounding Boxes，这个结果就是图像中物体可能位置的可能结果集合L。
多样性策略  论文给出了两个方面的多样化策略：颜色空间多样化，相似度计算的多样化。</description>
    </item>
    
    <item>
      <title>R-CNN系列简单总结</title>
      <link>https://gdy0924.github.io/posts/r-cnn%E7%B3%BB%E5%88%97%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/r-cnn%E7%B3%BB%E5%88%97%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/</guid>
      <description>参考链接：RCNN, Fast R-CNN 与 Faster RCNN理解及改进方法
R-CNN 包含三个步骤：
（1）候选区域选择：使用基于启发式的区域提取方法，用的方法是Selective Search，即从小区域开始，逐步合并两个最相关的相邻区域，重复此步骤，直到图像合并为一个区域，最后输出候选区域；
（2）CNN特征提取：将region proposal 缩放至统一大小，作为输入进行特征提取；
（3）分类与边界回归：将得到的特征向量送进每一类的SVM进行分类, 如果有十个类别，那么每个region proposal要跑10个SVM，得到类别；同时将特征向量做回归（左上角右下角的四个坐标），修正region proposal的位置（在这一步中包括NMS操作，用于去除掉同一类别中重复的box）
Fast R-CNN 包含四个步骤：
（1）对原始图片使用selective search算法得到约2k个region proposals（与R-CNN第一步相同）；
（2）将任意size的图片输入CNN，得到特征图，只做一次卷积，相比于R-CNN，每个region proposal都要经过一次卷积，大大减少了时间；
（3）在特征图中找到每一个region proposals对应的特征框，使用RoI池化层将每个特征框池化到统一大小；
（4）统一大小的特征框经过全连接层得到固定大小的特征向量，分别进行softmax分类（使用softmax代替了R-CNN里面的多个SVM分类器）和bbox回归。
RoI pooling层  RoI池化层用来将region proposals对应的特征框池化成统一大小，具体来说就是：将$h×w$大小的RoI特征框，划分成近似$h/H×w/W$大小的子窗口，形成$H×W$网格，将每个子窗口中的最大值输出到相应的网格单元中，如下图所示：是将一个$5×7$大小的region proposals，经过RoI pooling得到固定大小$2×2$的特征向量。 Faster R-CNN 参考链接： 提出一个RPN网络来代替之前的selective search操作，即利用网络来生成候选框，大大提高了效率，其包含四个步骤：
（1）提取特征：输入固定大小的图片，进过卷积层提取特征图feature map；
（2）生成region proposals: 经过Region Proposal Networks(RPN)网络生成region proposals，接着通过softmax判断anchors属于foreground还是background，再利用bounding box 回归修正anchors获得精确的proposals（候选区域）；
（3）ROI Pooling: 该层的输入是feature maps和proposals，综合这些信息后提取proposal feature maps；
（4）Classification: 将Roi pooling生成的proposal feature maps分别传入softmax分类和bounding box regression获得检测物体类别和检测框最终的精确位置。 因此，在训练Faster R-CNN的时候有四个损失：RPN 分类损失：anchor是否为前景（二分类）；RPN位置回归损失：anchor位置微调；RoI 分类损失：RoI所属类别（K+1类）；RoI位置回归损失：继续对RoI位置微调。四个损失相加作为最后的损失，反向传播，更新参数。
RPN  上述图片的下边部分的两个分支，就是RPN网络，上面分支通过softmax分类anchors获得positive和negative分类，下面分支用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。（其实RPN网络就相当于一个小型的目标检测网络）</description>
    </item>
    
    <item>
      <title>IoU&amp;NMS</title>
      <link>https://gdy0924.github.io/posts/iounms/</link>
      <pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/iounms/</guid>
      <description>IoU  IoU的全称为交并比（Intersection over Union），即 “预测的边框” 和 “真实的边框” 的交集和并集的比值，如下图所示： NMS 参考链接：NMS
非极大值抑制（Non-Maximum suppression，NMS）用于消除同一个物体上的冗余预测框。
传统NMS  其主要思想是：先对网络预测出的所有边界框按照分数由高到低排序，然后选取分数最高的预测框作为target，分别计算target与其余剩下预测框的重叠程度（用IoU来衡量），若重叠程度大于某一预先设定的阈值，则认为该预测框与target是同时负责预测同一个物体的，所以将该边界框删除，否则予以保留。接着在未被删除的预测框中选择分数最高的预测框作为新target，重复以上过程，直至判断出所有的框是否应该删除。 Soft-NMS  在传统NMS中，对于$IoU\geq NMS$阈值的相邻框，其做法是将其得分暴力置0。这对于有遮挡的情况十分不友好。因此Soft-NMS的做法是采取得分惩罚机制，使用一个与IoU正相关的惩罚函数对得分$s$进行惩罚： 其中，$M$代表当前的最大得分框。</description>
    </item>
    
    <item>
      <title>Transformer</title>
      <link>https://gdy0924.github.io/posts/transformer/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/transformer/</guid>
      <description>原文链接：Transformer
Abstract  目前主要的序列模型是主要是基于复杂的循环或者卷积神经网络的，包括一个编码器（encoder）和解码器（decoder）。性能最好的模型通过一种注意力机制将编码器和解码器连接起来。我们提出了一种新的简单的网络架构，Transformer，完全基于注意机制，完全取消递归和卷积，并通过实验验证了其有效性。
Introduction  循环神经网络，特别是长短期记忆LSTM和门控循环GRU神经网络，已经成为序列建模和transduction问题的最先进的方法，如语言建模和机器翻译。此后，许多努力继续推动循环语言模型和编解码器架构。
循环模型通常会沿着输入和输出序列的位置进行计算，它们生成一系列隐藏状态$h_{t}$，通过前一个隐藏状态$h_{t-1}$和输入$t$的计算得来。这种顺序性质阻碍了训练过程的并行化，这在较长的序列长度时变得至关重要，因为内存限制了样本之间的批处理。
注意力机制已经成为各种任务中引人注目的序列建模和transduction模型的一个组成部分，其允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离，通常与循环神经网络一起使用。
在本文，我们提出了Transformer，一个不需要循环神经网络的模型架构，而是完全依赖于一个注意力机制来提取输入和输出之间的全局依赖关系，允许更多的并行化。
Background  为了减少顺序计算，提出了扩展神经GPU、ByteNe和ConvS2S等，这些方法都使用卷积神经网络作为基本的block，并行计算所有输入和输出位置的隐藏表示。在这些模型中，关联两个任意输入或输出位置的信号所需的操作数量随着距离的增大而增长，这使得学习远距离位置之间的依赖关系更加困难。
自注意力机制，是一种注意机制，将单个序列的不同位置联系起来，以计算序列的表示。自注意力机制已成功地应用于各种任务中，包括阅读理解、抽象摘要、文本隐含和学习任务独立的句子表征。
据我们所知，Transformer是第一个完全依赖于自注意力机制来计算其输入和输出的表示的transduction模型，而不使用序列对齐的循环或卷积神经网络。
Model Architecture  大多数神经序列transduction模型都是编码器（encoder）-解码器（decoder）结构。其中，编码器将一个符号表示的输入序列$(x_{1},&amp;hellip;,x_{n})$映射到一个连续表示的序列$z=(z_{1},&amp;hellip;,z_{n})$。给定$z$，解码器一次生成一个符号的输出序列$(y_{1},&amp;hellip;,y_{m})$，在生成下一个符号时，使用之前生成的符号作为附加的输入。
Transformer遵循encoder-decoder架构，使用堆叠的自注意力机制，编码器和解码器的全连接层，分别如下图的左半部分和右半部分所示： Encoder and Decoder Stacks encoder和decoder的堆叠  Encoder： encoder由6个完全相同的层组成（n=6），每个层都有两个子层，第一个子层是一个multi-head自注意机制，第二个子层是一个简单的、全连接的前馈网络。我们在两个子层中使用残差连接，接着进行层归一化（layer normalization）。也就是说，每个子层的输出是$LayerNorm(x+Sublayer(x))$，其中$Sublayer(x)$是由子层本身实现的函数。为了方便残差连接，模型中的所有子层以及embedding层都会产生$d_{model}=512$的输出。
Decoder： decoder也由6个完全相同的层组成（n=6）。除了每个编码器层中的两个子层外，decoder还增加了第三个子层，该子层对encoder的输出进行multi-head自注意操作。与encoder类似，我们在每个子层使用残差连接，然后进行层归一化。我们还修改了decoder中的自注意子层，以防止位置关注后续的位置。
Attention 注意力机制  注意力机制可以描述为将查询（query）和一组键-值对（key-value）映射到输出，其中query、key、value和输出都是向量。输出是对value的加权求和，分配给每个value的权重由query与相应key通过函数计算得来的。
Scaled Dot-Product Attention  我们将我们的注意力机制称为“Scaled Dot-Product Attention”（下图所示）。输入由query和key（维度为$d_{k}$），以及value（维度为$d_{v}$）组成。我们计算query和所有key的点乘，除以$\sqrt{d_{k}}$，并通过SoftMax获得value的权重。 实际上，我们将一组query、key和value整合成矩阵形式$Q$、$K$和$V$，同时计算一组query注意力分数，计算公式如下： $$ Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$ 两个最常用的注意力函数是 additive attention和dot-product attention。dot-product attention与我们的算法相同，除了比例因子$\frac{1}{\sqrt{d_{k}}}$。点积注意力更快，更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。
对于较小的$d_{k}$值，这两种注意力机制的表现相似，在$d_{k}$值较大的情况下， additive attention优于dot-product attention。我们猜测，对于大的$d_{k}$，其得到的点积结果也越来越大，使SoftMax进入一个具有极小梯度的区域，因此，为了避免上述问题，我们用比例因子$\frac{1}{\sqrt{d_{k}}}$来缩放点积。
Multi-Head Attention  我们发现，不使用$d_{model}$维的key、value和query来得到单一的注意力函数，而是对key、value和query进行$h$次的线性投影，分别使用不同的、学习到的$d_{k}$、$d_{v}$和$d_{q}$维的线性投影。在每个query、key和value的投影结果上，我们并行地执行注意力函数，生成$d_{v}$维输出，这些输出被连接起来并再次线性变换，从而得到最终的值，如下图所示： multi-head注意力机制允许模型共同关注来自不同位置的不同表示子空间的信息，公式如下： $$ MultiHead(Q,K,V)=Concat(h_{1},&amp;hellip;,h_{h})W^{O} $$ $$ where; h_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}) $$ 其中的投影是参数矩阵，$W_{i}^{Q}\in\mathbb{R}^{d_{model}\times d_{k}}$，$W_{i}^{K}\in\mathbb{R}^{d_{model}\times d_{k}}$，$W_{i}^{V}\in\mathbb{R}^{d_{model}\times d_{V}}$，$W^{O}\in\mathbb{R}^{hd_{v}\times d_{model}}$。</description>
    </item>
    
    <item>
      <title>Attention&amp;Self-Attention</title>
      <link>https://gdy0924.github.io/posts/attentionself-attention/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/attentionself-attention/</guid>
      <description>参考连接：
注意力机制
大白话浅谈【注意力机制】
注意力机制（Attention）
(强推)李宏毅2021/2022春机器学习课程
Encoder-Decoder 框架  下图是一个Encoder-Decoder 框架的例子，对应任务是机器翻译。 对于句子对&amp;lt;source，target&amp;gt;，将给定输入句子source，通过Encoder-Decoder框架生成目标句子target。其中，source和target都是一组单词序列： $$ source=\left \langle x_{1},x_{2},&amp;hellip;,x_{m} \right \rangle $$ $$ target=\left \langle y_{1},y_{2},&amp;hellip;,y_{n} \right \rangle $$ Encoder是对source进行编码，转换成中间语义$C$： $$ C=F(x_{1},x_{2},&amp;hellip;,x_{m}) $$ 对于解码器Decoder，其任务是根据中间语义C和当前已经生成的历史信息来生成下一时刻要生成的单词： $$ y_{i}=G(C,y_{1},y_{2},&amp;hellip;,y_{i-1}) $$ 具体来说，就是： $$ y_{1}=G(C) $$ $$ y_{2}=G(C,y_{1}) $$ $$ y_{3}=G(C,y_{1},y_{2}) $$ 从上述公式可以看出，对于每个输出$y$对应的都是相同的语义编码$C$，而$C$是对所有输入$x$“平等对待”所求得的，因此，对于target中的任何一个单词，source中任意单词对某个目标单词$y$来说影响力都是相同的。
注意力机制  Attention是从大量信息中筛选出少量重要信息，并聚焦到这些重要信息上，忽略大多不重要的信息。权重越大越聚焦于其对应的Value值上，即权值代表了信息的重要性，而Value是其对应的信息。(可以说Key和Value是一样的东西)
理解  引入注意力机制之后，上述框架图就变成了下边这样： 即对于每一个输出$y_{i}$都有其对应的自己的语义编码信息$C_{i}$，那么这个不同的$C_{i}$就代表了对输出$x$的不同“注意力”，如下公式所示： $$ C_{1}=F(\alpha_{11}x_{1},\alpha_{12}x_{2},&amp;hellip;,\alpha_{1m}x_{m}) $$ 公式中的$\alpha$就体现了对不同输入$x$的权重，生成target的过程就变成了： $$ y_1=G(C_1) $$ $$ y_{2}=G(C_{2},y_{1}) $$ $$ y_{3}=G(C_{3},y_{1},y_{2}) $$
具体计算  Attention机制涉及到三个矩阵：$K$、$Q$、$V$，对这三个矩阵通俗的理解就是：图书馆（source）里有很多书（value），为了方便查找，我们给书做了编号（key）。当我们想要了解漫威（query）的时候，我们就可以看看那些动漫、电影、甚至二战相关的书籍。（$Q$是decoder的输出）
其计算流程图如下所示： 共包含三个步骤：
（1）根据Query和Key计算二者的相似度，得到注意力得分$s_{i}$，常见的方法包括：求两者的向量点积、求两者的向量cosine相似性或者引入额外的神经网络来求值等；</description>
    </item>
    
    <item>
      <title>ShuffleNet</title>
      <link>https://gdy0924.github.io/posts/shufflenet/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/shufflenet/</guid>
      <description>ShuffleNet 原文链接：ShuffleNet
Abstract  引入了一种计算效率非常高的CNN架构，名为ShuffleNet，它是专门为计算能力非常有限的移动设备而设计的。新的架构采用了两种新的操作，pointwise group convolution和channel shuffle，在保持精度的同时大大降低了计算成本。
Introduction  构建更深更大的卷积神经网络(CNNs)是解决主要视觉识别任务的主要趋势。最精确的CNN通常有数百层和数千个通道，因此需要的计算量非常大。但在非常有限的计算预算下，也需要追求最佳的准确性，如无人机，如无人机、机器人和智能手机等通用移动平台。但现有的许多工作都专注于剪枝、压缩或低位表示一个“基本的”网络架构。在本文，我们的目标是探索一个高效的基本结构。
性能好的网络如Xception和ResNeXt，在规模小的网络中效率较低，因为密集的1×1卷积很昂贵。我们提出使用pointwise group convolution来降低1×1卷积的计算复杂度，为了克服group convolution带来的副作用，提出一种新的操作：channel shuffle，可以使信息在通道中流通。基于上述两种技术，构建一个高效的结构：ShuffleNet。相比于其他架构，对于给定的计算复杂度，可以实现更多的特征图通道，以编码更多信息，这对于很小的网络性能是很重要的。
Related Work Efficient Model Designs  深度神经网络在计算机视觉任务中取得的成功，模型设计发挥了重要的作用。例如，与简单的叠加卷积层相比，GoogLeNet以更低的复杂度增加了网络的深度。SqueezeNet在保持精度的同时显著减少了参数和计算。ResNet利用高效的bottleneck结构来实现令人印象深刻的性能。SENet引入了一个架构单元，可以以轻微的计算成本提高性能。与此同时，最近的一项工作采用了强化学习和模型搜索来探索有效的模型设计，所提出的移动NASNet模型达到了与我们对应的ShuffleNet模型相当的性能，但是NASNet并没有报告在极小的模型上的结果，也没有评估在移动设备上的实际测试时间。
Group Convolution  Group Convolution的概念首次在AlexNet中引入，用于将模型分布在两个GPU上，在ResNeXt中很好地证明了其有效性。在Xception中提出的深度可分离卷积depthwise separable convolution。最近，MobileNet利用深度可分离的卷积，并在轻量级模型中获得了最先进的结果。我们的工作以一种新的形式推广了group convolution和depthwise separable convolution。
Channel Shuffle Operation  channel shuffle操作的想法在之前的高效模型设计中几乎没有被提到过，即使CNN库cuda-convnet支持随机稀疏卷积(random sparse convolution)层，相当于group convolution之后进行channel shuffle。这种random shuffle操作很少被使用。最近，另一个工作也采用了这个想法来进行两阶段卷积，但其并没有专门研究channel shuffle本身的有效性及其在小模型设计中的应用。
Model Acceleration  在加速推理的同时，保证预训练模型的准确性。
Approach Channel Shuffle for Group Convolutions  Xception和ResNeXt，在构建块中引入高效的深度可分离卷积或组卷积，以在表示能力和计算成本之间取得良好的权衡。然而，我们注意到，这两种设计都没有完全考虑到1×1卷积（也称为Pointwise Convolution），需要相当大的复杂性。在小型网络中，昂贵的Pointwise Convolution导致通道数量有限，可能会严重损害精度。
为了解决这个问题，一个简单的解决方法是使用通道稀疏连接，例如组卷积也同样应用在1×1卷积上。通过确保每个卷积只在相应的输入通道组上操作，组卷积大大降低了计算成本。然而，如果多个组卷积堆叠在一起，就有一个副作用：某个通道的输出只与输入通道的一小部分有关。如上图中的（a）显示了两个堆叠的组卷积。很明显，来自某一组的输出只与组内的输入有关，这阻止了通道组之间的信息交流。
如果我们允许组卷积获得来自不同组的输入数据，如上图中的（b）所示，那么输入通道和输出通道将完全相关。具体来说，对于上一组层生成的特征图，我们可以首先将每个组中的通道划分为几个子组，然后向下一层中的每个组提供不同的子组。这可以通过channel shuffle操作实现，如上图中的（c）：假设一个卷积层，有g个组，输出有g×n个通道，即每组的输出通道为n；我们首先将输出通道维度重塑为(g，n)，然后将其转平，作为下一层的输入，如下图所示： ShuffleNet Unit  利用channel shuffle操作，我们提出了ShuffleNet网络。从下图中的（a）模块出发，它是一个残差模块。其中3×3的卷积使用深度可分离卷积实现，将第一个1×1卷积替换成点组卷积紧跟着一个channel shuffle，形成一个ShuffleNet Unit，如下图的（b）所示，第二个点组卷积是为了恢复通道数目，以匹配shortcut连接。对于下采样模块，我们进行了两个修改，如下图（c）所示：（1）在shortcut通路上添加了一个平均池化层；（2）用通道连接替换元素相加，可以扩大通道，无需额外的计算成本。 由于点组卷积与channel shuffle，ShuffleNet Unit相同的设置下具有更小的复杂性，与ResNet和ResNeXt相比，例如：对于输入通道c，输出通道m，分组g的模块来说，ResNet需要$ 2cm+9m^{2}$，ResNeXt需要$ 2cm+\frac{9m^{2}}{g}$，而ShuffleNet 需要$\frac{2cm}{g}+9m$。即在相同的计算量下，ShuffleNet可以拥有更多的通道数。</description>
    </item>
    
    <item>
      <title>分组卷积&amp;可分离卷积</title>
      <link>https://gdy0924.github.io/posts/%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</guid>
      <description>分组卷积（Group Convolution）  标准的卷积如下图的上部分所示，则其参数量为：$(KernelSize\times KernelSize\times c_{1})\times c_{2}$；括号中表示一个卷积核所包含的数目，括号外表示需要的卷积核个数。
分组卷积如下图的下半部分所示，将输入特征图按照通道数分成$g$组，则其参数量为：一组卷积操作所需的参数量×组数=（一组卷积操作所需的参数量）×$g$=（一组卷积中卷积核的个数×卷积核的参数）×$g$=$(\frac{c_{2}}{g}\times (KernelSize\times KernelSize\times \frac{c_{1}}{g}))×g$=$(KernelSize\times KernelSize\times c_{1})\times c_{2}\times \frac{1}{g}$。
因此，分组卷积的参数量是标准卷积的$\frac{1}{g}$。 可分离卷积 空间可分离卷积（Spatially Separable Convolutions）  将nxn的卷积分成1xn和nx1两步计算，以3×3卷积为例：（GoogleNetV3中使用） 深度可分离卷积（Depthwise Separable Convolution）  它的核心思想是将一个完整的卷积运算分解为两步进行，分别为逐深度卷积Depthwise Convolution与逐点卷积Pointwise Convolution。
原始的卷积操作与下图所示，其参数量为：$(KernelSize\times KernelSize\times c_{1})\times c_{2}$，假设输入层为一个大小为64×64的图片，输入通道为3，输出通道为4，卷积核大小为3×3，则其具体参数量为$3×3×3×4=108$。 深度可分离卷积的第一步是逐深度卷积，如下图所示，可以看出，就是将分组卷积中的组数设置为通道数，该卷积操作的输入和输出通道数相同，这一步只融合了空间信息，缺少对通道之间的信息交流；该步的参数量为：$(KernelSize\times KernelSize\times 1)\times c_{1}$，以上述例子为例，其具体参数量为$3×3×1×3=27$。 第二步就是逐点卷积，使通道之间的信息可以流通，如下图所示，用1×1的卷积组合不同深度卷积的输出，得到一组新的输出，通过1×1卷积的个数控制输出通道数；该步的参数量为$(1\times 1\times c_{1})\times c_{2}$，其具体参数量为$1×1×3×4=12$。 因此，深度可分离卷积的总参数量为$27+12=39$，差不多是原始卷积操作（108）的1/3。
总结  分组卷积和可分离卷积都是可以用来减少参数量，通过这两种卷积可以对训练好的复杂模型进行压缩得到小模型，如ResNeXt，也可以直接设计小模型并进行训练，如MobileNet、ShuffleNet等。</description>
    </item>
    
    <item>
      <title>ResNet</title>
      <link>https://gdy0924.github.io/posts/resnet/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/resnet/</guid>
      <description>ResNet 原文连接：ResNet
Abstract  更深的网络往往更难被训练，因此提出残差学习模块，在简化网络训练的同时，加深网络的深度，可以达到152层。
Introduction  深度卷积神经网络为图像分类带来了一系列突破。深度神经网络以端到端、多层的方式集成了由低到高的特征表示，并且特征的“级别”可以通过堆叠层的数量（深度）来增加。有研究表明，网络深度是至关重要的。
深度增加的同时，会带来一个问题，即梯度消失/爆炸。然而，这个问题已经被归一化初始化和中间归一化层所解决，这使得具有几十层的网络开始收敛于反向传播的随机梯度下降(SGD)。
当更深层次的网络能够开始收敛时，出现网络退化的问题，即随着网络深度的增加，精度达到饱和，然后迅速下降。但这种网络退化不是由过拟合引起的，是由于在适当的深度模型中添加更多的层会导致更高的训练误差。
训练精度的下降表明，并不是所有的结构都同样容易进行优化。通过构造更深的模型，有一个解决方案：增加的层是恒等映射（ identity mappings），而其他层是从浅层复制过来的。这表明，一个更深的模型不会比它较浅的模型产生更高的训练误差。
在本文中，引入深度残差学习框架解决网络退化问题，我们不是希望每几个堆叠层直接匹配所需的映射$H(x)$，而是匹配$F(x)$的另一个映射，即残差映射$H(x)-x$，那么原始的映射就变成了$F(x)+x$。在极端情况下，如果一个恒等映射是最优的，那么将残差推到零要比用一堆非线性层来拟合一个恒等映射更容易。
公式$F(x)+x$可用Shortcut Connection实现，并且Shortcut Connection只需实现恒等映射，既不增加额外参数也不增加计算量。
我们在ImageNe上进行了全面的实验来证明退化问题，并评估我们的方法，得到：（1）深度残差网很容易优化，但对应的“普通”网络（简单的堆叠层）表现出更高的训练误差；（2）深度残差网可以很容易地从大大增加的深度中获得精度，产生的结果比以前的网络更好得多
Related Work Residual Representations  残差表示作为一种重构或预处理方法是有效的。首先，在图像识别任务中的VLAD （矢量量化），有研究表明编码残差矢量比编码原始矢量更有效；其次，在低级视觉和计算机图形学中，需要求解部分微分方程，也被证明这些求解器比没有考虑残差性质的标准求解器收敛得快得多。
Shortcut Connections  训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出。在一些研究中，某些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸（如GoogleNet）。
同时，Highway Networks（下图）实现了带有门控单元的shortcut connection，需要参数控制，而 ResNet没有，这样就不会增加额外参数。（Highway Networks相当于对输入一部分进行处理（和传统神经网络相同），一部分直接通过）另外Highway Networks并没有精度与网络深度的显著提高（没有超过100层）。 Deep Residual Learning Residual Learning  假设存在映射$H(x)$，我们不是期望堆叠的层映射近似$H(x)$，而是让这些层近似于一个残差函数$F(x):=H(x)-x$。原来的映射就变成了$F(x)+x$，虽然这两种形式都应该能够渐近地近似于所期望的函数，但从之前的研究可以看出，其学习的容易程度可能是不同的，残差收敛速度更快并且效果更好。（其主要思想是将堆叠的非线性层从，拟合原来的最优解映射输出$H(x)$，变成去，拟合输出和输入的差$F(x) = H(x) - x$，此时原最优解映射$H(x)$就可以改写成$F(x) + x$。而作者认为这两种表达的效果相同，但是优化的难度却并不相同，）
正如之前说的那样，如果增加的层可以构造成恒等映射，那么更深的模型的训练误差应该不大于对应较浅的模型。而通过多个非线性层逼近恒等映射是存在困难的，因此，通过残差学习重构，如果恒等映射是最优的，模型就可以简单地将多个非线性层的权值（即$F(x)$）推向零，以接近恒等映射。但在实际情况下，恒等映射不太可能是最优的。
Identity Mapping by Shortcuts  残差块的公式定义和结构图如下所示： $$ y=F(x,\lbrace W_{i} \rbrace)+x $$ 其中，$x$和$y$是输入与输出，函数$F(x,\lbrace W_{i} \rbrace)$是要学习的残差映射。对应图中的两层的例子。$x$和$F(X)$通过Shortcut Connection和元素级相加实现的。公式中的Shortcut Connection不会引入额外的参数也不引入计算复杂度。同时$x$和$F(X)$的维度必须相同，如果不相同，通过$W_{s}$实现维度匹配： $$ y=F(x,\lbrace W_{i} \rbrace)+W_{s}x $$ 残差函数$F$的形式是灵活的。本文实验涉及两层或三层的函数$F$，也可能有更多的层。但如果只有一层，类似于线性层：$y=W_{1}x+x$，，从实验可以看出没有效果。</description>
    </item>
    
    <item>
      <title>GoogleNet</title>
      <link>https://gdy0924.github.io/posts/googlenetcode/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/googlenetcode/</guid>
      <description>GoogLeNet获得了2014年ILSVRC比赛分类任务的冠军，其利用Inception模块，在加深网络深度的同时，减少了参数量，从而减少计算资源的利用。
实现 基于pytorch实现的代码如下：
1import torch 2import torch.nn as nn 3import torch.nn.functional as F 4 5#卷积+BN+ReLu 6class BasicConv2d(nn.Module): 7 def __init__(self, in_channels, out_channals, **kwargs): 8 super(BasicConv2d, self).__init__() 9 self.conv = nn.Conv2d(in_channels, out_channals, **kwargs) 10 self.bn = nn.BatchNorm2d(out_channals) 11 12 def forward(self, x): 13 x = self.conv(x) 14 x = self.bn(x) 15 return F.relu(x) 16 17# Inception模块 18class Inception(nn.Module): 19 #（输入通道数，1×1卷积的输出通道数，3×3卷积的降维通道数，3×3卷积的输出通道数， 20 #5×5卷积的降维通道数，5×5卷积的输出通道数，池化通道数） 21 def __init__(self, in_planes,n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes): 22 super(Inception, self).</description>
    </item>
    
    <item>
      <title>GoogleNet</title>
      <link>https://gdy0924.github.io/posts/googlenetpaper/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/googlenetpaper/</guid>
      <description>GoogLeNet获得了2014年ILSVRC比赛分类任务的冠军，其利用Inception模块，在加深网络深度的同时，减少了参数量，从而减少计算资源的利用。
GoogLeNetV1 原文链接：GoogleNetV1
Abstract  提出一种名为“Inception”的深度卷积神经网络架构，可以增加网络的深度和宽度，同时保持计算量不变，从而提高网络精度。
Introduction  随着深度学习的发展，卷积网络在图像领域发展迅速，算法的效率，特别是计算量和内存的使用，变得越来越重要，在该网络中，我们也考虑到了这一方面。我们提出了一种更加高效的深度神经网络架构，名为“Inception module”，直接增加了网络的深度。
Motivation and High Level Considerations  提高深度神经网络性能的最直接的方法是增加网络的规模，包括：网络的深度（层数）和宽度（每一层的单元数量）。但存在两个问题：
（1）更大的尺寸通常意味着大量的参数，这使得扩大后的网络更容易发生过拟合，特别是在训练集有限的情况下；
（2）网络的扩大伴随着计算资源的使用显著增加，如果增加的部分使用效率较低（例如，如果大多数权重最终接近于零），那么就会浪费大量的计算。
解决上述问题的根本方法是：将完全连接转换成稀疏连接，一方面模拟了生物系统，另一方面，有研究证明了稀疏网络的有效性。
之前的研究为了打破网络对称性和提高学习能力，都使用了随机稀疏连接，但是，计算机软硬件对非均匀稀疏数据的计算效率很差，所以在AlexNet中又重新使用了全连接层，目的是为了更好地优化并行运算。
因此，需要一种网络结构，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能，提出“Inception”结构来实现此目的。
Architectural Details 上图是提出的最原始的基本结构：
（1）采用不同大小的卷积核实现不同大小的感受野，在模块最后进行拼接实现不同尺度特征的融合；
（2）卷积核大小采用1、3和5，是为了方便对齐，即在设定卷积步长stride=1后，只要分别设定padding=0、1、2，不同卷积核操作之后得到的特征图大小就相同，方面最后的拼接操作；
（3）由于pooling操作被证明很有效，因此引入了池化分支；
（4）网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例会增加。
上述模块存在一个大问题，在该结构中，即使具有大量卷积核的卷积层中，少量的5×5卷积也会带来巨大的计算量，并且由于卷积结果与池化结果进行合并，因此从上一个模块到下一个模块，输出通道肯定要增加。
因此提出了改进的模块，在计算量大的地方引入降维操作，即在3×3和5×5卷积操作之前，先利用1×1卷积进行通道数的减少，从而减少网络参数，在减少计算量的同时，做一定的正则化，同时还引入了更多的非线性变换。改进的模块如下所示： 该结构的好处是：
（1）允许在每个阶段显著增加单元的数量，而不会导致计算复杂性的不受控制的爆炸；
（2）与直觉相一致，即视觉信息应该在不同的尺度上进行处理，然后进行聚合，以便下一阶段可以同时从不同的尺度上提取特征。
GoogLeNet 上图为完整的GoogLeNet网络结构，由于技术的原因，先进行传统的卷积层，只在较高的层开始使用Inception模块，其特点如下：
（1）该网络有22层深（如果加上池化层，是27层）；
（2）实验发现，用平均池化代替全连接层能够提高精度，因此使用了average pooling，同时使用dropout，但是依旧在网络最后添加了一个额外的全连接层，主要是为了更加方便地调整和微调网络；
（3）由于网络的深度相对较大，因此需要解决梯度回传的问题，在网络中间的层，额外添加了两个辅助的softmax用于向前传导梯度，在训练过程中，它们的损失以折扣权值加到网络的总损失中（辅助分类器的损失加权为0.3），在测试时这两个额外的softmax会被去掉，辅助分类器的结构如下：
（a）5×5大小的平均池化，stride=3；
（b）1×1×128卷积操作，用于降维和非线性激活； （c）1024个神经元的全连接层； （d）p=70%的Dropout； （e）以Softmax损失作为分类器的线性层，分类数为1000。
Conclusions  通过构建密集的块结构来近似最优的稀疏结构，从而达到提高性能而又不大量增加计算量的目的，是一个可行的方法，通过实验也证明了Inception模块的优势。
GoogleNetV2 原文链接：GoogleNetV2
Inception V2学习了VGGNet，用两个3×3的卷积代替5×5的大卷积（用以降低参数量并减轻过拟合），还提出了著名的BatchNormalization方法。BN是一个非常有效的正则化方法，可以让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅提高（详见BatchNormalization）。
GoogleNetV3 原文链接：GoogleNetV2
General Design Principles  提出一些基于使用卷积网络的各种架构选择的设计原则，包含一定的推测，不过经过实验证明基本是有效的：
（1）avoid representational bottlenecks 避免表征瓶颈：（特别是在网络浅层）信息流前向传播过程中显然不能经过高度压缩的层，即表征瓶颈，从输入到输出，特征图的宽和高基本都会逐渐变小，但是不能一下子就变得很小；
（2）higher dimensional representations 高维特征： 高维特征更容易区分和处理，从而会加快训练；
（3）spatial aggregation 空间聚合： 空间聚合可以在低维嵌入上进行，而不会造成太多或任何表征能力的损失，同时还可以提高速度，比如在进行3x3卷积之前，可以对输入先进行降维而不会产生严重的后果；</description>
    </item>
    
    <item>
      <title>Open World Object Detector</title>
      <link>https://gdy0924.github.io/posts/open-world-object-detector/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/open-world-object-detector/</guid>
      <description>原文链接：ORE
Abstract  开放世界对象检测（Open World Object Detector）模型的任务是：识别未知的（unknown）对象， 即没有明确监督标签；当相应的标签逐步被接收到时，可学习到未知对象的类别，并且不忘记之前学习到的标签。以上是该论文提出的问题，并引入了一个新的模型：ORE（Open World Object Detector），该模型基于对比聚类和基于能量的位置对象识别。
Introduction  深度学习中目标检测任务是识别和定位图像中的物体。现有的研究都有一个强有力的假设：要检测的所有类别都在训练阶段给出。当上述假设不存在，那么就有两个具有挑战性的问题：测试图像可能包含未知类别，这些对象应被归为未知类（unknown）；当关于未知类别的对象信息可用时，模型应该逐步学习这些新的类别。该论文提出了一个新的问题：一个模型应该能够将未知对象识别为未知类，在训练数据阶段以统一的方式逐步学习和识别他们。
contributions 1、引入一个新的问题Open World Object Detector；
2、提出一种新方法ORE，基于对比聚类，一个未知提议网络和基于能量的未知类识别；
3、提出一个新的全面的实验设置；
4、所提出的方法在增量目标检测上取得了最优异的成绩。
Related Work Open Set Classification  虽然这种模型可以识别出未知的实例，但它们不能在多个训练过程中以增量的方式动态地更新自己，即无法学习未知类别。
Open World Classification  这种模型可以识别已知类和未知类的对象，并在提供未知的新标签时自适应地改进自己，但是他们没有在图像分类基准上进行测试。
Open Set Detection  一些研究发现传统物体检测模型经常将未知类别的物体识别成已知的某一类别，因此其处理方式通常为：1. 新增一个background类别 2. 去除未知类别的物体。但是上述两个方法都不能在真实动态环境中使用。
Open World Object Detection  在任意时刻$t$，我们已知类别表示为$K^{t}= \lbrace 1,2,&amp;hellip;,C \rbrace \subset N^{+}$（共$C$个类别），其中$N^{+}$代表正整数的集合，同时假设存在一组未知类别$U=\lbrace C+1,&amp;hellip; \rbrace$，在推理过程中可能会遇到。已知类别$K_{t}$在数据集$D^{t}=\lbrace X^{t},Y^{t} \rbrace$中被标记，其中$X$、$Y$分别表示输入图像和标签。输入图像集由$M$张训练图像组成，$X^{t}=\lbrace I_{1},&amp;hellip;,I_{M} \rbrace$以及每张图像对应的标签集$Y^{t}=\lbrace Y_{1},&amp;hellip;,Y_{M} \rbrace$。每个$Y_{i}=\lbrace y_{1},y_{2},&amp;hellip;,y_{K} \rbrace$代表$K$个对象实例的类别标签和定位（即一张图像上有$K$个被标记的对象实例），并且$y_{k}=\left [ l_{k},x_{k},y_{k},w_{k},h_{k} \right ]$，其中$l_{k}$代表类别，$ x_{k}$，$y_{k}$、$w_{k}$、$h_{k}$分别代表 bounding box的中心坐标、宽和高。</description>
    </item>
    
    <item>
      <title>VGG</title>
      <link>https://gdy0924.github.io/posts/vggcode/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/vggcode/</guid>
      <description>VGG
实现 基于pytorch实现的代码如下：
1import torch.nn as nn 2import torch.nn.functional as F 3from torchsummary import summary 4 5class VGG(nn.Module): 6 &amp;#34;&amp;#34;&amp;#34; 7VGG builder 8&amp;#34;&amp;#34;&amp;#34; 9 def __init__(self, arch: object, num_classes=1000) -&amp;gt; object: 10 super(VGG, self).__init__() 11 self.in_channels = 3 12 self.conv3_64 = self.__make_layer(64, arch[0]) 13 self.conv3_128 = self.__make_layer(128, arch[1]) 14 self.conv3_256 = self.__make_layer(256, arch[2]) 15 self.conv3_512a = self.__make_layer(512, arch[3]) 16 self.conv3_512b = self.__make_layer(512, arch[4]) 17 self.fc1 = nn.Linear(7*7*512, 4096) 18 self.bn1 = nn.</description>
    </item>
    
    <item>
      <title>VGG</title>
      <link>https://gdy0924.github.io/posts/vggpaper/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/vggpaper/</guid>
      <description>原文链接：VGG
VGG网络获得了2014年ILSVRC比赛的分类任务的亚军和定位任务的冠军，其主要贡献是通过更小的卷积核堆叠构建更深层的网络。 Abstract  该论文研究了卷积网络深度对精度的影响，通过使用小的卷积核（大小为3×3）加深网络的深度，将深度增加到了16-19层，并取得了不错的效果。
Introduction  卷积神经网络在大规模图像核视频识别方面取得了巨大成功。在该论文中，我们关注卷积网络的另一个方面——网络的深度。通过添加更多的卷积层来增加网络的深度，并且使用的卷积核大小很小。
ConvNet Configurations Architecture  输入为224×224大小的RGB图像，每层都使用3×3大小的卷积核提取特征，固定步长为1，填充为1，以保证卷积前后的特征图大小相同。最大池化的大小为2×2，步长也为2，即特征图缩小一半，并且不是所有的卷积层后边都跟池化层。
卷积层后跟着三个全连接层，前两个有4096个通道，最后一个有1000个通道（因为比赛中的类别数是1000），最后一层是Softmax层。
每一层后都跟着非线性激活层，并且我们没有用到（除了一个架构）AlexNet所提出的LRN层，通过实验发现LRN层并没有提高ILSVRC数据集上的性能，但却导致了内存消耗和计算时间的增加。
Configurations 从图中可以看出，不同网络的架构是相同的，除了深度不同，从11层的网络（8层卷积和3个FC层）到19层网络（16层卷积和3个FC层）。通道数由最开始的64，每次操作都使通道数增加一倍，直到512层。由于卷积核小，因此虽然网络很深，但其网络参数没有很多。
Discussion  不同于之前的网络，我们没有在网络第一层使用较大的感受野，如11×11或7×7，而是利用小的感受野（大小为3×3）贯穿整个网络结构。可以看出，两个3×3大小的感受野堆叠起来的感受野与一个5×5大小的感受野相同，三个3×3大小的感受野堆叠起来的感受野与一个7×7大小的感受野相同。使用三个3×3大小的感受野代替一个7×7大小的感受野的好处有：（1）三层感受野就引入了三次非线性变换，因此增加了网络的非线性表达能力；（2）三个3×3大小的感受野共有27个参数，而一个7×7大小的感受野有49个参数，因此可以减少网络的参数数量，可以看作对网络做了相应的正则化。
在16层的网络中，我们使用得到了1×1的卷积核，在不改变感受野的前提下增加了非线性，同时输入通道核输出通道是相同的。
GoogleNet与我们的相似之处就是它是基于较深的网络（22层）和小卷积核（除了3×3，他们还用到了1×1和5×5）。他们的网络结构比我们更加复杂 ，并且在第一层就将图像分辨率降低了很多，以减少计算量。
Classification Framework Training  使用具有动量的小批量梯度下降算法，Dropout，对输入图像进行随机剪裁、随机水平翻转和随机RGB颜色位移等操作。
考虑了两种训练图像尺寸S的方法：
（1）单尺寸训练：在实验中，使用两种尺寸大小验证模型，S=256和S=384。给定一个卷积网络，我们首先使用S=256训练网络。为了加快S=384网络的训练，我们使用S=256预训练的权值初始化；
（2）多尺寸训练：每个训练图像在一定范围内$\left [ S_{min},S_{max} \right ]$随机采样S，其中，$S_{min}=256,S_{max}=512$。我们通过微调单尺度模型来训练多尺度模型，并使用固定的S=384进行预训练。
Testing  在测试阶段，首先将输入图像缩放至最小的尺寸$Q$，同时$Q$并不一定等于训练中的$S$，在训练阶段，每个$S$对应的网络，使用多个不同的$Q$对其进行测试。接着将网络最后三层的全连接层改为卷积层，第一个FC层改为7×7×4096的卷积层，第二个改为1×1×4096，第三个改为1×1×1000，可以看出，最终的通道数对应的是分类数。通过改为卷积层，可以接受输入大小不同的图片，只是最终得到的特征图的大小不同，但其通道数依旧对应分类数，可以通过对最终的特征图进行空间平均（求和，sum-pooled）得到Softmax输入之前的分数。我们还通过图像的水平翻转来增加测试集，将原始图像和翻转图像的Softmax分数进行平均，以获得图像的最终分数。 Conclusion  提出了比较深的卷积网络（19个权重层）用于大尺度的图像分类，实验结果表明，更深层的网络有利于提高分类精度。</description>
    </item>
    
    <item>
      <title>Relevance-CAM</title>
      <link>https://gdy0924.github.io/posts/relevance-campaper/</link>
      <pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/relevance-campaper/</guid>
      <description>原文链接：Relevance-CAM
Abstract  目前在深度学习的可解释性领域，一种主要的常见的是基于类激活映射(Class Activation Mapping, CAM)的方法，常用在卷积网络的最后一层。在本文中，提出了一种相关加权类激活映射(Relevance-weighted Class Activation Mapping, Relevance-CAM)，以解决基于梯度的CAM在梯度消失方面的问题，并且该方法在网络的任意一层都可以应用。
Introduction  随着深度神经网络的不断发展，可解释网络也被广泛研究，其方法主要分为基于类激活映射(CAM)的和基于分解的。基于CAM的方法通过全局平均池化得到每个通道的权重，接着利用最后一层卷积的特征图进行加权线性求和操作得到可视化的结果。但是该方法只能用在最后一层，且必须用到全局平均池化和全连接层。基于梯度的CAM，Grad-CAM和 Grad-CAM++ 对其进行改进，使用梯度作为权重。层相关性传播(Layer-wise Relevance Propagation, LRP)是基于分解的方法。LRP通过特定的传播规则将最终的输出分数向后传播，重新分配到输入图像中，从而解决了梯度消失的问题。 Contributions 1、提出了Relevance-CAM，解决了梯度消失的问题，同时在中间层也可以工作； 2、通过可视化热图，展示了我们所提出的方法优于其他方法； 3、通过Average Drop、Average Increase和Intersection over Union三种客观数值证明我们所提出的方法优于其他方法； 4、我们通过实验发现，即使在浅层，网络也可以提取到类特定信息。
Background CAM 原文链接：CAM 类激活映射(CAM)通过全局平均池化层获得权重，接着将最后一个卷积层的特征图进行线性加权求和，如下图所示。但是该方法有很大的局限性，只能用于最后一层的可视化，并且要在原网络的基础上增加全局平均池化层。 Grad-CAM 原文链接：Grad-CAM Grad-CAM改变了权重计算的方法，利用最后输出分数对某一层特征图的梯度作为权重，以解决CAM所存在的问题。 其计算公式如下：
$$ L_{Grad-CAM}^{c}=\sum_k\alpha _k^{c}A_k $$
$$ \alpha _k^{c}=GP\left ( \frac{\partial y^{c}}{\partial A_k} \right ) $$
其中，$ A_k $是第$ k $个通道的特征图，$ y^{c} $是类别$ c $对应的得分，$\alpha _{k}^{c}$ 代表权重，GP()表示全局平均函数。
Layer-wise Relevance Propagation(LRP) 原文链接：LRP LRP通过分层地将相关性分数从输出传播到输入图像，从而获得原始图像中每个像素的相关性分数，并且传播过程满足守恒且分数恒不小于零，如下所示： $$ \forall x:f_{c}\left ( x \right )=\sum_{p}R_{p}^{l}\left ( x \right ) $$</description>
    </item>
    
    <item>
      <title>BatchNormalization</title>
      <link>https://gdy0924.github.io/posts/batchnormalization/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/batchnormalization/</guid>
      <description>批归一化主要用在非线性激活函数层前，它不仅可以加快模型训练时的收敛速度，而且还能够使模型训练过程更加稳定，避免梯度爆炸或者梯度消失，也起到一定的正则化作用，所以在目前的网络中被广泛使用到。
计算步骤及公式 1、计算均值与方差 首先，对于输入的数值集合$ B= $ $\lbrace$ $ x_{1&amp;hellip;m} $ $\rbrace$，计算其均值$ \mu _{B} $和方差$ \sigma _{B}^{2} $，如下所示： $$ \mu _{B}=\frac{1}{m} \sum_{i=1}^{n} {x_i} $$
$$ \sigma _{B}^{2} = \frac{1}{m} \sum_{i=1}^{m} (x_i- \mu _{B})^{2} $$
2、数据标准化 将集合$ B $转化为均值为0、方差为1的正态分布$ \tilde{x_{i}} $，其中$ \epsilon $是引入的一个极小常数，以防止在公式中出现除零的情况 $$ \tilde{x_{i}}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\epsilon }} $$
3、BN 引入可训练参数$ \gamma $和$ \beta $，对数据实现平移和缩放操作，通过引入的两个还原参数，可以使网络学习到原始的特征分布，在一定程度上保留了原始数据的分布 $$ y_{i}=\gamma \tilde{x_{i}}+\beta
$$
在全连接层中，BN作用在特征维上，而在卷积层中，BN作用在通道维上，具体来说，就是对feature map的通道方向求均值和方差，即假设batch size=n, feature map的shape= (w, h, c), 则会对c个$nwh $的特征分别求出c个均值和方差。
预测阶段的BN 在训练阶段，可能没有许多的数据来形成一整个batch，可能只用单个数据来进行测试，这时，单个数据就无法计算上边公式中的均值与方差。 在训练阶段，针对每一个batch使用BN公式计算出的均值 $ \mu= $ $\lbrace$ $\mu ^{1},\mu ^{2},&amp;hellip;,\mu ^{n} $ $\rbrace$ 和 $ \sigma =$ $\lbrace$ $\sigma ^{1},\sigma ^{2},&amp;hellip;,\sigma ^{n} $ $\rbrace$ ，分别计算，其中$ p $是新引入的可调节参数。 $$ \bar{\mu }=p\bar{\mu }+\left ( 1-p \right )\mu ^{t} $$</description>
    </item>
    
    <item>
      <title>AlexNet</title>
      <link>https://gdy0924.github.io/posts/alexnetcode/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/alexnetcode/</guid>
      <description>AlexNet在在2012年的ImageNet竞赛中取得了冠军，作为第一个深度卷积网络在该比赛中获得如此好的成绩，AlexNet证实了深度卷积网络的潜力，并引起了研究者们的极大热情。 AlexNet共包含8层，其中前5层为卷积层，后三层为全连接层，最后一个全连接层的输出是1000维，输入softmax产生最终的输出：1000类的标签分布。
实现 基于pytorch实现的代码如下：
1 2import torch 3from torch import nn 4from d2l import torch as d2l 5 6net = nn.Sequential( 7 nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),nn.ReLU(), 8 nn.MaxPool2d(kernel_size=3,stride=2), 9 nn.Conv2d(96,256,kernel_size=5,padding=2),nn.ReLU(), 10 nn.MaxPool2d(kernel_size=3,stride=2), 11 nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(), 12 nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(), 13 nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLU(), 14 nn.MaxPool2d(kernel_size=3,stride=2),nn.Flatten(), 15 nn.Linear(6400,4096),nn.ReLU(),nn.Dropout(p=0.5), 16 nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(p=0.5), 17 nn.Linear(4096,10)) 18 19X = torch.randn(1,1,224,224) 20for layer in net: 21 X=layer(X) 22 print(layer.__class__.__name__,&amp;#39;output shape:\t&amp;#39;,X.shape) 23 24batch_size=128 25train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=224) 26 27def evaluate_accuracy_gpu(net,data_iter,device=None): 28 if isinstance(net, nn.Module): 29 net.eval() 30 if not device: 31 device = next(iter(net.</description>
    </item>
    
    <item>
      <title>AlexNet</title>
      <link>https://gdy0924.github.io/posts/alexnetpaper/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/alexnetpaper/</guid>
      <description>AlexNet在在2012年的ImageNet竞赛中取得了冠军，作为第一个深度卷积网络在该比赛中获得如此好的成绩，AlexNet证实了深度卷积网络的潜力，并引起了研究者们的极大热情。 AlexNet共包含8层，其中前5层为卷积层，后三层为全连接层，最后一个全连接层的输出是1000维，输入softmax产生最终的输出：1000类的标签分布。
原文链接：AlexNet
网络架构 输入层 输入图片的大小为224×224，并包含R、G、B三个通道。对于原始的数据，AlexNet进行了数据增强操作。
Layer1 输入图片：224×224×3（227×227×3） 卷积核：11×11×96 步长(stride)：4 填充(padding)：1 输出大小：54×54×96（55×55×96） 池化：size=3×3，stride =2, padding=0 第一个卷积层使用96个11×11的卷积核对图片进行特征提取，并且经过ReLU、LRN层和最大池化层得到第一层的输出，大小为27×27×96。
Layer2 输入图片：27×27×96 卷积核：5×5×256 步长(stride)：1 填充(padding)：2 输出大小：27×27×256 池化：size=3×3，stride =2, padding=0 第二个卷积层使用256个5×5的卷积核对图片进行特征提取，并且经过ReLU、LRN层和最大池化层得到第二层的输出，大小为13×13×256。
Layer3 输入图片：13×13×256 卷积核：3×3×384 步长(stride)：1 填充(padding)：1 输出大小：13×13×384 第三个卷积层使用384个3×3的卷积核对图片进行特征提取，并且经过ReLU非线性激活得到第三层的输出，大小为13×13×384。
Layer4 输入图片：13×13×384 卷积核：3×3×384 步长(stride)：1 填充(padding)：1 输出大小：13×13×384 第四个卷积层使用384个3×3的卷积核对图片进行特征提取，并且经过ReLU非线性激活得到第四层的输出，大小为13×13×384。
Layer5 输入图片：13×13×384 卷积核：3×3×256 步长(stride)：1 填充(padding)：1 输出大小：13×13×256 池化：size=3×3，stride =2, padding=0 第五个卷积层使用256个3×3的卷积核对图片进行特征提取，并且经过ReLU非线性激活和池化层得到第五层的输出，大小为6×6×384。
全连接层 第6、7、8层都为全连接层，并且每层的神经元个数都为4096个，最后经过softmax得到最终1000个类别的分类结果。
Innovation创新点 ReLU非线性激活 AlexNet是第一个使用ReLU函数作为激活函数的网络，之前使用最多的激活函数是Sigmiod函数，函数图像如下所示。可以看出，Sigmiod函数在输入x的值很大或者很小的时候，其梯度非常小，几乎接近于0，那么在反向传播过程中，由于梯度的链式法则，就会导致网络的浅层得到的梯度为0，无法正常更新权重，因此AlexNet就提出可以使用ReLU函数来解决梯度消失的问题。ReLU函数在输入x大于0时，其梯度一直为1，解决了梯度消失问题，并且在输入x小于0时，输出为0，就使得网络更加稀疏，从而减少了参数的相互依存关系，缓解了过拟合问题。 多GPU 由于当时GPU内存的限制，AlexNet将网络放在2两GPU上进行训练，从网络架构图中可以看出，每一层都是将通道数一份为2，分别放在不同的GPU上，并且规定GPU只能在特定的层进行通信交流。
LRN 虽然使用ReLU函数不需要再进行标准化，不过实验表明局部响应标准化(Local Response Normalization)有助于泛化。其公式如下： $$ b_{x,y}^{i}=a_{x,y}^{i}/\left ( k+\alpha \sum_{j=max\left ( 0,i-n/2 \right )}^{min\left ( N-1,i+n/2 \right )}\left ( a_{x,y}^{j} \right )^{2}\right )^{\beta } $$ 其中, $ a_{x,y}^{i} $表示经过激活函数ReLU得到的特征图对应位置为(x,y)的输出值，$ b_{x,y}^{i} $ 表示经过LRN后的输出值，$ N $ 为卷积核的个数，$k$、$n$、$\alpha$、$\beta$为超参数，在该论文中设置的分别为：$k=2$，$n=5$，$\alpha=10^{-4}$，$\beta=0.</description>
    </item>
    
    <item>
      <title>LeNet</title>
      <link>https://gdy0924.github.io/posts/lenetcode/</link>
      <pubDate>Sun, 26 Dec 2021 20:58:21 +0800</pubDate>
      
      <guid>https://gdy0924.github.io/posts/lenetcode/</guid>
      <description>LeNet是很简单的一个经典卷积神经网络，主要用于手写数字识别，所以是一个多分类任务，并且是十个类别。
实现 基于pytorch实现的代码如下：
1 2import torch 3from torch import nn 4from d2l import torch as d2l 5 6class Reshape(torch.nn.Module): 7 def forward(self,x): 8 return x.view(-1,1,28,28) 9 10net = torch.nn.Sequential( 11 Reshape(),nn.Conv2d(1,6,kernel_size=5,padding=2),nn.Sigmoid(), 12 nn.AvgPool2d(kernel_size=2,stride=2), 13 nn.Conv2d(6,16,kernel_size=5),nn.Sigmoid(), 14 nn.AvgPool2d(kernel_size=2,stride=2),nn.Flatten(), 15 nn.Linear(16*5*5,120),nn.Sigmoid(), 16 nn.Linear(120,84),nn.Sigmoid(), 17 nn.Linear(84,10)) 18 19X = torch.rand(size=(1,1,28,28),dtype=torch.float32) 20for layer in net: 21 X = layer(X) 22 print(layer.__class__.__name__,&amp;#39;output shape：\t&amp;#39;,X.shape) 23 24batch_size=256 25train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size) 26 27def evaluate_accuracy_gpu(net,data_iter,device=None): 28 if isinstance(net, nn.Module): 29 net.</description>
    </item>
    
    <item>
      <title>LeNet</title>
      <link>https://gdy0924.github.io/posts/lenetpapar/</link>
      <pubDate>Sun, 26 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/lenetpapar/</guid>
      <description>LeNet是很简单的一个经典卷积神经网络，主要用于手写数字识别，所以是一个多分类任务，并且是十个类别。该网络架构如下图所示，每一层分别是卷积层、池化层、卷积层、池化层、卷积层和全连接层，最后连接Softmax实现分类。
原文链接：LeNet
网络架构 输入层 将输入图像的尺寸统一归一化为32×32×1，其中第一个32代表图片的高度，第二个32代表图片的宽度，1是指通道数，由于对应的数据集是黑白的，所以其通道数为1，对于彩色的图片，通道数为3，分别对应R、G、B。
第一层：卷积层 输入图片：32×32×1 卷积核：5×5×6 步长：1 输出大小：28×28×6 神经元数量：28×28×6 参数个数：(5×5+1)×6 第一层为卷积层，使用6个大小为5×5的卷积核，提取图片的feature map，得到6个大小为28×28的feature map。
第二层：池化层 输入：28×28×6 核大小：2×2×6 步长：2 输出大小：14×14×6 神经元数量：14×14×6 参数个数：2×6 通过池化层对图像进行下采样，在该层采用的是最大池化，即选择区域中的最大值作为采样的值，除了最大池化外，还有平均池化等。
第三层：卷积层 输入图片：14×14×6 卷积核：5×5×16 步长：1 输出大小：10×10×16 参数：6×(3×5×5+1)+6×(4×5×5+1)+3×(4×5×5+1)+1×(6××5+1) 该层使用16个大小为5×5的卷积核，对大小为14×14、通道数为6的feature map进行卷积，，最终得到16个大小为10×10的feature map。 对于该层的16个卷积核，其中前六个与上一层的相连三个feature map相对应，接着六个卷积核与上一层的相连四个feature map相对应，接下来的三个与上一层的部分不相连的四个feature map相对应，最后一个卷积核与上一层得到的所有feature map对应。
第四层：池化层 输入：10×10×16 核大小：2×2×16 步长：2 输出大小：5×5×16 神经元数量：5×5×16 参数：2×16 对16个10×10大小的feature map进行最大池化，得到16个大小为5×5的feature map。
第五层：卷积层 输入图片：5×5×16 卷积核：5×5×120 步长：1 输出大小：1×1×120 神经元数量：28×28×6 参数：(16×5×5+1)×120 该层使用120个大小为5×5的卷积核，对图片进行卷积操作，得到120个大小为1×1的feature map。
第六层：全连接层 输入大小：120 输出大小：84 该层为全连接层，共有84个神经元。
输出层 输入大小：84 输出大小：10 该层为全连接层，包含10个神经元，对应最后的十个分类情况。
特点 在S2与C3之间，输入的feature map和输出的feature map之间并不是全连接的，而是局部连接的，如图所示。其中行对应的是C3的feature map，列对应的是S2的feature map。以第0列为例，C3的第一个feature map是由S2的前三个feature map经过卷积核操作得到的，而C3的第七个feature map，也就是第6列，是由S2的前四个feature map经过卷积核操作得到的。 </description>
    </item>
    
    <item>
      <title>模型评价指标</title>
      <link>https://gdy0924.github.io/posts/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</link>
      <pubDate>Sat, 11 Dec 2021 15:52:51 +0800</pubDate>
      
      <guid>https://gdy0924.github.io/posts/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</guid>
      <description>对于训练好的模型，我们通常更加关注该模型在未知数据上的性能“好坏”，也就是模型的泛化能力如何。要对模型的泛化性能进行评估，就需要有衡量模型泛化能力的评价标准，即评价指标，或者称为性能度量。针对两种不同的任务类型：分类任务和回归任务，有各自不同的评价指标。
回归任务 回归任务简单来说就是对连续值进行预测，比如：面积大小、数量多少等，最常用的性能度量是平均绝对误差(MAE)和均方误差(MSE)。
MAE 平均绝对误差就是计算预测值与真实值之间的距离，公式如下：
$$ MAE=\frac{1}{n}\sum_{i=1}^{n}\left | y_{i}-\hat{y_{i}} \right | $$
MSE 均方误差就是计算预测值与真实值之间距离的平方和，公式如下：
$$ MSE=\frac{1}{n}\sum_{i=1}^{n}\left ( y_{i}-\hat{y_{i}} \right )^{2} $$
分类任务 分类任务简单来说就是对离散值进行预测，比如是不是、属不属于、或者属于哪一类，最常用的性能度量是准确率、错误率、精确率、召回率和F1-score等。 在分类任务中，基础指标是混淆矩阵，在混淆矩阵的基础上可以产生精确率、召回率等不同的评价指标。
混淆矩阵 当把数据集中的正负样本分开来看时，将会产生以下四个指标： TP(True Positive)：真正例，即该样本的真实标签为正类，预测也为正类 TN(True Negative)：真反例，即该样本的真实标签为负类，预测也为负类 FP(False Positive)：假正例，即该样本的真实标签为负类，但预测为正类 FN(False Negative)：假反例，即该样本的真实标签为正类，但预测为负类 将上述四个指标放在一个矩阵中，即可得到混淆矩阵。
真实情况预测结果正例反例正例TPFN反例FPTN错误率(Error Rate) 即分类错误的样本数占样本总数的比例，公式如下：
$$ ErrorRate=\frac{FN+FP}{TP+FN+FP+TN} $$
准确率(Accuracy) 即分类正确的样本数占样本总数的比例，公式如下：
$$ ACC=\frac{TP+TN}{TP+FN+FP+TN} $$
精确率(Precision) 又称查准率，即被预测为正例的样本中真实标签为正例的比例，公式如下：
$$ P=\frac{TP}{TP+FP} $$</description>
    </item>
    
    <item>
      <title>test</title>
      <link>https://gdy0924.github.io/posts/test/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gdy0924.github.io/posts/test/</guid>
      <description>原文 壬戌之秋，七月既望，蘇子與客泛舟遊於赤壁之下。清風徐來，水波不興。舉酒屬客，誦明月之詩，歌窈窕之章。少焉，月出於東山之上，徘徊於斗牛之間。白露橫江，水光接天。縱一葦之所如，凌萬頃之茫然。浩浩乎如馮虛御風，而不知其所止；飄飄乎如遺世獨立,羽化而登仙。
於是飲酒樂甚，扣舷而歌之。歌曰：“桂棹兮蘭槳，擊空明兮溯流光。渺渺兮予懷，望美人兮天一方。”客有吹洞簫者，倚歌而和之。其聲嗚嗚然，如怨如慕，如泣如訴；餘音嫋嫋，不絕如縷。舞幽壑之潛蛟，泣孤舟之嫠婦。
蘇子愀然，正襟危坐，而問客曰：“何爲其然也？”客曰：“‘月明星稀，烏鵲南飛。’此非曹孟德之詩乎？西望夏口，東望武昌，山川相繆，鬱乎蒼蒼，此非孟德之困於周郎者乎？方其破荊州，下江陵，順流而東也，舳艫千里，旌旗蔽空，釃酒臨江，橫槊賦詩，固一世之雄也，而今安在哉？況吾與子漁樵於江渚之上，侶魚蝦而友麋鹿，駕一葉之扁舟，舉匏樽以相屬。寄蜉蝣於天地，渺滄海之一粟。哀吾生之須臾，羨長 江之無窮。挾飛仙以遨遊，抱明月而長終。知不可乎驟得，託遺響於悲風。”
蘇子曰：“客亦知夫水與月乎？逝者如斯，而未嘗往也；盈虛者如彼，而卒莫消長也。蓋將自其變者而觀之，則天地曾不能以一瞬；自其不變者而觀之，則物與我皆無盡也，而又何羨乎！且夫天地之間，物各有主，苟非吾之所有，雖一毫而莫取。惟江上之清風，與山間之明月，耳得之而爲聲，目遇之而成色，取之無禁，用之不竭。是造物者之無盡藏也，而吾與子之所共適。”(共適 一作：共食) 客喜而笑，洗盞更酌。餚核既盡，杯盤狼籍。相與枕藉乎舟中，不知東方之既白。</description>
    </item>
    
  </channel>
</rss>
