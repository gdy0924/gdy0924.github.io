<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.89.4" />
  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/Googlecode.min.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Gao dy" />
  <meta property="og:url" content="https://gdy0924.github.io/posts/setr/" />
  <link rel="canonical" href="https://gdy0924.github.io/posts/setr/" /><link rel="alternate" type="application/atom+xml" href="https://gdy0924.github.io/index.xml" title="XY&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gdy0924.github.io\/"
      },
      "articleSection" : "posts",
      "name" : "SETR",
      "headline" : "SETR",
      "description" : "原文链接：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers\nAbstract  最近的语义分割方法采用encoder-decoder结构的全卷积网络：编码器逐步降低分辨率，并捕捉更抽象的特征和更大的感受野。但是由于上下文信息对于分割任务很重要，最近的研究集中在通过空洞卷积或引入注意力模块来增加感受野，不过都是在encoder-decoder结构中进行修改。\n在本文中，我们将语义分割作为一个序列到序列的预测任务。具体来说：提出一个纯Transformer，即没有卷积和降低分辨率，将图像编码为一系列patch，在Transformer的每一层都使用全局上下文信息。该encoder可以与一个简单的decoder结合，提供一个强大的分割模型，称为SEgmentation TRansformer（SETR）。\nIntroduction  标准的全卷积网络具有encoder-decoder架构：encoder用于捕捉特征，decoder对由编码器产生的特征进行像素级分类。其中，encoder由堆叠的卷积层组成，特征图的分辨率逐渐降低，encoder能够学习更多的抽象特征，并且逐渐增大感受野。但是存在一些限制，即不能学习远程的依赖信息，感受野有限对于语义分割任务是不好的。\n为了克服上述的限制，一种方法是直接修改卷积操作：大的卷积核大小，空洞卷积和图像\/特征金字塔结构；另一种方法是将注意力模块引入FCN架构中：旨在建模特征图中所有像素的全局交互信息。当应用于语义分割任务时，一种常见的设计是将注意力模块与FCN体系结构相结合，并将注意层放置在顶部。针对上述两种方法，标准的encoder-decoder架构是不变的。最近，开始尝试摆脱卷积，关注纯注意力模型，但是，即使没有卷积，也不会改变模型结构的本质：encoder（下采样）-decoder（上采样）架构。\n在本文中，我们提出用纯Transformer代替基于堆叠卷积层的编码器，从而产生一种新的分割模型，称为SEgmentation TRansformer（SETR）。该Transformer编码器将输入图像视为图像patch序列，并使用全局自注意力机制对该序列进行转换，用于特征学习。具体地说：首先将一个图像分解成一个固定大小的patch，形成patch序列；然后，对每个patch的像素向量应用线性变换层，得到一系列特征embedding向量作为Transformer的输入；得到Transformer输出的特征，使用decoder来恢复原始图像的分辨率。图像分辨率没有下采样，在Transformer的每一层都进行全局上下文建模。（灵感来源于ViT）\nContributions：\n（1）将图像分割问题作为一个序列到序列的预测任务，以替换全卷积网络；\n（2）利用Transformer框架，实现纯注意力机制的特征表示encoder；\n（3）进一步提出了三种不同复杂性的decoder架构。\nRelated work Semantic segmentation  最近的许多研究都集中在解决有限的感受野\/上下文信息建模问题：DeepLab引入空洞卷积，PSANet提出用于动态捕捉远程上下文的空间注意力模块；DANet提出空间注意力和通道注意力。所有这些方法都基于FCNs，特征编码和提取部分都是基于经典卷积网络，如VGG和ResNet。在该工作中，我们从不同的角度重新思考语义分割任务。\nTransformer  AANet将卷积和自注意力机制集中训练，LRNet探索局部自注意力，以避免全局自注意力带来的大量计算，DETR使用Transformer进行目标检测，其中Transformer被附加在检测头中，STTR和LSTR分别采用Transformer进行视差估计和车道形状预测。最近，ViT是第一个纯Transformer图像分类模型，达到了最优的结果，为语义分割任务中使用纯Transformer的编码器设计提供了灵感。\nMethod FCN-based semantic segmentation  在基于FCN的语义分割网络中，encoder由堆叠的卷积层组成：第一层以图像作为输入，大小为$H×W×3$；后续层的输入是一个大小为$h×w×d$的三维张量，由于卷积操作的局域性，感受野随层数的增加而增加，取决于卷积核核的大小（通常是3×3）。因此，只有具有大感受野的更高层次才能在这个FCN架构中捕捉更多的上下文信息。然而，一旦达到一定深度，增加更多层的优势会迅速减少。因此，对上下文建模的感受野有限是经典FCN结构的限制。\n最近，一些研究发现，将FCN与注意机制相结合是学习上下文信息的一种有效的策略。这些方法为例减少计算量，将注意力机制限制在较小尺寸的更高层次的特征图中，导致缺乏对较低层次特征图的学习，为了克服这一限制，我们提出了一种完全基于自注意力机制的编码器，称为SEgmentation TRansformers（SETR）。\nSegmentation transformers (SETR) Image to sequence  SETR遵循NLP的输入-输出形式，具体地，Transformer，如上图的（a）所示，以一维的特征embedding序列$Z\\in \\mathbb{R} ^{L\\times C}$作为输入，$L$为序列的长度，$C$为隐含通道的大小。因此，图像序列化需要将输入图像$x\\in \\mathbb{R} ^{H\\times W\\times 3}$转换为$Z$。\n图像序列化的一种简单方法是将图像像素值flatten为一个大小为$3HW$的一维向量，对于大小为$480×480×3$的典型图像，得到的向量长度将为$691200$。考虑到Transformer的二次复杂度，这种高维向量不可能在空间和时间上同时被处理。因此，将每个像素作为转换器的输入是不可能的。\n最终，决定设置Transformer的输入序列长度$L$为$\\frac{H}{16}\\times \\frac{W}{16}=\\frac{HW}{256}$。为了获得$\\frac{HW}{256}$长的输入序列，我们将一个图像$x\\in \\mathbb{R} ^{H\\times W\\times 3}$均匀地划分成为$\\frac{H}{16}\\times \\frac{W}{16}$个patch，每个patch的大小为$16×16$，然后将patch flatten为一个序列。利用线性变换函数$f:p\\rightarrow e\\in \\mathbb{R} ^{C}$，将每个向量化的patch进一步映射到潜在的高维embedding空间，得到图像$x$的一维patch embedding序列。为了对patch的空间信息进行编码，每个位置$i$学习一个特定的embedding $p_{i}$，以形成最终的输入序列：$E=\\lbrace e_{1}\u002bp_{1},e_{2}\u002bp_{2},\u0026hellip;,e_{L}\u002bp_{L} \\rbrace$。",
      "inLanguage" : "en-US",
      "author" : "Gao dy",
      "creator" : "Gao dy",
      "publisher": "Gao dy",
      "accountablePerson" : "Gao dy",
      "copyrightHolder" : "Gao dy",
      "copyrightYear" : "2022",
      "datePublished": "2022-05-07 00:00:00 \u002b0000 UTC",
      "dateModified" : "2022-05-07 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gdy0924.github.io\/posts\/setr\/",
      "keywords" : [  ]
  }
</script>
<title>SETR</title>
  <meta property="og:title" content="SETR" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="原文链接：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers
Abstract  最近的语义分割方法采用encoder-decoder结构的全卷积网络：编码器逐步降低分辨率，并捕捉更抽象的特征和更大的感受野。但是由于上下文信息对于分割任务很重要，最近的研究集中在通过空洞卷积或引入注意力模块来增加感受野，不过都是在encoder-decoder结构中进行修改。
在本文中，我们将语义分割作为一个序列到序列的预测任务。具体来说：提出一个纯Transformer，即没有卷积和降低分辨率，将图像编码为一系列patch，在Transformer的每一层都使用全局上下文信息。该encoder可以与一个简单的decoder结合，提供一个强大的分割模型，称为SEgmentation TRansformer（SETR）。
Introduction  标准的全卷积网络具有encoder-decoder架构：encoder用于捕捉特征，decoder对由编码器产生的特征进行像素级分类。其中，encoder由堆叠的卷积层组成，特征图的分辨率逐渐降低，encoder能够学习更多的抽象特征，并且逐渐增大感受野。但是存在一些限制，即不能学习远程的依赖信息，感受野有限对于语义分割任务是不好的。
为了克服上述的限制，一种方法是直接修改卷积操作：大的卷积核大小，空洞卷积和图像/特征金字塔结构；另一种方法是将注意力模块引入FCN架构中：旨在建模特征图中所有像素的全局交互信息。当应用于语义分割任务时，一种常见的设计是将注意力模块与FCN体系结构相结合，并将注意层放置在顶部。针对上述两种方法，标准的encoder-decoder架构是不变的。最近，开始尝试摆脱卷积，关注纯注意力模型，但是，即使没有卷积，也不会改变模型结构的本质：encoder（下采样）-decoder（上采样）架构。
在本文中，我们提出用纯Transformer代替基于堆叠卷积层的编码器，从而产生一种新的分割模型，称为SEgmentation TRansformer（SETR）。该Transformer编码器将输入图像视为图像patch序列，并使用全局自注意力机制对该序列进行转换，用于特征学习。具体地说：首先将一个图像分解成一个固定大小的patch，形成patch序列；然后，对每个patch的像素向量应用线性变换层，得到一系列特征embedding向量作为Transformer的输入；得到Transformer输出的特征，使用decoder来恢复原始图像的分辨率。图像分辨率没有下采样，在Transformer的每一层都进行全局上下文建模。（灵感来源于ViT）
Contributions：
（1）将图像分割问题作为一个序列到序列的预测任务，以替换全卷积网络；
（2）利用Transformer框架，实现纯注意力机制的特征表示encoder；
（3）进一步提出了三种不同复杂性的decoder架构。
Related work Semantic segmentation  最近的许多研究都集中在解决有限的感受野/上下文信息建模问题：DeepLab引入空洞卷积，PSANet提出用于动态捕捉远程上下文的空间注意力模块；DANet提出空间注意力和通道注意力。所有这些方法都基于FCNs，特征编码和提取部分都是基于经典卷积网络，如VGG和ResNet。在该工作中，我们从不同的角度重新思考语义分割任务。
Transformer  AANet将卷积和自注意力机制集中训练，LRNet探索局部自注意力，以避免全局自注意力带来的大量计算，DETR使用Transformer进行目标检测，其中Transformer被附加在检测头中，STTR和LSTR分别采用Transformer进行视差估计和车道形状预测。最近，ViT是第一个纯Transformer图像分类模型，达到了最优的结果，为语义分割任务中使用纯Transformer的编码器设计提供了灵感。
Method FCN-based semantic segmentation  在基于FCN的语义分割网络中，encoder由堆叠的卷积层组成：第一层以图像作为输入，大小为$H×W×3$；后续层的输入是一个大小为$h×w×d$的三维张量，由于卷积操作的局域性，感受野随层数的增加而增加，取决于卷积核核的大小（通常是3×3）。因此，只有具有大感受野的更高层次才能在这个FCN架构中捕捉更多的上下文信息。然而，一旦达到一定深度，增加更多层的优势会迅速减少。因此，对上下文建模的感受野有限是经典FCN结构的限制。
最近，一些研究发现，将FCN与注意机制相结合是学习上下文信息的一种有效的策略。这些方法为例减少计算量，将注意力机制限制在较小尺寸的更高层次的特征图中，导致缺乏对较低层次特征图的学习，为了克服这一限制，我们提出了一种完全基于自注意力机制的编码器，称为SEgmentation TRansformers（SETR）。
Segmentation transformers (SETR) Image to sequence  SETR遵循NLP的输入-输出形式，具体地，Transformer，如上图的（a）所示，以一维的特征embedding序列$Z\in \mathbb{R} ^{L\times C}$作为输入，$L$为序列的长度，$C$为隐含通道的大小。因此，图像序列化需要将输入图像$x\in \mathbb{R} ^{H\times W\times 3}$转换为$Z$。
图像序列化的一种简单方法是将图像像素值flatten为一个大小为$3HW$的一维向量，对于大小为$480×480×3$的典型图像，得到的向量长度将为$691200$。考虑到Transformer的二次复杂度，这种高维向量不可能在空间和时间上同时被处理。因此，将每个像素作为转换器的输入是不可能的。
最终，决定设置Transformer的输入序列长度$L$为$\frac{H}{16}\times \frac{W}{16}=\frac{HW}{256}$。为了获得$\frac{HW}{256}$长的输入序列，我们将一个图像$x\in \mathbb{R} ^{H\times W\times 3}$均匀地划分成为$\frac{H}{16}\times \frac{W}{16}$个patch，每个patch的大小为$16×16$，然后将patch flatten为一个序列。利用线性变换函数$f:p\rightarrow e\in \mathbb{R} ^{C}$，将每个向量化的patch进一步映射到潜在的高维embedding空间，得到图像$x$的一维patch embedding序列。为了对patch的空间信息进行编码，每个位置$i$学习一个特定的embedding $p_{i}$，以形成最终的输入序列：$E=\lbrace e_{1}&#43;p_{1},e_{2}&#43;p_{2},&amp;hellip;,e_{L}&#43;p_{L} \rbrace$。" />
  <meta name="description" content="原文链接：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers
Abstract  最近的语义分割方法采用encoder-decoder结构的全卷积网络：编码器逐步降低分辨率，并捕捉更抽象的特征和更大的感受野。但是由于上下文信息对于分割任务很重要，最近的研究集中在通过空洞卷积或引入注意力模块来增加感受野，不过都是在encoder-decoder结构中进行修改。
在本文中，我们将语义分割作为一个序列到序列的预测任务。具体来说：提出一个纯Transformer，即没有卷积和降低分辨率，将图像编码为一系列patch，在Transformer的每一层都使用全局上下文信息。该encoder可以与一个简单的decoder结合，提供一个强大的分割模型，称为SEgmentation TRansformer（SETR）。
Introduction  标准的全卷积网络具有encoder-decoder架构：encoder用于捕捉特征，decoder对由编码器产生的特征进行像素级分类。其中，encoder由堆叠的卷积层组成，特征图的分辨率逐渐降低，encoder能够学习更多的抽象特征，并且逐渐增大感受野。但是存在一些限制，即不能学习远程的依赖信息，感受野有限对于语义分割任务是不好的。
为了克服上述的限制，一种方法是直接修改卷积操作：大的卷积核大小，空洞卷积和图像/特征金字塔结构；另一种方法是将注意力模块引入FCN架构中：旨在建模特征图中所有像素的全局交互信息。当应用于语义分割任务时，一种常见的设计是将注意力模块与FCN体系结构相结合，并将注意层放置在顶部。针对上述两种方法，标准的encoder-decoder架构是不变的。最近，开始尝试摆脱卷积，关注纯注意力模型，但是，即使没有卷积，也不会改变模型结构的本质：encoder（下采样）-decoder（上采样）架构。
在本文中，我们提出用纯Transformer代替基于堆叠卷积层的编码器，从而产生一种新的分割模型，称为SEgmentation TRansformer（SETR）。该Transformer编码器将输入图像视为图像patch序列，并使用全局自注意力机制对该序列进行转换，用于特征学习。具体地说：首先将一个图像分解成一个固定大小的patch，形成patch序列；然后，对每个patch的像素向量应用线性变换层，得到一系列特征embedding向量作为Transformer的输入；得到Transformer输出的特征，使用decoder来恢复原始图像的分辨率。图像分辨率没有下采样，在Transformer的每一层都进行全局上下文建模。（灵感来源于ViT）
Contributions：
（1）将图像分割问题作为一个序列到序列的预测任务，以替换全卷积网络；
（2）利用Transformer框架，实现纯注意力机制的特征表示encoder；
（3）进一步提出了三种不同复杂性的decoder架构。
Related work Semantic segmentation  最近的许多研究都集中在解决有限的感受野/上下文信息建模问题：DeepLab引入空洞卷积，PSANet提出用于动态捕捉远程上下文的空间注意力模块；DANet提出空间注意力和通道注意力。所有这些方法都基于FCNs，特征编码和提取部分都是基于经典卷积网络，如VGG和ResNet。在该工作中，我们从不同的角度重新思考语义分割任务。
Transformer  AANet将卷积和自注意力机制集中训练，LRNet探索局部自注意力，以避免全局自注意力带来的大量计算，DETR使用Transformer进行目标检测，其中Transformer被附加在检测头中，STTR和LSTR分别采用Transformer进行视差估计和车道形状预测。最近，ViT是第一个纯Transformer图像分类模型，达到了最优的结果，为语义分割任务中使用纯Transformer的编码器设计提供了灵感。
Method FCN-based semantic segmentation  在基于FCN的语义分割网络中，encoder由堆叠的卷积层组成：第一层以图像作为输入，大小为$H×W×3$；后续层的输入是一个大小为$h×w×d$的三维张量，由于卷积操作的局域性，感受野随层数的增加而增加，取决于卷积核核的大小（通常是3×3）。因此，只有具有大感受野的更高层次才能在这个FCN架构中捕捉更多的上下文信息。然而，一旦达到一定深度，增加更多层的优势会迅速减少。因此，对上下文建模的感受野有限是经典FCN结构的限制。
最近，一些研究发现，将FCN与注意机制相结合是学习上下文信息的一种有效的策略。这些方法为例减少计算量，将注意力机制限制在较小尺寸的更高层次的特征图中，导致缺乏对较低层次特征图的学习，为了克服这一限制，我们提出了一种完全基于自注意力机制的编码器，称为SEgmentation TRansformers（SETR）。
Segmentation transformers (SETR) Image to sequence  SETR遵循NLP的输入-输出形式，具体地，Transformer，如上图的（a）所示，以一维的特征embedding序列$Z\in \mathbb{R} ^{L\times C}$作为输入，$L$为序列的长度，$C$为隐含通道的大小。因此，图像序列化需要将输入图像$x\in \mathbb{R} ^{H\times W\times 3}$转换为$Z$。
图像序列化的一种简单方法是将图像像素值flatten为一个大小为$3HW$的一维向量，对于大小为$480×480×3$的典型图像，得到的向量长度将为$691200$。考虑到Transformer的二次复杂度，这种高维向量不可能在空间和时间上同时被处理。因此，将每个像素作为转换器的输入是不可能的。
最终，决定设置Transformer的输入序列长度$L$为$\frac{H}{16}\times \frac{W}{16}=\frac{HW}{256}$。为了获得$\frac{HW}{256}$长的输入序列，我们将一个图像$x\in \mathbb{R} ^{H\times W\times 3}$均匀地划分成为$\frac{H}{16}\times \frac{W}{16}$个patch，每个patch的大小为$16×16$，然后将patch flatten为一个序列。利用线性变换函数$f:p\rightarrow e\in \mathbb{R} ^{C}$，将每个向量化的patch进一步映射到潜在的高维embedding空间，得到图像$x$的一维patch embedding序列。为了对patch的空间信息进行编码，每个位置$i$学习一个特定的embedding $p_{i}$，以形成最终的输入序列：$E=\lbrace e_{1}&#43;p_{1},e_{2}&#43;p_{2},&amp;hellip;,e_{L}&#43;p_{L} \rbrace$。" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{font-family:bree serif,sans-serif;-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  

  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="XY&#39;s Blog">
  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Bungee+Shade" rel="stylesheet">
  
  

  
  
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/"
      >Gao dy</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/gdy0924" target="_blank">About</a>
  </div>
  
</div>
<div class="row end-xs">
  
  <div class="lang-switch col-xs-3 col-xs-offset-9">
    <a href="/en/">English</a>
  </div>
    
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">SETR</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2022-05-07 00:00:00 UTC">
                07 May 2022
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://gdy0924.github.io/">@Gao dy</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>原文链接：<a href="https://arxiv.org/pdf/2012.15840.pdf">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers</a></p>
<h2 id="abstract">Abstract</h2>
<p>    最近的语义分割方法采用encoder-decoder结构的全卷积网络：编码器逐步降低分辨率，并捕捉更抽象的特征和更大的感受野。但是由于上下文信息对于分割任务很重要，最近的研究集中在通过空洞卷积或引入注意力模块来增加感受野，不过都是在encoder-decoder结构中进行修改。<br>
    在本文中，我们将语义分割作为一个序列到序列的预测任务。具体来说：提出一个纯Transformer，即没有卷积和降低分辨率，将图像编码为一系列patch，在Transformer的每一层都使用全局上下文信息。该encoder可以与一个简单的decoder结合，提供一个强大的分割模型，称为SEgmentation TRansformer（SETR）。</p>
<h2 id="introduction">Introduction</h2>
<p>    标准的全卷积网络具有encoder-decoder架构：encoder用于捕捉特征，decoder对由编码器产生的特征进行像素级分类。其中，encoder由堆叠的卷积层组成，特征图的分辨率逐渐降低，encoder能够学习更多的抽象特征，并且逐渐增大感受野。但是存在一些限制，即不能学习远程的依赖信息，感受野有限对于语义分割任务是不好的。<br>
    为了克服上述的限制，一种方法是直接修改卷积操作：大的卷积核大小，空洞卷积和图像/特征金字塔结构；另一种方法是将注意力模块引入FCN架构中：旨在建模特征图中所有像素的全局交互信息。当应用于语义分割任务时，一种常见的设计是将注意力模块与FCN体系结构相结合，并将注意层放置在顶部。针对上述两种方法，标准的encoder-decoder架构是不变的。最近，开始尝试摆脱卷积，关注纯注意力模型，但是，即使没有卷积，也不会改变模型结构的本质：encoder（下采样）-decoder（上采样）架构。<br>
    在本文中，我们提出用纯Transformer代替基于堆叠卷积层的编码器，从而产生一种新的分割模型，称为SEgmentation TRansformer（SETR）。该Transformer编码器将输入图像视为图像patch序列，并使用全局自注意力机制对该序列进行转换，用于特征学习。具体地说：首先将一个图像分解成一个固定大小的patch，形成patch序列；然后，对每个patch的像素向量应用线性变换层，得到一系列特征embedding向量作为Transformer的输入；得到Transformer输出的特征，使用decoder来恢复原始图像的分辨率。图像分辨率没有下采样，在Transformer的每一层都进行全局上下文建模。（灵感来源于ViT）<br>
    <em><strong>Contributions</strong></em>：<br>
    （1）将图像分割问题作为一个序列到序列的预测任务，以替换全卷积网络；<br>
    （2）利用Transformer框架，实现纯注意力机制的特征表示encoder；<br>
    （3）进一步提出了三种不同复杂性的decoder架构。</p>
<h2 id="related-work">Related work</h2>
<h3 id="semantic-segmentation">Semantic segmentation</h3>
<p>    最近的许多研究都集中在解决有限的感受野/上下文信息建模问题：DeepLab引入空洞卷积，PSANet提出用于动态捕捉远程上下文的空间注意力模块；DANet提出空间注意力和通道注意力。所有这些方法都基于FCNs，特征编码和提取部分都是基于经典卷积网络，如VGG和ResNet。在该工作中，我们从不同的角度重新思考语义分割任务。</p>
<h3 id="transformer">Transformer</h3>
<p>    AANet将卷积和自注意力机制集中训练，LRNet探索局部自注意力，以避免全局自注意力带来的大量计算，DETR使用Transformer进行目标检测，其中Transformer被附加在检测头中，STTR和LSTR分别采用Transformer进行视差估计和车道形状预测。最近，ViT是第一个纯Transformer图像分类模型，达到了最优的结果，为语义分割任务中使用纯Transformer的编码器设计提供了灵感。</p>
<h2 id="method">Method</h2>
<p><img src="/img/setr1.PNG" alt=""></p>
<h3 id="fcn-based-semantic-segmentation">FCN-based semantic segmentation</h3>
<p>    在基于FCN的语义分割网络中，encoder由堆叠的卷积层组成：第一层以图像作为输入，大小为$H×W×3$；后续层的输入是一个大小为$h×w×d$的三维张量，由于卷积操作的局域性，感受野随层数的增加而增加，取决于卷积核核的大小（通常是3×3）。因此，只有具有大感受野的更高层次才能在这个FCN架构中捕捉更多的上下文信息。然而，一旦达到一定深度，增加更多层的优势会迅速减少。因此，对上下文建模的感受野有限是经典FCN结构的限制。<br>
    最近，一些研究发现，将FCN与注意机制相结合是学习上下文信息的一种有效的策略。这些方法为例减少计算量，将注意力机制限制在较小尺寸的更高层次的特征图中，导致缺乏对较低层次特征图的学习，为了克服这一限制，我们提出了一种完全基于自注意力机制的编码器，称为SEgmentation TRansformers（SETR）。</p>
<h3 id="segmentation-transformers-setr">Segmentation transformers (SETR)</h3>
<h4 id="image-to-sequence">Image to sequence</h4>
<p>    SETR遵循NLP的输入-输出形式，具体地，Transformer，如上图的（a）所示，以一维的特征embedding序列$Z\in \mathbb{R} ^{L\times C}$作为输入，$L$为序列的长度，$C$为隐含通道的大小。因此，图像序列化需要将输入图像$x\in \mathbb{R} ^{H\times W\times 3}$转换为$Z$。<br>
    图像序列化的一种简单方法是将图像像素值flatten为一个大小为$3HW$的一维向量，对于大小为$480×480×3$的典型图像，得到的向量长度将为$691200$。考虑到Transformer的二次复杂度，这种高维向量不可能在空间和时间上同时被处理。因此，将每个像素作为转换器的输入是不可能的。<br>
    最终，决定设置Transformer的输入序列长度$L$为$\frac{H}{16}\times \frac{W}{16}=\frac{HW}{256}$。为了获得$\frac{HW}{256}$长的输入序列，我们将一个图像$x\in \mathbb{R} ^{H\times W\times 3}$均匀地划分成为$\frac{H}{16}\times \frac{W}{16}$个patch，每个patch的大小为$16×16$，然后将patch flatten为一个序列。利用线性变换函数$f:p\rightarrow e\in \mathbb{R} ^{C}$，将每个向量化的patch进一步映射到潜在的高维embedding空间，得到图像$x$的一维patch embedding序列。为了对patch的空间信息进行编码，每个位置$i$学习一个特定的embedding $p_{i}$，以形成最终的输入序列：$E=\lbrace e_{1}+p_{1},e_{2}+p_{2},&hellip;,e_{L}+p_{L} \rbrace$。</p>
<h4 id="transformer-1">Transformer</h4>
<p>    以一维embedding序列$E$作为输入，采用基于Transformer的encoder来学习特征表示，这意味着每个Transformer层都有全局的感受野，可以一次性解决现有FCN编码器的有限感受野的问题。<br>
    Transformer编码器由$L_{e}$层多头自注意力（MSA）和多层感知器（MLP）块组成，如上图（a）所示。<br>
    在每一层$l$中，自注意力机制的输入是一个三元组$(query,key,value)$，该三元组通过输入$Z^{l-1}\in \mathbb{R}^{L\times C}$计算出来的，即：
$$
query=Z^{l-1}W_{Q},key=Z^{l-1}W_{K},value=Z^{l-1}W_{V}
$$
    其中，$W_{Q}/W_{K}/W_{V} \in \mathbb{R}^{C\times d}$是三个可学习的参数矩阵，$d$是$(query,key,value)$的维度。自注意力机制就表示如下：
$$
SA(Z^{l-1})=Z^{l-1}+softmax(\frac{Z^{l-1}W_{Q}(Z^{l-1}W_{K})^{T}}{\sqrt{d}})(Z^{l-1}W_{V})
$$
    MS为包含m个独立的SA操作，并将其输出拼接起来：$MSA(Z^{l-1})=[SA_{1}(Z^{l-1});SA_{2}(Z^{l-1});&hellip;SA_{m}(Z^{l-1})]W_{O}$。MSA的输出经过MLP，与残差连接相连作为该层的输出：
$$
Z^{l}=MSA(Z^{l-1})+MLP(MSA(Z^{l-1}))
$$
    在MSA和MLP块之前通过层归一化，用$\lbrace {Z^{1},Z^{2},&hellip;,Z^{L_{e}}} \rbrace$表示Transformer层的特征。</p>
<h3 id="decoder-designs">Decoder designs</h3>
<p>    为了评估SETR中encoder特征表示$Z$的有效性，我们提出了三种不同的decoder架构来进行像素级的分割。decoder要在原始二维图像空间$(H×W)$上生成分割结果，因此需要reshape编码器的特征输出$Z$，从2D（$\frac{HW}{256}\times C$）转换为3D特征图（$\frac{H}{16}\times \frac{W}{16}\times C$）。接下来，将介绍提出的三个解码器。</p>
<h4 id="naive-upsamplingnaive">Naive upsampling(Naive)</h4>
<p>    首先将Transformer的特征$Z^{L_{e}}$投射到类别数的维度上（例如，Cityscapes为19），采用一个简单的2层网络，其架构为：1×1卷积+BN(with ReLU)+1×1卷积；然后，将输出上采样到原始图像的分辨率；最后是具有像素级交叉熵损失的分类层。（称为SETR-Naive）</p>
<h4 id="progressive-upsampling-pup">Progressive UPsampling (PUP)</h4>
<p>    采用一种渐进的上采样策略，即交替使用conv层和上采样操作。将上采样的倍数限制在2倍，总共需要4次上采样操作才能使尺寸为$\frac{H}{16}\times \frac{W}{16}$的$Z^{L_{e}}$转换为原始图像分辨率，如图中的（b）。（称为 SETR-PUP）</p>
<h4 id="multi-level-feature-aggregation-mla">Multi-Level feature Aggregation (MLA)</h4>
<p>    如上图中的（c）所示，引入多层次的特征聚合，类似于特征金字塔，但是与特征金字塔不同，因为每个SETR层的特征表示$Z^{l}$的分辨率是相同没有金字塔形状。将$\lbrace Z^{m}\rbrace$，其中$m\in \lbrace \frac{L_{e}}{M},2\frac{L_{e}}{M},&hellip;,M\frac{L_{e}}{M} \rbrace$，作为输入，以$\frac{L_{e}}{M}$为步长，从encoder中的$L_{e}$层中均匀的提取特征表示。针对提取到的$M$个特征表示，将每个特征$Z^{l}$先reshape（$\frac{HW}{256}\times C$变为$\frac{H}{16}\times \frac{W}{16}\times C$）。以上图中的（c）为例，reshape之后，每个路径分别经过一个1×1卷积，然后下层路径上的特征图与上层路径上的特征图进行元素相加，接着每个路径再分别通过两个3×3卷积，第二次3×3卷积通道减半，然后每个通道进行上采样操作，将上采样得到的特征图通过通道拼接进行融合，然后再进行卷积操作，再上采样4倍得到最终结果。（称为SETR-MLA）</p>
<h2 id="实验">实验</h2>
<p>    评价标准：mIOU平均交并比；SS：单尺度预测；MS：多尺度预测。</p>
<h3 id="transformer-backbone-variants">Transformer backbone variants</h3>
<p><img src="/img/setr2.PNG" alt=""></p>
<h3 id="comparing-setr-variants">Comparing SETR variants</h3>
<p><img src="/img/setr3.PNG" alt="">
    其中，Pre表示预训练Transformer，R表示随机初始化；Base表示使用ViT模型的初始化，DeiT表示使用DeiT模型的权重初始化 。<br>
    从上表可以看出：<br>
    （1）逐步对特征图进行上采样，SETR-PUP获得了最好的性能，SETR-MLA性能较差的一个可能原因是，不同transformer层的特征输出不像特征金字塔网络那样具有不同分辨率的优势；<br>
    （2）使用T-Large优于T-Base；<br>
    （3）虽然SETR-PUP-Base（76.71）的表现比Hybrid-Base（76.76）差，但在进行更多迭代(80k)时，性能更好（78.02），证明了在语义分割中可以替代FCN编码器的设计，并进一步验证了该模型的有效性。</p>
<h3 id="comparison-to-state-of-the-art">Comparison to state-of-the-art</h3>
<h4 id="ade20k-dataset">ADE20K dataset</h4>
<p><img src="/img/setr4.PNG" alt=""></p>
<h4 id="pascal-context-dataset">Pascal Context dataset</h4>
<p><img src="/img/setr5.PNG" alt=""></p>
<h4 id="cityscapes-validation-set">Cityscapes validation set</h4>
<p><img src="/img/setr6.PNG" alt=""></p>
<h2 id="conclusion">Conclusion</h2>
<p>    在本文中，我们提出将语义分割任务作为序列到序列预测任务。与现有的基于FCN的方法相比，不使用空洞卷积或注意力模块扩大感受野，完全消除对FCN的依赖。用Transformer实现了所提出的想法，在特征学习的每个阶段可以建模全局上下文信息。提出一组不同复杂度的解码器，建立了效果不错的分割模型。</p>
<h4 id="heading"></h4>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://joway.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>