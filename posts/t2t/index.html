<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.89.4" />
  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/Googlecode.min.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Gao dy" />
  <meta property="og:url" content="https://gdy0924.github.io/posts/t2t/" />
  <link rel="canonical" href="https://gdy0924.github.io/posts/t2t/" /><link rel="alternate" type="application/atom+xml" href="https://gdy0924.github.io/index.xml" title="XY&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gdy0924.github.io\/"
      },
      "articleSection" : "posts",
      "name" : "T2T",
      "headline" : "T2T",
      "description" : "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\nAbstract  ViT将图像分割为patch，经过多个transformer层对特征进行提取，实现分类任务。但是，在中型数据集上从头开始训练时，ViT的性能不如CNN网络。可能有以下几个原因：（1）简单的将图像分割为patch作为token，无法建模重要的局部信息，如相邻像素之间的边和线等，导致训练样本效率低；（2）ViT的backbone中有冗余的注意力计算，导致计算量大，导致对于固定计算预算和有限训练样本不能提取更加丰富的特征。\n为了解决上述问题，我们提出了新的架构Tokens-To-Token Vision Transformer（T2T-ViT），包括：（1）一个层级的Tokens-To-Token的转换，通过递归将相邻的tokens聚合成一个token，即Tokens-To-Token，这样就可以对token周围的局部结构进行建模，并减少token长度；（2）基于CNN架构的deep-narrow视觉transformer架构。\nIntroduction  虽然ViT证明了transformer结构在视觉任务中很有前景，但其在中小型数据集从头训练时，效果不如卷积网络。主要有两个限制：（1）输入图像直接生硬的分割为patch，无法建模局部结构，如边和线，因此需要更多的训练样本；（2）ViT的backbone过于冗余，没有与CNN设计相类似，因此导致特征提取丰富度有限并且模型训练困难。 为了验证假设，对ViT和ResNet的特征图进行了可视化，如上图所示：可以看出，ResNet捕捉的特征从局部（conv1）逐渐延伸到全局（conv25），然而，ViT的局部信息建模的很差，但全局信息被捕获到了。因此，可以说ViT直接分割图像为patch，忽视了局部结构；ViT中许多通道为0（红色框所示），表明ViT的backbone不如ResNets。\n我们提出了新的transformer架构，解决上述问题。（1）与ViT中的直接分割patch不同，我们提出了一个渐进的模块，逐步将相邻的tokens聚合为一个token，可以对周围token的局部信息进行建模，并迭代地减少token的长度，具体地说，在T2T模块中，transformer层的输出被reshape成图像，然后进行有重叠的分割，最后通过flatten将patch聚合在一起；（2）为了找到更高效的backbone，参考CNN架构设计，提出了更深更窄的transformer backbone。\nContributions：\n（1）提出的transformer架构（T2T\u002b高效backbone）在不使用预训练的情况下，效果好于卷积网络；\n（2）提出了一种新的渐进的token方法，可以建模token之间的局部结构；\n（3）证明CNN的更深更窄的架构同样适用于transformer架构，并提出更高效的transformer backbone。\nRelated Work Transformers in Vision  ViT证明了纯transformer架构也可以在图像分类上有很好的效果，然而，ViT依赖于大规模数据集，需要进行模型预训练，并且需要大量的计算资源；我们提出的T2T-ViT不需要预训练，也不依赖于大规模数据集。\nTokens-to-Token ViT  为了解决ViT中简单token和backbone低效的问题，提出了Tokens-to-Token Vision Transformer（T2T-ViT），可以逐步将图像转化为patch，并使用更加高效的backbone。\nT2T-ViT由两部分组成：（1）层级的Tokens-to-Token模块，对图像的局部信息进行建模，逐步减少token的长度；（2）更高效的T2T-ViT backbone。\nTokens-to-Token: Progressive Tokenization  Tokens-to-Token模块是为了解决ViT中简单分割patch的问题，逐步将图像分割为patch，从而对局部信息进行建模，迭代的减少token的长度。每个T2T包含两个步骤：Re-structurization和Soft Split (SS)，如下图所示： Re-structurization  来自前一层的输出token序列，是通过MSA和MLP处理后得到的，即： $$ T^{\u0027}=MLP(MSA(T)) $$ 然后得到的$T^{\u0027}$将被reshape成空间维度中的图像： $$ I=Reshape(T^{\u0027}) $$ Reshape是将$T^{\u0027}\\in \\mathbb{R}^{l\\times c}$转换为$I\\in \\mathbb{R}^{h\\times w\\times c}$，其中，$l$是$T^{\u0027}$的长度，$h,w,c$是图像的高、宽和通道数，并且$l=h×w$\nSoft Split  在reshape成图像$I$之后，对其图像进行Soft Split，以对局部信息进行建模，减少token的长度。\n将图像分割为重叠的patch，每个patch都可以与相邻的patch有关联，分割得到的tokens被连接为一个token，从而从周围的像素中聚合局部信息。",
      "inLanguage" : "en-US",
      "author" : "Gao dy",
      "creator" : "Gao dy",
      "publisher": "Gao dy",
      "accountablePerson" : "Gao dy",
      "copyrightHolder" : "Gao dy",
      "copyrightYear" : "2022",
      "datePublished": "2022-06-08 00:00:00 \u002b0000 UTC",
      "dateModified" : "2022-06-08 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gdy0924.github.io\/posts\/t2t\/",
      "keywords" : [  ]
  }
</script>
<title>T2T</title>
  <meta property="og:title" content="T2T" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
Abstract  ViT将图像分割为patch，经过多个transformer层对特征进行提取，实现分类任务。但是，在中型数据集上从头开始训练时，ViT的性能不如CNN网络。可能有以下几个原因：（1）简单的将图像分割为patch作为token，无法建模重要的局部信息，如相邻像素之间的边和线等，导致训练样本效率低；（2）ViT的backbone中有冗余的注意力计算，导致计算量大，导致对于固定计算预算和有限训练样本不能提取更加丰富的特征。
为了解决上述问题，我们提出了新的架构Tokens-To-Token Vision Transformer（T2T-ViT），包括：（1）一个层级的Tokens-To-Token的转换，通过递归将相邻的tokens聚合成一个token，即Tokens-To-Token，这样就可以对token周围的局部结构进行建模，并减少token长度；（2）基于CNN架构的deep-narrow视觉transformer架构。
Introduction  虽然ViT证明了transformer结构在视觉任务中很有前景，但其在中小型数据集从头训练时，效果不如卷积网络。主要有两个限制：（1）输入图像直接生硬的分割为patch，无法建模局部结构，如边和线，因此需要更多的训练样本；（2）ViT的backbone过于冗余，没有与CNN设计相类似，因此导致特征提取丰富度有限并且模型训练困难。 为了验证假设，对ViT和ResNet的特征图进行了可视化，如上图所示：可以看出，ResNet捕捉的特征从局部（conv1）逐渐延伸到全局（conv25），然而，ViT的局部信息建模的很差，但全局信息被捕获到了。因此，可以说ViT直接分割图像为patch，忽视了局部结构；ViT中许多通道为0（红色框所示），表明ViT的backbone不如ResNets。
我们提出了新的transformer架构，解决上述问题。（1）与ViT中的直接分割patch不同，我们提出了一个渐进的模块，逐步将相邻的tokens聚合为一个token，可以对周围token的局部信息进行建模，并迭代地减少token的长度，具体地说，在T2T模块中，transformer层的输出被reshape成图像，然后进行有重叠的分割，最后通过flatten将patch聚合在一起；（2）为了找到更高效的backbone，参考CNN架构设计，提出了更深更窄的transformer backbone。
Contributions：
（1）提出的transformer架构（T2T&#43;高效backbone）在不使用预训练的情况下，效果好于卷积网络；
（2）提出了一种新的渐进的token方法，可以建模token之间的局部结构；
（3）证明CNN的更深更窄的架构同样适用于transformer架构，并提出更高效的transformer backbone。
Related Work Transformers in Vision  ViT证明了纯transformer架构也可以在图像分类上有很好的效果，然而，ViT依赖于大规模数据集，需要进行模型预训练，并且需要大量的计算资源；我们提出的T2T-ViT不需要预训练，也不依赖于大规模数据集。
Tokens-to-Token ViT  为了解决ViT中简单token和backbone低效的问题，提出了Tokens-to-Token Vision Transformer（T2T-ViT），可以逐步将图像转化为patch，并使用更加高效的backbone。
T2T-ViT由两部分组成：（1）层级的Tokens-to-Token模块，对图像的局部信息进行建模，逐步减少token的长度；（2）更高效的T2T-ViT backbone。
Tokens-to-Token: Progressive Tokenization  Tokens-to-Token模块是为了解决ViT中简单分割patch的问题，逐步将图像分割为patch，从而对局部信息进行建模，迭代的减少token的长度。每个T2T包含两个步骤：Re-structurization和Soft Split (SS)，如下图所示： Re-structurization  来自前一层的输出token序列，是通过MSA和MLP处理后得到的，即： $$ T^{&#39;}=MLP(MSA(T)) $$ 然后得到的$T^{&#39;}$将被reshape成空间维度中的图像： $$ I=Reshape(T^{&#39;}) $$ Reshape是将$T^{&#39;}\in \mathbb{R}^{l\times c}$转换为$I\in \mathbb{R}^{h\times w\times c}$，其中，$l$是$T^{&#39;}$的长度，$h,w,c$是图像的高、宽和通道数，并且$l=h×w$
Soft Split  在reshape成图像$I$之后，对其图像进行Soft Split，以对局部信息进行建模，减少token的长度。
将图像分割为重叠的patch，每个patch都可以与相邻的patch有关联，分割得到的tokens被连接为一个token，从而从周围的像素中聚合局部信息。" />
  <meta name="description" content="Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
Abstract  ViT将图像分割为patch，经过多个transformer层对特征进行提取，实现分类任务。但是，在中型数据集上从头开始训练时，ViT的性能不如CNN网络。可能有以下几个原因：（1）简单的将图像分割为patch作为token，无法建模重要的局部信息，如相邻像素之间的边和线等，导致训练样本效率低；（2）ViT的backbone中有冗余的注意力计算，导致计算量大，导致对于固定计算预算和有限训练样本不能提取更加丰富的特征。
为了解决上述问题，我们提出了新的架构Tokens-To-Token Vision Transformer（T2T-ViT），包括：（1）一个层级的Tokens-To-Token的转换，通过递归将相邻的tokens聚合成一个token，即Tokens-To-Token，这样就可以对token周围的局部结构进行建模，并减少token长度；（2）基于CNN架构的deep-narrow视觉transformer架构。
Introduction  虽然ViT证明了transformer结构在视觉任务中很有前景，但其在中小型数据集从头训练时，效果不如卷积网络。主要有两个限制：（1）输入图像直接生硬的分割为patch，无法建模局部结构，如边和线，因此需要更多的训练样本；（2）ViT的backbone过于冗余，没有与CNN设计相类似，因此导致特征提取丰富度有限并且模型训练困难。 为了验证假设，对ViT和ResNet的特征图进行了可视化，如上图所示：可以看出，ResNet捕捉的特征从局部（conv1）逐渐延伸到全局（conv25），然而，ViT的局部信息建模的很差，但全局信息被捕获到了。因此，可以说ViT直接分割图像为patch，忽视了局部结构；ViT中许多通道为0（红色框所示），表明ViT的backbone不如ResNets。
我们提出了新的transformer架构，解决上述问题。（1）与ViT中的直接分割patch不同，我们提出了一个渐进的模块，逐步将相邻的tokens聚合为一个token，可以对周围token的局部信息进行建模，并迭代地减少token的长度，具体地说，在T2T模块中，transformer层的输出被reshape成图像，然后进行有重叠的分割，最后通过flatten将patch聚合在一起；（2）为了找到更高效的backbone，参考CNN架构设计，提出了更深更窄的transformer backbone。
Contributions：
（1）提出的transformer架构（T2T&#43;高效backbone）在不使用预训练的情况下，效果好于卷积网络；
（2）提出了一种新的渐进的token方法，可以建模token之间的局部结构；
（3）证明CNN的更深更窄的架构同样适用于transformer架构，并提出更高效的transformer backbone。
Related Work Transformers in Vision  ViT证明了纯transformer架构也可以在图像分类上有很好的效果，然而，ViT依赖于大规模数据集，需要进行模型预训练，并且需要大量的计算资源；我们提出的T2T-ViT不需要预训练，也不依赖于大规模数据集。
Tokens-to-Token ViT  为了解决ViT中简单token和backbone低效的问题，提出了Tokens-to-Token Vision Transformer（T2T-ViT），可以逐步将图像转化为patch，并使用更加高效的backbone。
T2T-ViT由两部分组成：（1）层级的Tokens-to-Token模块，对图像的局部信息进行建模，逐步减少token的长度；（2）更高效的T2T-ViT backbone。
Tokens-to-Token: Progressive Tokenization  Tokens-to-Token模块是为了解决ViT中简单分割patch的问题，逐步将图像分割为patch，从而对局部信息进行建模，迭代的减少token的长度。每个T2T包含两个步骤：Re-structurization和Soft Split (SS)，如下图所示： Re-structurization  来自前一层的输出token序列，是通过MSA和MLP处理后得到的，即： $$ T^{&#39;}=MLP(MSA(T)) $$ 然后得到的$T^{&#39;}$将被reshape成空间维度中的图像： $$ I=Reshape(T^{&#39;}) $$ Reshape是将$T^{&#39;}\in \mathbb{R}^{l\times c}$转换为$I\in \mathbb{R}^{h\times w\times c}$，其中，$l$是$T^{&#39;}$的长度，$h,w,c$是图像的高、宽和通道数，并且$l=h×w$
Soft Split  在reshape成图像$I$之后，对其图像进行Soft Split，以对局部信息进行建模，减少token的长度。
将图像分割为重叠的patch，每个patch都可以与相邻的patch有关联，分割得到的tokens被连接为一个token，从而从周围的像素中聚合局部信息。" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{font-family:bree serif,sans-serif;-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  

  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="XY&#39;s Blog">
  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Bungee+Shade" rel="stylesheet">
  
  

  
  
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/"
      >Gao dy</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/gdy0924" target="_blank">About</a>
  </div>
  
</div>
<div class="row end-xs">
  
  <div class="lang-switch col-xs-3 col-xs-offset-9">
    <a href="/en/">English</a>
  </div>
    
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">T2T</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2022-06-08 00:00:00 UTC">
                08 Jun 2022
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://gdy0924.github.io/">@Gao dy</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</p>
<h2 id="abstract">Abstract</h2>
<p>    ViT将图像分割为patch，经过多个transformer层对特征进行提取，实现分类任务。但是，在中型数据集上从头开始训练时，ViT的性能不如CNN网络。可能有以下几个原因：（1）简单的将图像分割为patch作为token，无法建模重要的局部信息，如相邻像素之间的边和线等，导致训练样本效率低；（2）ViT的backbone中有冗余的注意力计算，导致计算量大，导致对于固定计算预算和有限训练样本不能提取更加丰富的特征。<br>
    为了解决上述问题，我们提出了新的架构Tokens-To-Token Vision Transformer（T2T-ViT），包括：（1）一个层级的Tokens-To-Token的转换，通过递归将相邻的tokens聚合成一个token，即Tokens-To-Token，这样就可以对token周围的局部结构进行建模，并减少token长度；（2）基于CNN架构的deep-narrow视觉transformer架构。</p>
<h2 id="introduction">Introduction</h2>
<p>    虽然ViT证明了transformer结构在视觉任务中很有前景，但其在中小型数据集从头训练时，效果不如卷积网络。主要有两个限制：（1）输入图像直接生硬的分割为patch，无法建模局部结构，如边和线，因此需要更多的训练样本；（2）ViT的backbone过于冗余，没有与CNN设计相类似，因此导致特征提取丰富度有限并且模型训练困难。
<img src="/img/t2t1.PNG" alt="">
    为了验证假设，对ViT和ResNet的特征图进行了可视化，如上图所示：可以看出，ResNet捕捉的特征从局部（conv1）逐渐延伸到全局（conv25），然而，ViT的局部信息建模的很差，但全局信息被捕获到了。因此，可以说ViT直接分割图像为patch，忽视了局部结构；ViT中许多通道为0（红色框所示），表明ViT的backbone不如ResNets。<br>
    我们提出了新的transformer架构，解决上述问题。（1）与ViT中的直接分割patch不同，我们提出了一个渐进的模块，逐步将相邻的tokens聚合为一个token，可以对周围token的局部信息进行建模，并迭代地减少token的长度，具体地说，在T2T模块中，transformer层的输出被reshape成图像，然后进行有重叠的分割，最后通过flatten将patch聚合在一起；（2）为了找到更高效的backbone，参考CNN架构设计，提出了更深更窄的transformer backbone。<br>
    <em><strong>Contributions</strong></em>：<br>
    （1）提出的transformer架构（T2T+高效backbone）在不使用预训练的情况下，效果好于卷积网络；<br>
    （2）提出了一种新的渐进的token方法，可以建模token之间的局部结构；<br>
    （3）证明CNN的更深更窄的架构同样适用于transformer架构，并提出更高效的transformer backbone。</p>
<h2 id="related-work">Related Work</h2>
<h3 id="transformers-in-vision">Transformers in Vision</h3>
<p>    ViT证明了纯transformer架构也可以在图像分类上有很好的效果，然而，ViT依赖于大规模数据集，需要进行模型预训练，并且需要大量的计算资源；我们提出的T2T-ViT不需要预训练，也不依赖于大规模数据集。</p>
<h2 id="tokens-to-token-vit">Tokens-to-Token ViT</h2>
<p>    为了解决ViT中简单token和backbone低效的问题，提出了Tokens-to-Token Vision Transformer（T2T-ViT），可以逐步将图像转化为patch，并使用更加高效的backbone。<br>
    T2T-ViT由两部分组成：（1）层级的Tokens-to-Token模块，对图像的局部信息进行建模，逐步减少token的长度；（2）更高效的T2T-ViT backbone。</p>
<h3 id="tokens-to-token-progressive-tokenization">Tokens-to-Token: Progressive Tokenization</h3>
<p>    Tokens-to-Token模块是为了解决ViT中简单分割patch的问题，逐步将图像分割为patch，从而对局部信息进行建模，迭代的减少token的长度。每个T2T包含两个步骤：Re-structurization和Soft Split (SS)，如下图所示：
<img src="/img/t2t2.PNG" alt=""></p>
<h4 id="re-structurization">Re-structurization</h4>
<p>    来自前一层的输出token序列，是通过MSA和MLP处理后得到的，即：
$$
T^{'}=MLP(MSA(T))
$$
    然后得到的$T^{'}$将被reshape成空间维度中的图像：
$$
I=Reshape(T^{'})
$$
    Reshape是将$T^{'}\in \mathbb{R}^{l\times c}$转换为$I\in \mathbb{R}^{h\times w\times c}$，其中，$l$是$T^{'}$的长度，$h,w,c$是图像的高、宽和通道数，并且$l=h×w$</p>
<h4 id="soft-split">Soft Split</h4>
<p>    在reshape成图像$I$之后，对其图像进行Soft Split，以对局部信息进行建模，减少token的长度。<br>
    将图像分割为重叠的patch，每个patch都可以与相邻的patch有关联，分割得到的tokens被连接为一个token，从而从周围的像素中聚合局部信息。<br>
    Soft Split过程中，对于大小为$k×k$的patch，$s$表示overlapping，$p$表示overlapping，$k-s$与卷积中的步幅类似，那么对于$I\in \mathbb{R}^{h\times w\times c}$的图像，经过Soft Split之后的输出大小为：
$$
l_{o}=\left \lfloor \frac{h+2p-k}{k-s}+1 \right \rfloor \times \left \lfloor \frac{w+2p-k}{k-s}+1 \right \rfloor
$$
    每个patch的大小为$k×k×c$，将其flatten可以得到$T_{o}\in \mathbb{R}^{l_{o}\times ck^{2}}$，作为下一个transformer layer的输入。</p>
<h4 id="t2t-module">T2T module</h4>
<p>    通过Re-structurization和Soft Split操作，可以逐步减少token的长度。</p>
<h3 id="t2t-vit-backbone">T2T-ViT Backbone</h3>
<p>    由于ViT中许多通道是无效的，因此我们提出了一个更高效的backbone，以减少冗余，提高特征丰富度。我们探索了几种不同的ViT架构设计，借鉴卷积网络的思想。因为transformer layer中使用残差连接，一个简单的想法是应用密集连接DenseNet，或者应用Wide-ResNets或ResNeXt结构改变backbone中的通道维度和头数。我们共探索了五种设计架构：<br>
    （1）DenseNet中的密集连接；<br>
    （2）Deep-narrow vs. shallow-wide；<br>
    （3）Squeeze-an-Excitation (SE) Networks中的通道注意力；<br>
    （4）ResNeXt中的更多头数；<br>
    （5）GhostNet中的Ghost操作。<br>
    我们通过实验发现：（1）在ViT中，使用deep-narrow架构，减少通道冗余，增加深度，降低模型的尺寸，提高性能；（2）SE block中的通道注意力也能提高性能，但不如deep-narrow架构。<br>
    基于上述发现，我们提出了一个新的backbone， deep-narrow架构。具体地，拥有较小的通道数和维度，但有更多的层数。在T2T模块的最后一层$T_{f}$，添加一个class token，并加入位置embedding，之后经过transformer layer，最后经过FC层进行分类：
$$
T_{f_{o}}=\left [ t_{cls};T_{f} \right ]+E
$$
$$
T_{f_{i}}=MLP(MSA(T_{f_{i-1}}))
$$
$$
y=fc(LN(T_{f_{b}}))
$$</p>
<h3 id="t2t-vit-architecture">T2T-ViT Architecture</h3>
<p><img src="/img/t2t3.PNG" alt="">
    如上图所示，输入首先经过T2T模块，token的长度逐渐减少，然后T2T-ViT backbone以固定的token作为输入并输出预测。网络结构细节如下表所示：
<img src="/img/t2t4.PNG" alt=""></p>
<h2 id="实验">实验</h2>
<h3 id="t2t-vit-on-imagenet">T2T-ViT on ImageNet</h3>
<p><img src="/img/t2t5.PNG" alt="">
    Comparison between T2T-ViT and ViT by training from scratch on ImageNet
<img src="/img/t2t6.PNG" alt="">
    Comparison between our T2T-ViT with ResNets on ImageNet
<img src="/img/t2t7.PNG" alt="">
    Comparison between our lite T2T-ViT with MobileNets</p>
<h3 id="from-cnn-to-vit">From CNN to ViT</h3>
<p><img src="/img/t2t8.PNG" alt="">
    Deep-narrow structure benefits ViT：ViT-DN和ViT-SW结果比较可以看出，更深更窄的网络更好；<br>
    Dense connection hurts performance of both ViT and T2T-ViT：密集连接对卷积网络有利，但对transformer架构没有带来优势，ViT-Dense和T2T-ViT-Dense与baseline相比，效果都不好；<br>
    SE block improves both ViT and T2T-ViT：SENets、ViT-SE和T2T-ViT-SE的结果都有所提升，因此SE模块对卷积网络和transformer架构都有利；<br>
    ResNeXt structure has few effects on ViT and T2T-ViT：通过增加头数，ViT-ResNeXt和T2T-ViT-ResNeX可以看出，效果没有大的影响；<br>
    Ghost can further compress model and reduce MACs of T2T-ViT：在减少计算量和参数量的同时，也降低了精度。</p>
<h3 id="ablation-study">Ablation study</h3>
<p><img src="/img/t2t9.PNG" alt="">
    从上表可以看出，T2T模块可以提高精度，下表带c的是将T2T中的Soft Split替换为卷积操作，可以看出效果降低了；并且比较了deep-narrow架构的有效性，上边一行是更深更窄的架构，下边一行是更浅更宽的结构。</p>
<h2 id="conclusion">Conclusion</h2>
<p>    在本文中，我们提出了T2T-ViT，可以从头开始训练，并达到与卷积网络相当的性能，T2T-ViT有效的建模图像中的局部和全局信息，克服了ViT的局限性，引入T2T模块逐步的聚合token，并提出了更高效的transformer backbone，更深更窄的架构效果更好。</p>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://joway.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>