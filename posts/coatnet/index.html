<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.89.4" />
  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/Googlecode.min.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Gao dy" />
  <meta property="og:url" content="https://gdy0924.github.io/posts/coatnet/" />
  <link rel="canonical" href="https://gdy0924.github.io/posts/coatnet/" /><link rel="alternate" type="application/atom+xml" href="https://gdy0924.github.io/index.xml" title="XY&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gdy0924.github.io\/"
      },
      "articleSection" : "posts",
      "name" : "CoAtNet",
      "headline" : "CoAtNet",
      "description" : "CoAtNet: Marrying Convolution and Attention for All Data Sizes\nAbstract  在本文中，我们证明了，虽然transformer有更大的潜力，但是由于缺乏正确的归纳偏置，其泛化性可能要比卷积网络更差。为了结合两种网络的优势，我们提出了CoAtNet（Convolution and self-Attention），是一种混合模型：（1）深度逐通道卷积和自注意力机制通过简单的相对注意力来进行统一；（2）有规则的叠加卷积层和注意力层在提高模型的泛化性和效率方面有惊人的效果。\nIntroduction  虽然ViT的效果很好，但他在数据集量少的情况下，仍然不如卷积网络。这表明，transformer层可能缺少卷积网络所拥有的归纳偏置，因此需要大量的数据和计算量来弥补。最近很多工作，试图将归纳偏置来引入transformer，通过引入局部感受野、引入卷积层或带有卷积的FFN层。\n我们的研究表明，卷积网络具有更好的归纳偏置先验，并且收敛速度快，而注意力机制的模型能力\/容量更高，更能从大型的数据集上获益；结合卷积和注意力，可以更好的提高模型的泛化性和能力。我们提出两个想法：（1）逐通道的卷积可以有效的合并到注意力层中；（2）简单的堆叠卷积层和注意力层，可以提高模型的泛化性和能力。基于以上两点，我们提出了CoAtNet，结合了卷积和transformer的优势。\nModel  我们将如何最有效的组合卷积和注意力这个问题分为两部分：\n（1）如何在一个block中结合；\n（2）如何将不同类型的block垂直叠加在一起，形成一个完整的网络。\nMerging Convolution and Self-Attention  对于卷积，我们关注MBConv（如下图所示），MBConv主要有两个特点：\n（1）采用了Depthwise卷积，因此相比于传统卷积，Depthwise Conv的参数能够大大减少；\n（2）采用了“inverted bottleneck”的结构，即先升维再降维，在原来的resnet结构中，输入维度是大的，先降维，然后为了能够相加，再升维，在transformer block的FFN层也是先升维再降维。\n除了inverted bottleneck的相似性，我们还注意到，深度卷积（逐通道卷积）与自注意力机制都可以表示为一个预先定义的感受野中的每维的加权和。\n具体来说，卷积依赖一个固定的卷积核从局部的感受野接收信息： $$ y_{i}=\\sum_{j\\in L(i)}w_{i-j}\\odot x_{j} $$ 其中，$x_{i},y_{i}$分别表示位置$i$的输入和输出，$L(i)$表示$i$的局部邻域。\n相比之下，自注意力的感受野为整个空间位置，并归一化两两的相似性： $$ y_{i}=\\sum_{j\\in G}\\frac{exp(x_{i}^{T}x_{j})}{\\sum_{k\\in G}exp(x_{i}^{T}x_{k})} x_{j} $$ 其中，$G$为全局的空间。\n我们先分析一下他们的优势和劣势：\n（1）逐通道卷积核$w_{i-j}$是一个独立于输入的静态的值，而注意力权重$A_{i,j}$是依赖于输入的动态的值，因此，自注意力更容易捕获到不同位置的复杂交互信息，但是这种灵活性，带来了过拟合的风险，特别是在数据有限的情况下；\n（2）对于给定的任何位置对$(i,j)$，卷积权重$w_{i-j}$只关注它们之间的相对位移，即$i-j$，这被称为平移不变性，而transformer缺乏这一特性，这也可以解释为什么在数据集不多的情况下，卷积网络通常比transformer更好；\n（3）感受野的大小是注意力机制核卷积之间最重要的区别，更大的感受野可以提供更多的上下文信息，可能会提升网络的性能，因此，在视觉中使用自注意力机制是为了全局的感受野，然而自注意力机制的计算复杂度为二次方，也是需要权衡的。 上表为卷积和自注意力的优势比较，那么一个理想的模型应该是结合以上三个优势，基于上述两个公式，一个简单的想法就是：将静态全局卷积核与自注意力权重矩阵相加，在softmax之前或者之后，即： $$ y_{i}^{post}=\\sum_{j\\in G}(\\frac{exp(x_{i}^{T}x_{j})}{\\sum_{k\\in G}exp(x_{i}^{T}x_{k})}\u002bw_{i-j}) x_{j} $$ $$ y_{i}^{pre}=\\sum_{j\\in G}\\frac{exp(x_{i}^{T}x_{j}\u002bw_{i-j})}{\\sum_{k\\in G}exp(x_{i}^{T}x_{k}\u002bw_{i-k})} x_{j} $$ 为了在不增加参数数量的情况下引入全局卷积核，我们将$w_{i-j}$设置为一个标量，而不是公式1中的一个向量。我们使用pre-normalization作为CoAtNet模型的关键组成部分。\nVertical Layout Design  将卷积和注意力结合起来之后，接下来要考虑如何堆叠整个网络，如果直接使用上述公式，其复杂度是二次方的，会影响速度，因此，我们考虑了三种方式：",
      "inLanguage" : "en-US",
      "author" : "Gao dy",
      "creator" : "Gao dy",
      "publisher": "Gao dy",
      "accountablePerson" : "Gao dy",
      "copyrightHolder" : "Gao dy",
      "copyrightYear" : "2022",
      "datePublished": "2022-06-15 00:00:00 \u002b0000 UTC",
      "dateModified" : "2022-06-15 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gdy0924.github.io\/posts\/coatnet\/",
      "keywords" : [  ]
  }
</script>
<title>CoAtNet</title>
  <meta property="og:title" content="CoAtNet" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="CoAtNet: Marrying Convolution and Attention for All Data Sizes
Abstract  在本文中，我们证明了，虽然transformer有更大的潜力，但是由于缺乏正确的归纳偏置，其泛化性可能要比卷积网络更差。为了结合两种网络的优势，我们提出了CoAtNet（Convolution and self-Attention），是一种混合模型：（1）深度逐通道卷积和自注意力机制通过简单的相对注意力来进行统一；（2）有规则的叠加卷积层和注意力层在提高模型的泛化性和效率方面有惊人的效果。
Introduction  虽然ViT的效果很好，但他在数据集量少的情况下，仍然不如卷积网络。这表明，transformer层可能缺少卷积网络所拥有的归纳偏置，因此需要大量的数据和计算量来弥补。最近很多工作，试图将归纳偏置来引入transformer，通过引入局部感受野、引入卷积层或带有卷积的FFN层。
我们的研究表明，卷积网络具有更好的归纳偏置先验，并且收敛速度快，而注意力机制的模型能力/容量更高，更能从大型的数据集上获益；结合卷积和注意力，可以更好的提高模型的泛化性和能力。我们提出两个想法：（1）逐通道的卷积可以有效的合并到注意力层中；（2）简单的堆叠卷积层和注意力层，可以提高模型的泛化性和能力。基于以上两点，我们提出了CoAtNet，结合了卷积和transformer的优势。
Model  我们将如何最有效的组合卷积和注意力这个问题分为两部分：
（1）如何在一个block中结合；
（2）如何将不同类型的block垂直叠加在一起，形成一个完整的网络。
Merging Convolution and Self-Attention  对于卷积，我们关注MBConv（如下图所示），MBConv主要有两个特点：
（1）采用了Depthwise卷积，因此相比于传统卷积，Depthwise Conv的参数能够大大减少；
（2）采用了“inverted bottleneck”的结构，即先升维再降维，在原来的resnet结构中，输入维度是大的，先降维，然后为了能够相加，再升维，在transformer block的FFN层也是先升维再降维。
除了inverted bottleneck的相似性，我们还注意到，深度卷积（逐通道卷积）与自注意力机制都可以表示为一个预先定义的感受野中的每维的加权和。
具体来说，卷积依赖一个固定的卷积核从局部的感受野接收信息： $$ y_{i}=\sum_{j\in L(i)}w_{i-j}\odot x_{j} $$ 其中，$x_{i},y_{i}$分别表示位置$i$的输入和输出，$L(i)$表示$i$的局部邻域。
相比之下，自注意力的感受野为整个空间位置，并归一化两两的相似性： $$ y_{i}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})} x_{j} $$ 其中，$G$为全局的空间。
我们先分析一下他们的优势和劣势：
（1）逐通道卷积核$w_{i-j}$是一个独立于输入的静态的值，而注意力权重$A_{i,j}$是依赖于输入的动态的值，因此，自注意力更容易捕获到不同位置的复杂交互信息，但是这种灵活性，带来了过拟合的风险，特别是在数据有限的情况下；
（2）对于给定的任何位置对$(i,j)$，卷积权重$w_{i-j}$只关注它们之间的相对位移，即$i-j$，这被称为平移不变性，而transformer缺乏这一特性，这也可以解释为什么在数据集不多的情况下，卷积网络通常比transformer更好；
（3）感受野的大小是注意力机制核卷积之间最重要的区别，更大的感受野可以提供更多的上下文信息，可能会提升网络的性能，因此，在视觉中使用自注意力机制是为了全局的感受野，然而自注意力机制的计算复杂度为二次方，也是需要权衡的。 上表为卷积和自注意力的优势比较，那么一个理想的模型应该是结合以上三个优势，基于上述两个公式，一个简单的想法就是：将静态全局卷积核与自注意力权重矩阵相加，在softmax之前或者之后，即： $$ y_{i}^{post}=\sum_{j\in G}(\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})}&#43;w_{i-j}) x_{j} $$ $$ y_{i}^{pre}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j}&#43;w_{i-j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k}&#43;w_{i-k})} x_{j} $$ 为了在不增加参数数量的情况下引入全局卷积核，我们将$w_{i-j}$设置为一个标量，而不是公式1中的一个向量。我们使用pre-normalization作为CoAtNet模型的关键组成部分。
Vertical Layout Design  将卷积和注意力结合起来之后，接下来要考虑如何堆叠整个网络，如果直接使用上述公式，其复杂度是二次方的，会影响速度，因此，我们考虑了三种方式：" />
  <meta name="description" content="CoAtNet: Marrying Convolution and Attention for All Data Sizes
Abstract  在本文中，我们证明了，虽然transformer有更大的潜力，但是由于缺乏正确的归纳偏置，其泛化性可能要比卷积网络更差。为了结合两种网络的优势，我们提出了CoAtNet（Convolution and self-Attention），是一种混合模型：（1）深度逐通道卷积和自注意力机制通过简单的相对注意力来进行统一；（2）有规则的叠加卷积层和注意力层在提高模型的泛化性和效率方面有惊人的效果。
Introduction  虽然ViT的效果很好，但他在数据集量少的情况下，仍然不如卷积网络。这表明，transformer层可能缺少卷积网络所拥有的归纳偏置，因此需要大量的数据和计算量来弥补。最近很多工作，试图将归纳偏置来引入transformer，通过引入局部感受野、引入卷积层或带有卷积的FFN层。
我们的研究表明，卷积网络具有更好的归纳偏置先验，并且收敛速度快，而注意力机制的模型能力/容量更高，更能从大型的数据集上获益；结合卷积和注意力，可以更好的提高模型的泛化性和能力。我们提出两个想法：（1）逐通道的卷积可以有效的合并到注意力层中；（2）简单的堆叠卷积层和注意力层，可以提高模型的泛化性和能力。基于以上两点，我们提出了CoAtNet，结合了卷积和transformer的优势。
Model  我们将如何最有效的组合卷积和注意力这个问题分为两部分：
（1）如何在一个block中结合；
（2）如何将不同类型的block垂直叠加在一起，形成一个完整的网络。
Merging Convolution and Self-Attention  对于卷积，我们关注MBConv（如下图所示），MBConv主要有两个特点：
（1）采用了Depthwise卷积，因此相比于传统卷积，Depthwise Conv的参数能够大大减少；
（2）采用了“inverted bottleneck”的结构，即先升维再降维，在原来的resnet结构中，输入维度是大的，先降维，然后为了能够相加，再升维，在transformer block的FFN层也是先升维再降维。
除了inverted bottleneck的相似性，我们还注意到，深度卷积（逐通道卷积）与自注意力机制都可以表示为一个预先定义的感受野中的每维的加权和。
具体来说，卷积依赖一个固定的卷积核从局部的感受野接收信息： $$ y_{i}=\sum_{j\in L(i)}w_{i-j}\odot x_{j} $$ 其中，$x_{i},y_{i}$分别表示位置$i$的输入和输出，$L(i)$表示$i$的局部邻域。
相比之下，自注意力的感受野为整个空间位置，并归一化两两的相似性： $$ y_{i}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})} x_{j} $$ 其中，$G$为全局的空间。
我们先分析一下他们的优势和劣势：
（1）逐通道卷积核$w_{i-j}$是一个独立于输入的静态的值，而注意力权重$A_{i,j}$是依赖于输入的动态的值，因此，自注意力更容易捕获到不同位置的复杂交互信息，但是这种灵活性，带来了过拟合的风险，特别是在数据有限的情况下；
（2）对于给定的任何位置对$(i,j)$，卷积权重$w_{i-j}$只关注它们之间的相对位移，即$i-j$，这被称为平移不变性，而transformer缺乏这一特性，这也可以解释为什么在数据集不多的情况下，卷积网络通常比transformer更好；
（3）感受野的大小是注意力机制核卷积之间最重要的区别，更大的感受野可以提供更多的上下文信息，可能会提升网络的性能，因此，在视觉中使用自注意力机制是为了全局的感受野，然而自注意力机制的计算复杂度为二次方，也是需要权衡的。 上表为卷积和自注意力的优势比较，那么一个理想的模型应该是结合以上三个优势，基于上述两个公式，一个简单的想法就是：将静态全局卷积核与自注意力权重矩阵相加，在softmax之前或者之后，即： $$ y_{i}^{post}=\sum_{j\in G}(\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})}&#43;w_{i-j}) x_{j} $$ $$ y_{i}^{pre}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j}&#43;w_{i-j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k}&#43;w_{i-k})} x_{j} $$ 为了在不增加参数数量的情况下引入全局卷积核，我们将$w_{i-j}$设置为一个标量，而不是公式1中的一个向量。我们使用pre-normalization作为CoAtNet模型的关键组成部分。
Vertical Layout Design  将卷积和注意力结合起来之后，接下来要考虑如何堆叠整个网络，如果直接使用上述公式，其复杂度是二次方的，会影响速度，因此，我们考虑了三种方式：" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{font-family:bree serif,sans-serif;-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  

  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="XY&#39;s Blog">
  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Bungee+Shade" rel="stylesheet">
  
  

  
  
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/"
      >Gao dy</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/gdy0924" target="_blank">About</a>
  </div>
  
</div>
<div class="row end-xs">
  
  <div class="lang-switch col-xs-3 col-xs-offset-9">
    <a href="/en/">English</a>
  </div>
    
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">CoAtNet</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2022-06-15 00:00:00 UTC">
                15 Jun 2022
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://gdy0924.github.io/">@Gao dy</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>CoAtNet: Marrying Convolution and Attention for All Data Sizes</p>
<h2 id="abstract">Abstract</h2>
<p>    在本文中，我们证明了，虽然transformer有更大的潜力，但是由于缺乏正确的归纳偏置，其泛化性可能要比卷积网络更差。为了结合两种网络的优势，我们提出了CoAtNet（Convolution and self-Attention），是一种混合模型：（1）深度逐通道卷积和自注意力机制通过简单的相对注意力来进行统一；（2）有规则的叠加卷积层和注意力层在提高模型的泛化性和效率方面有惊人的效果。</p>
<h2 id="introduction">Introduction</h2>
<p>    虽然ViT的效果很好，但他在数据集量少的情况下，仍然不如卷积网络。这表明，transformer层可能缺少卷积网络所拥有的归纳偏置，因此需要大量的数据和计算量来弥补。最近很多工作，试图将归纳偏置来引入transformer，通过引入局部感受野、引入卷积层或带有卷积的FFN层。<br>
    我们的研究表明，卷积网络具有更好的归纳偏置先验，并且收敛速度快，而注意力机制的模型能力/容量更高，更能从大型的数据集上获益；结合卷积和注意力，可以更好的提高模型的泛化性和能力。我们提出两个想法：（1）逐通道的卷积可以有效的合并到注意力层中；（2）简单的堆叠卷积层和注意力层，可以提高模型的泛化性和能力。基于以上两点，我们提出了CoAtNet，结合了卷积和transformer的优势。</p>
<h2 id="model">Model</h2>
<p>    我们将如何最有效的组合卷积和注意力这个问题分为两部分：<br>
    （1）如何在一个block中结合；<br>
    （2）如何将不同类型的block垂直叠加在一起，形成一个完整的网络。</p>
<h3 id="merging-convolution-and-self-attention">Merging Convolution and Self-Attention</h3>
<p>    对于卷积，我们关注MBConv（如下图所示），MBConv主要有两个特点：<br>
    （1）采用了Depthwise卷积，因此相比于传统卷积，Depthwise Conv的参数能够大大减少；<br>
    （2）采用了“inverted bottleneck”的结构，即先升维再降维，在原来的resnet结构中，输入维度是大的，先降维，然后为了能够相加，再升维，在transformer block的FFN层也是先升维再降维。</p>
<p><img src="/img/coat1.jpg" alt="">
    除了inverted bottleneck的相似性，我们还注意到，深度卷积（逐通道卷积）与自注意力机制都可以表示为一个预先定义的感受野中的每维的加权和。<br>
    具体来说，卷积依赖一个固定的卷积核从局部的感受野接收信息：
$$
y_{i}=\sum_{j\in L(i)}w_{i-j}\odot x_{j}
$$
    其中，$x_{i},y_{i}$分别表示位置$i$的输入和输出，$L(i)$表示$i$的局部邻域。<br>
    相比之下，自注意力的感受野为整个空间位置，并归一化两两的相似性：
$$
y_{i}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})} x_{j}
$$
    其中，$G$为全局的空间。<br>
    我们先分析一下他们的优势和劣势：<br>
    （1）逐通道卷积核$w_{i-j}$是一个独立于输入的静态的值，而注意力权重$A_{i,j}$是依赖于输入的动态的值，因此，自注意力更容易捕获到不同位置的复杂交互信息，但是这种灵活性，带来了过拟合的风险，特别是在数据有限的情况下；<br>
    （2）对于给定的任何位置对$(i,j)$，卷积权重$w_{i-j}$只关注它们之间的相对位移，即$i-j$，这被称为平移不变性，而transformer缺乏这一特性，这也可以解释为什么在数据集不多的情况下，卷积网络通常比transformer更好；<br>
    （3）感受野的大小是注意力机制核卷积之间最重要的区别，更大的感受野可以提供更多的上下文信息，可能会提升网络的性能，因此，在视觉中使用自注意力机制是为了全局的感受野，然而自注意力机制的计算复杂度为二次方，也是需要权衡的。
<img src="/img/coat2.PNG" alt="">
    上表为卷积和自注意力的优势比较，那么一个理想的模型应该是结合以上三个优势，基于上述两个公式，一个简单的想法就是：将静态全局卷积核与自注意力权重矩阵相加，在softmax之前或者之后，即：
$$
y_{i}^{post}=\sum_{j\in G}(\frac{exp(x_{i}^{T}x_{j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k})}+w_{i-j}) x_{j}
$$
$$
y_{i}^{pre}=\sum_{j\in G}\frac{exp(x_{i}^{T}x_{j}+w_{i-j})}{\sum_{k\in G}exp(x_{i}^{T}x_{k}+w_{i-k})} x_{j}
$$
    为了在不增加参数数量的情况下引入全局卷积核，我们将$w_{i-j}$设置为一个标量，而不是公式1中的一个向量。我们使用pre-normalization作为CoAtNet模型的关键组成部分。</p>
<h3 id="vertical-layout-design">Vertical Layout Design</h3>
<p>    将卷积和注意力结合起来之后，接下来要考虑如何堆叠整个网络，如果直接使用上述公式，其复杂度是二次方的，会影响速度，因此，我们考虑了三种方式：<br>
    （1）对特征图进行下降采样以减少尺寸大小，并对降采样的图像进行全局相对注意力；<br>
    （2）强制进行局部注意力，将全局感受野限制在局部感受野上，像卷积一样；<br>
    （3）用一些线性的注意力代替二次自注意力机制。<br>
    我们简单的尝试了第三种方式，但没有一个很好的结果；对于第二种方式，我们发现实现局部注意力涉及到许多密集的内存访问等操作，在加速器上运行缓慢；因此我们选择第一种方式。<br>
    对于方式一，下采样可以通过（1）ViT中的卷积stem（如：步幅为16×16）；（2）卷积网络种多阶段的渐进pooling方式。基于上述，我们提出了5种变体，并利用实验进行比较。<br>
    （1）当使用ViT stem时，直接堆叠L层的transformer block进行注意力计算，记为$ViT_{REL}$；<br>
    （2）当使用多阶段的方式时，我们构建了一个5阶段的网络，将分辨率逐渐下降，在每个阶段，空间大小减少一倍，通道数量增加一倍。<br>
    stage S0：一个简单的两层卷积；<br>
    stage S1：应用SE模块的MBConv块，因为尺寸太大无法使用全局注意力；<br>
    stage S2-stage S4：考虑使用MBConv或Transformer块，卷积块之前必须有transformer块，该约束是基于先验，即卷积在早期阶段可以更好的提取局部特征，这形成了4种变体，即C-C-C-C、C-C-C-T、C-C-T-T、C-T-T-T，其中C表示卷积，T表示Transformer。<br>
    比较模型的两个方面：泛化能力（genralization）和学习能力（model capacity）：<br>
    （1）泛化能力：当训练损失相同时，测试集的准确率越高，泛化能力越强，泛化能力用来衡量模型对于没见过数据的判断准确度；<br>
    （2）学习能力：当学习数据是庞大、冗余的，学习能力强的模型能够获得更好的性能，学习能力用来衡量拟合大数据集的能力。 
<img src="/img/coat3.PNG" alt="">
    上面这张图展示了在ImageNet-1K（小数据集），JFT（大数据集）上的训练损失和验证准确率。 <br>
    从ImageNet-1K的结果可以看出，针对泛化能力
$$
C-C-C-C \approx C-C-C-T \geq C-C-T-T &gt; C-T-T-T \gg ViT_{REL}
$$
    从JFT的结果可以看出，针对学习能力
$$
C-C-T-T \approx C-T-T-T &gt; ViT_{REL} &gt; C-C-C-T &gt; C-C-C-C
$$
    上述结果表明：卷积越多，更能提高泛化性；transformer层越多，并不意味着更高的处理能力，虽然$ViT_{REL}$有比C-C-C-T 和 C-C-C-C更好，体现了其潜力，但是C-C-T-T 和 C-T-T-T明显优于$ViT_{REL}$。<br>
    下图为C-C-T-T 和 C-T-T-T之间的微调结果的比较，可以看出C-C-T-T效果更好。
<img src="/img/coat4.PNG" alt=""></p>
<h2 id="model-details">Model Details</h2>
<p><img src="/img/coat5.PNG" alt="">
    对于S1到S4的降采样部分，transformer block是通过最大池化来实现的：
$$
x \leftarrow Proj(Pool(x))+Attention(Pool(Norm(x)))
$$
    MBConv block是通过步幅为2的卷积实现的：
$$
x \leftarrow Proj(Pool(x))+Conv(DepthConv(Conv(Norm(x),stride=2)))
$$</p>
<h2 id="实验">实验</h2>
<h3 id="imagenet-1k">ImageNet-1K</h3>
<p><img src="/img/coat6.PNG" alt=""></p>
<h3 id="imagenet-21k">ImageNet-21K</h3>
<p><img src="/img/coat7.PNG" alt="">
<img src="/img/coat8.PNG" alt=""></p>
<h3 id="jft">JFT</h3>
<p><img src="/img/coat9.PNG" alt=""></p>
<h3 id="ablation-studies">Ablation Studies</h3>
<h4 id="relative-attention">relative attention</h4>
<p><img src="/img/coat10.PNG" alt=""></p>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://joway.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>